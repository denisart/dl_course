{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import (load_svhn, random_split_train_val)\n",
    "from gradient_check import (check_gradient)\n",
    "from metrics import (multiclass_accuracy) \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return (train_flat_with_ones, test_flat_with_ones)\n",
    "    \n",
    "(train_X, train_y, test_X, test_y) = load_svhn('data', max_train=10000, max_test=1000)    \n",
    "(train_X, test_X) = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "(train_X, train_y, val_X, val_y) = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return (float(x*x), 2*x)\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return (np.sum(x), np.ones_like(x))\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return (np.sum(x), np.ones_like(x))\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "(loss, grad) = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "(loss, dW) = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 848.506097\n",
      "Epoch 1, loss: 897.610354\n",
      "Epoch 2, loss: 887.062839\n",
      "Epoch 3, loss: 899.847533\n",
      "Epoch 4, loss: 903.229359\n",
      "Epoch 5, loss: 873.693128\n",
      "Epoch 6, loss: 897.060762\n",
      "Epoch 7, loss: 864.561807\n",
      "Epoch 8, loss: 807.178617\n",
      "Epoch 9, loss: 888.076321\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f44d329d550>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 866.488352\n",
      "Epoch 1, loss: 897.336547\n",
      "Epoch 2, loss: 840.448551\n",
      "Epoch 3, loss: 934.985654\n",
      "Epoch 4, loss: 889.664696\n",
      "Epoch 5, loss: 846.211620\n",
      "Epoch 6, loss: 834.637070\n",
      "Epoch 7, loss: 908.846336\n",
      "Epoch 8, loss: 908.560078\n",
      "Epoch 9, loss: 935.403798\n",
      "Epoch 10, loss: 868.430845\n",
      "Epoch 11, loss: 873.655364\n",
      "Epoch 12, loss: 916.147119\n",
      "Epoch 13, loss: 914.876922\n",
      "Epoch 14, loss: 856.522188\n",
      "Epoch 15, loss: 899.084341\n",
      "Epoch 16, loss: 860.270770\n",
      "Epoch 17, loss: 843.229992\n",
      "Epoch 18, loss: 924.493572\n",
      "Epoch 19, loss: 906.764991\n",
      "Epoch 20, loss: 862.833836\n",
      "Epoch 21, loss: 891.184120\n",
      "Epoch 22, loss: 894.018905\n",
      "Epoch 23, loss: 860.247082\n",
      "Epoch 24, loss: 872.084032\n",
      "Epoch 25, loss: 890.494551\n",
      "Epoch 26, loss: 901.892515\n",
      "Epoch 27, loss: 861.288840\n",
      "Epoch 28, loss: 912.163174\n",
      "Epoch 29, loss: 857.869678\n",
      "Epoch 30, loss: 907.418319\n",
      "Epoch 31, loss: 868.421805\n",
      "Epoch 32, loss: 879.685065\n",
      "Epoch 33, loss: 939.769988\n",
      "Epoch 34, loss: 849.728835\n",
      "Epoch 35, loss: 900.694255\n",
      "Epoch 36, loss: 899.251392\n",
      "Epoch 37, loss: 898.805436\n",
      "Epoch 38, loss: 909.411438\n",
      "Epoch 39, loss: 878.995671\n",
      "Epoch 40, loss: 851.013981\n",
      "Epoch 41, loss: 902.019401\n",
      "Epoch 42, loss: 892.544714\n",
      "Epoch 43, loss: 853.811389\n",
      "Epoch 44, loss: 911.937678\n",
      "Epoch 45, loss: 853.527459\n",
      "Epoch 46, loss: 927.030032\n",
      "Epoch 47, loss: 883.118869\n",
      "Epoch 48, loss: 879.613121\n",
      "Epoch 49, loss: 850.676468\n",
      "Epoch 50, loss: 902.160142\n",
      "Epoch 51, loss: 898.940710\n",
      "Epoch 52, loss: 898.473664\n",
      "Epoch 53, loss: 826.761479\n",
      "Epoch 54, loss: 884.191081\n",
      "Epoch 55, loss: 947.939415\n",
      "Epoch 56, loss: 881.728945\n",
      "Epoch 57, loss: 882.879972\n",
      "Epoch 58, loss: 877.892761\n",
      "Epoch 59, loss: 902.378032\n",
      "Epoch 60, loss: 872.420011\n",
      "Epoch 61, loss: 899.387367\n",
      "Epoch 62, loss: 870.926245\n",
      "Epoch 63, loss: 906.268950\n",
      "Epoch 64, loss: 900.412630\n",
      "Epoch 65, loss: 949.255162\n",
      "Epoch 66, loss: 892.154852\n",
      "Epoch 67, loss: 921.620546\n",
      "Epoch 68, loss: 889.591022\n",
      "Epoch 69, loss: 900.029704\n",
      "Epoch 70, loss: 947.297270\n",
      "Epoch 71, loss: 862.052119\n",
      "Epoch 72, loss: 921.027017\n",
      "Epoch 73, loss: 887.631747\n",
      "Epoch 74, loss: 861.561860\n",
      "Epoch 75, loss: 918.917756\n",
      "Epoch 76, loss: 854.909021\n",
      "Epoch 77, loss: 941.039024\n",
      "Epoch 78, loss: 849.073703\n",
      "Epoch 79, loss: 887.982805\n",
      "Epoch 80, loss: 901.339326\n",
      "Epoch 81, loss: 900.770372\n",
      "Epoch 82, loss: 879.903139\n",
      "Epoch 83, loss: 895.949135\n",
      "Epoch 84, loss: 876.929758\n",
      "Epoch 85, loss: 867.712821\n",
      "Epoch 86, loss: 903.949027\n",
      "Epoch 87, loss: 885.259221\n",
      "Epoch 88, loss: 854.968344\n",
      "Epoch 89, loss: 942.165573\n",
      "Epoch 90, loss: 868.263913\n",
      "Epoch 91, loss: 906.629900\n",
      "Epoch 92, loss: 883.627404\n",
      "Epoch 93, loss: 882.557010\n",
      "Epoch 94, loss: 886.023176\n",
      "Epoch 95, loss: 910.552793\n",
      "Epoch 96, loss: 909.181854\n",
      "Epoch 97, loss: 934.475570\n",
      "Epoch 98, loss: 836.225265\n",
      "Epoch 99, loss: 921.249239\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning_rates = 0.001, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 858.653085\n",
      "Epoch 1, loss: 848.698227\n",
      "Epoch 2, loss: 838.383866\n",
      "Epoch 3, loss: 857.984227\n",
      "Epoch 4, loss: 854.129741\n",
      "Epoch 5, loss: 816.428145\n",
      "Epoch 6, loss: 828.838006\n",
      "Epoch 7, loss: 816.143844\n",
      "Epoch 8, loss: 857.506588\n",
      "Epoch 9, loss: 786.412804\n",
      "Epoch 10, loss: 802.052675\n",
      "Epoch 11, loss: 870.972238\n",
      "Epoch 12, loss: 793.265152\n",
      "Epoch 13, loss: 789.953776\n",
      "Epoch 14, loss: 831.739074\n",
      "Epoch 15, loss: 824.070435\n",
      "Epoch 16, loss: 785.296449\n",
      "Epoch 17, loss: 797.819781\n",
      "Epoch 18, loss: 821.508249\n",
      "Epoch 19, loss: 832.284481\n",
      "Epoch 20, loss: 770.281749\n",
      "Epoch 21, loss: 775.968055\n",
      "Epoch 22, loss: 801.273713\n",
      "Epoch 23, loss: 801.367975\n",
      "Epoch 24, loss: 811.304169\n",
      "Epoch 25, loss: 825.096932\n",
      "Epoch 26, loss: 789.003683\n",
      "Epoch 27, loss: 812.160981\n",
      "Epoch 28, loss: 822.171363\n",
      "Epoch 29, loss: 811.161561\n",
      "Epoch 30, loss: 802.867604\n",
      "Epoch 31, loss: 770.577506\n",
      "Epoch 32, loss: 807.281932\n",
      "Epoch 33, loss: 770.753310\n",
      "Epoch 34, loss: 791.822304\n",
      "Epoch 35, loss: 795.399713\n",
      "Epoch 36, loss: 808.637217\n",
      "Epoch 37, loss: 785.569946\n",
      "Epoch 38, loss: 816.574054\n",
      "Epoch 39, loss: 811.556311\n",
      "Epoch 40, loss: 775.738457\n",
      "Epoch 41, loss: 794.286250\n",
      "Epoch 42, loss: 798.517133\n",
      "Epoch 43, loss: 775.713569\n",
      "Epoch 44, loss: 793.254527\n",
      "Epoch 45, loss: 781.220695\n",
      "Epoch 46, loss: 793.671285\n",
      "Epoch 47, loss: 785.409718\n",
      "Epoch 48, loss: 816.772085\n",
      "Epoch 49, loss: 784.699434\n",
      "Epoch 50, loss: 755.709094\n",
      "Epoch 51, loss: 778.817372\n",
      "Epoch 52, loss: 770.365022\n",
      "Epoch 53, loss: 808.942872\n",
      "Epoch 54, loss: 766.702610\n",
      "Epoch 55, loss: 813.286163\n",
      "Epoch 56, loss: 779.244060\n",
      "Epoch 57, loss: 783.144997\n",
      "Epoch 58, loss: 774.440570\n",
      "Epoch 59, loss: 781.855058\n",
      "Epoch 60, loss: 738.129889\n",
      "Epoch 61, loss: 802.955233\n",
      "Epoch 62, loss: 800.463068\n",
      "Epoch 63, loss: 751.725140\n",
      "Epoch 64, loss: 762.640266\n",
      "Epoch 65, loss: 764.872885\n",
      "Epoch 66, loss: 757.103212\n",
      "Epoch 67, loss: 774.390775\n",
      "Epoch 68, loss: 808.038841\n",
      "Epoch 69, loss: 745.586477\n",
      "Epoch 70, loss: 800.943129\n",
      "Epoch 71, loss: 794.433550\n",
      "Epoch 72, loss: 735.969536\n",
      "Epoch 73, loss: 790.584933\n",
      "Epoch 74, loss: 728.623466\n",
      "Epoch 75, loss: 810.727636\n",
      "Epoch 76, loss: 766.031406\n",
      "Epoch 77, loss: 744.670716\n",
      "Epoch 78, loss: 766.921458\n",
      "Epoch 79, loss: 784.162498\n",
      "Epoch 80, loss: 770.796036\n",
      "Epoch 81, loss: 776.847767\n",
      "Epoch 82, loss: 787.998497\n",
      "Epoch 83, loss: 731.423118\n",
      "Epoch 84, loss: 813.205044\n",
      "Epoch 85, loss: 711.310362\n",
      "Epoch 86, loss: 779.895329\n",
      "Epoch 87, loss: 766.679680\n",
      "Epoch 88, loss: 770.838626\n",
      "Epoch 89, loss: 737.405346\n",
      "Epoch 90, loss: 754.727478\n",
      "Epoch 91, loss: 785.953265\n",
      "Epoch 92, loss: 798.539162\n",
      "Epoch 93, loss: 726.954254\n",
      "Epoch 94, loss: 804.090619\n",
      "Epoch 95, loss: 788.089593\n",
      "Epoch 96, loss: 810.987513\n",
      "Epoch 97, loss: 710.063433\n",
      "Epoch 98, loss: 784.685862\n",
      "Epoch 99, loss: 794.500947\n",
      "Epoch 100, loss: 782.199965\n",
      "Epoch 101, loss: 743.327543\n",
      "Epoch 102, loss: 743.864390\n",
      "Epoch 103, loss: 801.771325\n",
      "Epoch 104, loss: 766.578454\n",
      "Epoch 105, loss: 763.479803\n",
      "Epoch 106, loss: 744.806421\n",
      "Epoch 107, loss: 773.381680\n",
      "Epoch 108, loss: 781.069661\n",
      "Epoch 109, loss: 759.868503\n",
      "Epoch 110, loss: 770.817008\n",
      "Epoch 111, loss: 778.527868\n",
      "Epoch 112, loss: 761.395521\n",
      "Epoch 113, loss: 755.228863\n",
      "Epoch 114, loss: 773.717678\n",
      "Epoch 115, loss: 750.004830\n",
      "Epoch 116, loss: 764.579008\n",
      "Epoch 117, loss: 734.284853\n",
      "Epoch 118, loss: 780.691724\n",
      "Epoch 119, loss: 779.008349\n",
      "Epoch 120, loss: 770.773472\n",
      "Epoch 121, loss: 738.974294\n",
      "Epoch 122, loss: 790.256086\n",
      "Epoch 123, loss: 751.364958\n",
      "Epoch 124, loss: 746.844818\n",
      "Epoch 125, loss: 730.385736\n",
      "Epoch 126, loss: 751.488948\n",
      "Epoch 127, loss: 782.743896\n",
      "Epoch 128, loss: 781.265397\n",
      "Epoch 129, loss: 734.196397\n",
      "Epoch 130, loss: 785.009613\n",
      "Epoch 131, loss: 735.347255\n",
      "Epoch 132, loss: 809.285706\n",
      "Epoch 133, loss: 753.930829\n",
      "Epoch 134, loss: 719.517042\n",
      "Epoch 135, loss: 792.293570\n",
      "Epoch 136, loss: 758.917434\n",
      "Epoch 137, loss: 736.189126\n",
      "Epoch 138, loss: 711.358226\n",
      "Epoch 139, loss: 734.073619\n",
      "Epoch 140, loss: 788.669227\n",
      "Epoch 141, loss: 728.485735\n",
      "Epoch 142, loss: 781.946446\n",
      "Epoch 143, loss: 716.236152\n",
      "Epoch 144, loss: 769.018413\n",
      "Epoch 145, loss: 746.983259\n",
      "Epoch 146, loss: 758.833851\n",
      "Epoch 147, loss: 826.388289\n",
      "Epoch 148, loss: 742.816958\n",
      "Epoch 149, loss: 725.880149\n",
      "Epoch 150, loss: 769.746864\n",
      "Epoch 151, loss: 758.942202\n",
      "Epoch 152, loss: 737.661032\n",
      "Epoch 153, loss: 770.054130\n",
      "Epoch 154, loss: 728.370162\n",
      "Epoch 155, loss: 770.430992\n",
      "Epoch 156, loss: 712.796050\n",
      "Epoch 157, loss: 742.587351\n",
      "Epoch 158, loss: 777.640227\n",
      "Epoch 159, loss: 726.581289\n",
      "Epoch 160, loss: 727.353012\n",
      "Epoch 161, loss: 777.401248\n",
      "Epoch 162, loss: 753.130960\n",
      "Epoch 163, loss: 704.585358\n",
      "Epoch 164, loss: 741.515553\n",
      "Epoch 165, loss: 765.444028\n",
      "Epoch 166, loss: 738.237365\n",
      "Epoch 167, loss: 750.480249\n",
      "Epoch 168, loss: 747.176316\n",
      "Epoch 169, loss: 766.663330\n",
      "Epoch 170, loss: 735.067204\n",
      "Epoch 171, loss: 762.052306\n",
      "Epoch 172, loss: 722.710652\n",
      "Epoch 173, loss: 787.016433\n",
      "Epoch 174, loss: 746.728691\n",
      "Epoch 175, loss: 751.853131\n",
      "Epoch 176, loss: 710.002665\n",
      "Epoch 177, loss: 745.716616\n",
      "Epoch 178, loss: 740.688033\n",
      "Epoch 179, loss: 762.414725\n",
      "Epoch 180, loss: 773.416949\n",
      "Epoch 181, loss: 738.384665\n",
      "Epoch 182, loss: 735.973485\n",
      "Epoch 183, loss: 716.276408\n",
      "Epoch 184, loss: 733.201489\n",
      "Epoch 185, loss: 717.421916\n",
      "Epoch 186, loss: 776.354865\n",
      "Epoch 187, loss: 708.990082\n",
      "Epoch 188, loss: 790.122050\n",
      "Epoch 189, loss: 727.621616\n",
      "Epoch 190, loss: 710.273715\n",
      "Epoch 191, loss: 716.268615\n",
      "Epoch 192, loss: 749.326897\n",
      "Epoch 193, loss: 768.243613\n",
      "Epoch 194, loss: 752.370177\n",
      "Epoch 195, loss: 757.495083\n",
      "Epoch 196, loss: 747.985565\n",
      "Epoch 197, loss: 722.621330\n",
      "Epoch 198, loss: 714.522641\n",
      "Epoch 199, loss: 755.371707\n",
      "This accuracy = 0.187\n",
      "Start learning_rates = 0.001, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 848.717169\n",
      "Epoch 1, loss: 886.161903\n",
      "Epoch 2, loss: 860.884977\n",
      "Epoch 3, loss: 873.952563\n",
      "Epoch 4, loss: 886.533132\n",
      "Epoch 5, loss: 791.348688\n",
      "Epoch 6, loss: 841.334468\n",
      "Epoch 7, loss: 807.643548\n",
      "Epoch 8, loss: 837.550503\n",
      "Epoch 9, loss: 857.386483\n",
      "Epoch 10, loss: 817.685707\n",
      "Epoch 11, loss: 797.404235\n",
      "Epoch 12, loss: 830.453260\n",
      "Epoch 13, loss: 796.277410\n",
      "Epoch 14, loss: 822.397187\n",
      "Epoch 15, loss: 810.307427\n",
      "Epoch 16, loss: 833.392865\n",
      "Epoch 17, loss: 808.050438\n",
      "Epoch 18, loss: 855.105414\n",
      "Epoch 19, loss: 804.933373\n",
      "Epoch 20, loss: 798.215844\n",
      "Epoch 21, loss: 837.556891\n",
      "Epoch 22, loss: 792.171537\n",
      "Epoch 23, loss: 806.604835\n",
      "Epoch 24, loss: 780.799040\n",
      "Epoch 25, loss: 809.321042\n",
      "Epoch 26, loss: 766.869469\n",
      "Epoch 27, loss: 783.095711\n",
      "Epoch 28, loss: 864.898725\n",
      "Epoch 29, loss: 804.282337\n",
      "Epoch 30, loss: 837.594669\n",
      "Epoch 31, loss: 790.120596\n",
      "Epoch 32, loss: 814.911632\n",
      "Epoch 33, loss: 782.433774\n",
      "Epoch 34, loss: 786.670036\n",
      "Epoch 35, loss: 777.893059\n",
      "Epoch 36, loss: 759.258100\n",
      "Epoch 37, loss: 723.968969\n",
      "Epoch 38, loss: 829.248916\n",
      "Epoch 39, loss: 792.704009\n",
      "Epoch 40, loss: 816.658180\n",
      "Epoch 41, loss: 799.803843\n",
      "Epoch 42, loss: 762.574705\n",
      "Epoch 43, loss: 809.461958\n",
      "Epoch 44, loss: 807.928686\n",
      "Epoch 45, loss: 784.925351\n",
      "Epoch 46, loss: 794.784099\n",
      "Epoch 47, loss: 815.663086\n",
      "Epoch 48, loss: 745.250622\n",
      "Epoch 49, loss: 784.214944\n",
      "Epoch 50, loss: 776.307860\n",
      "Epoch 51, loss: 760.653368\n",
      "Epoch 52, loss: 807.538050\n",
      "Epoch 53, loss: 816.580714\n",
      "Epoch 54, loss: 749.265995\n",
      "Epoch 55, loss: 800.195041\n",
      "Epoch 56, loss: 795.916108\n",
      "Epoch 57, loss: 816.289093\n",
      "Epoch 58, loss: 787.588567\n",
      "Epoch 59, loss: 772.526163\n",
      "Epoch 60, loss: 755.755675\n",
      "Epoch 61, loss: 772.203068\n",
      "Epoch 62, loss: 803.493631\n",
      "Epoch 63, loss: 772.779954\n",
      "Epoch 64, loss: 816.279653\n",
      "Epoch 65, loss: 738.563756\n",
      "Epoch 66, loss: 744.058615\n",
      "Epoch 67, loss: 823.190473\n",
      "Epoch 68, loss: 738.311784\n",
      "Epoch 69, loss: 798.974864\n",
      "Epoch 70, loss: 774.287191\n",
      "Epoch 71, loss: 787.272706\n",
      "Epoch 72, loss: 722.029616\n",
      "Epoch 73, loss: 756.916786\n",
      "Epoch 74, loss: 809.615486\n",
      "Epoch 75, loss: 775.976557\n",
      "Epoch 76, loss: 788.295654\n",
      "Epoch 77, loss: 762.965206\n",
      "Epoch 78, loss: 761.318467\n",
      "Epoch 79, loss: 777.854493\n",
      "Epoch 80, loss: 751.797664\n",
      "Epoch 81, loss: 768.462541\n",
      "Epoch 82, loss: 770.607338\n",
      "Epoch 83, loss: 755.185780\n",
      "Epoch 84, loss: 808.010806\n",
      "Epoch 85, loss: 788.227838\n",
      "Epoch 86, loss: 718.064743\n",
      "Epoch 87, loss: 824.881340\n",
      "Epoch 88, loss: 766.059024\n",
      "Epoch 89, loss: 721.721282\n",
      "Epoch 90, loss: 781.508094\n",
      "Epoch 91, loss: 732.705250\n",
      "Epoch 92, loss: 822.652701\n",
      "Epoch 93, loss: 760.970743\n",
      "Epoch 94, loss: 766.224464\n",
      "Epoch 95, loss: 813.591459\n",
      "Epoch 96, loss: 738.061046\n",
      "Epoch 97, loss: 738.460998\n",
      "Epoch 98, loss: 805.455671\n",
      "Epoch 99, loss: 763.303635\n",
      "Epoch 100, loss: 793.816828\n",
      "Epoch 101, loss: 764.383789\n",
      "Epoch 102, loss: 712.396029\n",
      "Epoch 103, loss: 755.356943\n",
      "Epoch 104, loss: 738.682989\n",
      "Epoch 105, loss: 748.872428\n",
      "Epoch 106, loss: 751.621305\n",
      "Epoch 107, loss: 755.200322\n",
      "Epoch 108, loss: 762.365659\n",
      "Epoch 109, loss: 756.187395\n",
      "Epoch 110, loss: 762.930092\n",
      "Epoch 111, loss: 798.433189\n",
      "Epoch 112, loss: 727.257606\n",
      "Epoch 113, loss: 778.058355\n",
      "Epoch 114, loss: 698.934105\n",
      "Epoch 115, loss: 783.381407\n",
      "Epoch 116, loss: 776.107224\n",
      "Epoch 117, loss: 767.126043\n",
      "Epoch 118, loss: 750.609000\n",
      "Epoch 119, loss: 765.862562\n",
      "Epoch 120, loss: 753.860251\n",
      "Epoch 121, loss: 777.333907\n",
      "Epoch 122, loss: 743.748785\n",
      "Epoch 123, loss: 752.358841\n",
      "Epoch 124, loss: 730.993266\n",
      "Epoch 125, loss: 744.101916\n",
      "Epoch 126, loss: 725.292145\n",
      "Epoch 127, loss: 755.255628\n",
      "Epoch 128, loss: 771.401827\n",
      "Epoch 129, loss: 737.390669\n",
      "Epoch 130, loss: 707.136546\n",
      "Epoch 131, loss: 774.293421\n",
      "Epoch 132, loss: 769.528289\n",
      "Epoch 133, loss: 756.011048\n",
      "Epoch 134, loss: 761.805652\n",
      "Epoch 135, loss: 723.003821\n",
      "Epoch 136, loss: 788.693761\n",
      "Epoch 137, loss: 717.867626\n",
      "Epoch 138, loss: 795.508142\n",
      "Epoch 139, loss: 806.833630\n",
      "Epoch 140, loss: 729.946894\n",
      "Epoch 141, loss: 759.386792\n",
      "Epoch 142, loss: 708.059091\n",
      "Epoch 143, loss: 784.254239\n",
      "Epoch 144, loss: 756.710971\n",
      "Epoch 145, loss: 738.498884\n",
      "Epoch 146, loss: 766.351733\n",
      "Epoch 147, loss: 684.616239\n",
      "Epoch 148, loss: 770.019475\n",
      "Epoch 149, loss: 729.815693\n",
      "Epoch 150, loss: 724.626114\n",
      "Epoch 151, loss: 765.891666\n",
      "Epoch 152, loss: 785.851270\n",
      "Epoch 153, loss: 756.843444\n",
      "Epoch 154, loss: 711.109368\n",
      "Epoch 155, loss: 776.567313\n",
      "Epoch 156, loss: 764.925170\n",
      "Epoch 157, loss: 742.240864\n",
      "Epoch 158, loss: 747.373883\n",
      "Epoch 159, loss: 777.983213\n",
      "Epoch 160, loss: 717.517214\n",
      "Epoch 161, loss: 745.963658\n",
      "Epoch 162, loss: 766.914996\n",
      "Epoch 163, loss: 735.724015\n",
      "Epoch 164, loss: 752.163829\n",
      "Epoch 165, loss: 783.694523\n",
      "Epoch 166, loss: 734.942185\n",
      "Epoch 167, loss: 781.351694\n",
      "Epoch 168, loss: 715.789937\n",
      "Epoch 169, loss: 730.137621\n",
      "Epoch 170, loss: 775.078778\n",
      "Epoch 171, loss: 707.169693\n",
      "Epoch 172, loss: 719.207723\n",
      "Epoch 173, loss: 785.877240\n",
      "Epoch 174, loss: 764.678710\n",
      "Epoch 175, loss: 739.116857\n",
      "Epoch 176, loss: 786.941250\n",
      "Epoch 177, loss: 737.871831\n",
      "Epoch 178, loss: 728.333176\n",
      "Epoch 179, loss: 726.228453\n",
      "Epoch 180, loss: 716.686696\n",
      "Epoch 181, loss: 755.724027\n",
      "Epoch 182, loss: 694.433491\n",
      "Epoch 183, loss: 703.055968\n",
      "Epoch 184, loss: 731.770877\n",
      "Epoch 185, loss: 783.354739\n",
      "Epoch 186, loss: 701.420098\n",
      "Epoch 187, loss: 761.131995\n",
      "Epoch 188, loss: 748.818346\n",
      "Epoch 189, loss: 733.420308\n",
      "Epoch 190, loss: 661.071618\n",
      "Epoch 191, loss: 768.342437\n",
      "Epoch 192, loss: 752.840268\n",
      "Epoch 193, loss: 764.788673\n",
      "Epoch 194, loss: 743.305403\n",
      "Epoch 195, loss: 756.460044\n",
      "Epoch 196, loss: 736.192788\n",
      "Epoch 197, loss: 753.380814\n",
      "Epoch 198, loss: 744.805756\n",
      "Epoch 199, loss: 725.270833\n",
      "This accuracy = 0.196\n",
      "Start learning_rates = 0.001, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 866.006548\n",
      "Epoch 1, loss: 844.998758\n",
      "Epoch 2, loss: 828.737141\n",
      "Epoch 3, loss: 866.211033\n",
      "Epoch 4, loss: 825.876175\n",
      "Epoch 5, loss: 878.581961\n",
      "Epoch 6, loss: 864.516308\n",
      "Epoch 7, loss: 832.517321\n",
      "Epoch 8, loss: 821.776055\n",
      "Epoch 9, loss: 825.674158\n",
      "Epoch 10, loss: 851.990572\n",
      "Epoch 11, loss: 785.430298\n",
      "Epoch 12, loss: 828.861240\n",
      "Epoch 13, loss: 819.080706\n",
      "Epoch 14, loss: 788.502023\n",
      "Epoch 15, loss: 809.225233\n",
      "Epoch 16, loss: 749.757336\n",
      "Epoch 17, loss: 869.349282\n",
      "Epoch 18, loss: 788.425373\n",
      "Epoch 19, loss: 743.775133\n",
      "Epoch 20, loss: 857.503580\n",
      "Epoch 21, loss: 808.230982\n",
      "Epoch 22, loss: 791.326940\n",
      "Epoch 23, loss: 871.010288\n",
      "Epoch 24, loss: 802.142440\n",
      "Epoch 25, loss: 805.364822\n",
      "Epoch 26, loss: 866.786386\n",
      "Epoch 27, loss: 794.810878\n",
      "Epoch 28, loss: 813.623950\n",
      "Epoch 29, loss: 761.906853\n",
      "Epoch 30, loss: 811.991394\n",
      "Epoch 31, loss: 773.800453\n",
      "Epoch 32, loss: 781.777848\n",
      "Epoch 33, loss: 828.879633\n",
      "Epoch 34, loss: 778.192169\n",
      "Epoch 35, loss: 787.645703\n",
      "Epoch 36, loss: 815.802753\n",
      "Epoch 37, loss: 805.600232\n",
      "Epoch 38, loss: 777.376509\n",
      "Epoch 39, loss: 818.873469\n",
      "Epoch 40, loss: 782.946954\n",
      "Epoch 41, loss: 770.994485\n",
      "Epoch 42, loss: 789.505318\n",
      "Epoch 43, loss: 841.083517\n",
      "Epoch 44, loss: 775.696848\n",
      "Epoch 45, loss: 831.373413\n",
      "Epoch 46, loss: 751.217264\n",
      "Epoch 47, loss: 820.726995\n",
      "Epoch 48, loss: 816.455479\n",
      "Epoch 49, loss: 722.155183\n",
      "Epoch 50, loss: 796.753035\n",
      "Epoch 51, loss: 773.072799\n",
      "Epoch 52, loss: 804.921052\n",
      "Epoch 53, loss: 803.109840\n",
      "Epoch 54, loss: 752.175007\n",
      "Epoch 55, loss: 794.709542\n",
      "Epoch 56, loss: 809.989878\n",
      "Epoch 57, loss: 803.090286\n",
      "Epoch 58, loss: 821.198498\n",
      "Epoch 59, loss: 821.279366\n",
      "Epoch 60, loss: 740.404260\n",
      "Epoch 61, loss: 779.031662\n",
      "Epoch 62, loss: 761.850909\n",
      "Epoch 63, loss: 782.200035\n",
      "Epoch 64, loss: 795.638822\n",
      "Epoch 65, loss: 786.844177\n",
      "Epoch 66, loss: 772.537780\n",
      "Epoch 67, loss: 771.102677\n",
      "Epoch 68, loss: 771.404395\n",
      "Epoch 69, loss: 815.565308\n",
      "Epoch 70, loss: 766.139250\n",
      "Epoch 71, loss: 725.331969\n",
      "Epoch 72, loss: 780.204400\n",
      "Epoch 73, loss: 754.493962\n",
      "Epoch 74, loss: 785.544670\n",
      "Epoch 75, loss: 769.178507\n",
      "Epoch 76, loss: 800.695398\n",
      "Epoch 77, loss: 794.877578\n",
      "Epoch 78, loss: 738.984598\n",
      "Epoch 79, loss: 762.724045\n",
      "Epoch 80, loss: 781.068643\n",
      "Epoch 81, loss: 758.454832\n",
      "Epoch 82, loss: 796.975762\n",
      "Epoch 83, loss: 787.080245\n",
      "Epoch 84, loss: 754.408167\n",
      "Epoch 85, loss: 725.646643\n",
      "Epoch 86, loss: 741.461586\n",
      "Epoch 87, loss: 755.031257\n",
      "Epoch 88, loss: 805.011012\n",
      "Epoch 89, loss: 773.195504\n",
      "Epoch 90, loss: 774.107322\n",
      "Epoch 91, loss: 760.856740\n",
      "Epoch 92, loss: 741.511068\n",
      "Epoch 93, loss: 714.734266\n",
      "Epoch 94, loss: 802.574795\n",
      "Epoch 95, loss: 749.482681\n",
      "Epoch 96, loss: 778.103878\n",
      "Epoch 97, loss: 761.391956\n",
      "Epoch 98, loss: 784.248153\n",
      "Epoch 99, loss: 737.905773\n",
      "Epoch 100, loss: 751.328550\n",
      "Epoch 101, loss: 782.914766\n",
      "Epoch 102, loss: 775.771018\n",
      "Epoch 103, loss: 774.654974\n",
      "Epoch 104, loss: 765.037047\n",
      "Epoch 105, loss: 778.380415\n",
      "Epoch 106, loss: 750.619368\n",
      "Epoch 107, loss: 763.221895\n",
      "Epoch 108, loss: 740.391042\n",
      "Epoch 109, loss: 764.302072\n",
      "Epoch 110, loss: 731.058849\n",
      "Epoch 111, loss: 754.816657\n",
      "Epoch 112, loss: 721.740377\n",
      "Epoch 113, loss: 750.477899\n",
      "Epoch 114, loss: 788.010543\n",
      "Epoch 115, loss: 770.596702\n",
      "Epoch 116, loss: 763.317666\n",
      "Epoch 117, loss: 788.050156\n",
      "Epoch 118, loss: 737.980597\n",
      "Epoch 119, loss: 785.962283\n",
      "Epoch 120, loss: 743.072342\n",
      "Epoch 121, loss: 734.084175\n",
      "Epoch 122, loss: 726.776653\n",
      "Epoch 123, loss: 748.537307\n",
      "Epoch 124, loss: 801.342636\n",
      "Epoch 125, loss: 746.403040\n",
      "Epoch 126, loss: 731.046537\n",
      "Epoch 127, loss: 725.256153\n",
      "Epoch 128, loss: 713.613814\n",
      "Epoch 129, loss: 814.490126\n",
      "Epoch 130, loss: 750.511872\n",
      "Epoch 131, loss: 768.341802\n",
      "Epoch 132, loss: 744.663341\n",
      "Epoch 133, loss: 752.651646\n",
      "Epoch 134, loss: 745.105773\n",
      "Epoch 135, loss: 750.990831\n",
      "Epoch 136, loss: 788.237507\n",
      "Epoch 137, loss: 715.236665\n",
      "Epoch 138, loss: 757.114188\n",
      "Epoch 139, loss: 765.454040\n",
      "Epoch 140, loss: 742.565097\n",
      "Epoch 141, loss: 771.375729\n",
      "Epoch 142, loss: 735.788067\n",
      "Epoch 143, loss: 774.365759\n",
      "Epoch 144, loss: 725.465872\n",
      "Epoch 145, loss: 745.505720\n",
      "Epoch 146, loss: 749.426044\n",
      "Epoch 147, loss: 724.264664\n",
      "Epoch 148, loss: 743.738294\n",
      "Epoch 149, loss: 737.646355\n",
      "Epoch 150, loss: 766.383338\n",
      "Epoch 151, loss: 776.987941\n",
      "Epoch 152, loss: 741.005564\n",
      "Epoch 153, loss: 728.327798\n",
      "Epoch 154, loss: 737.038901\n",
      "Epoch 155, loss: 692.879277\n",
      "Epoch 156, loss: 767.984812\n",
      "Epoch 157, loss: 740.741776\n",
      "Epoch 158, loss: 802.908388\n",
      "Epoch 159, loss: 720.540875\n",
      "Epoch 160, loss: 750.410494\n",
      "Epoch 161, loss: 736.683198\n",
      "Epoch 162, loss: 774.623428\n",
      "Epoch 163, loss: 731.506343\n",
      "Epoch 164, loss: 695.790148\n",
      "Epoch 165, loss: 737.756070\n",
      "Epoch 166, loss: 747.797127\n",
      "Epoch 167, loss: 714.355357\n",
      "Epoch 168, loss: 749.054202\n",
      "Epoch 169, loss: 743.704614\n",
      "Epoch 170, loss: 734.693803\n",
      "Epoch 171, loss: 758.044484\n",
      "Epoch 172, loss: 763.454828\n",
      "Epoch 173, loss: 771.954898\n",
      "Epoch 174, loss: 714.890934\n",
      "Epoch 175, loss: 762.619370\n",
      "Epoch 176, loss: 723.218259\n",
      "Epoch 177, loss: 756.984120\n",
      "Epoch 178, loss: 742.563243\n",
      "Epoch 179, loss: 756.814220\n",
      "Epoch 180, loss: 803.597485\n",
      "Epoch 181, loss: 732.138540\n",
      "Epoch 182, loss: 722.280550\n",
      "Epoch 183, loss: 761.186073\n",
      "Epoch 184, loss: 723.427470\n",
      "Epoch 185, loss: 754.908977\n",
      "Epoch 186, loss: 731.366608\n",
      "Epoch 187, loss: 746.696898\n",
      "Epoch 188, loss: 764.915271\n",
      "Epoch 189, loss: 742.809782\n",
      "Epoch 190, loss: 733.455902\n",
      "Epoch 191, loss: 777.886881\n",
      "Epoch 192, loss: 734.329482\n",
      "Epoch 193, loss: 709.338196\n",
      "Epoch 194, loss: 745.999473\n",
      "Epoch 195, loss: 743.997171\n",
      "Epoch 196, loss: 742.852433\n",
      "Epoch 197, loss: 766.185213\n",
      "Epoch 198, loss: 709.931536\n",
      "Epoch 199, loss: 810.282907\n",
      "This accuracy = 0.203\n",
      "Start learning_rates = 0.0001, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 688.166829\n",
      "Epoch 1, loss: 681.677236\n",
      "Epoch 2, loss: 676.528411\n",
      "Epoch 3, loss: 671.859781\n",
      "Epoch 4, loss: 668.273619\n",
      "Epoch 5, loss: 665.033221\n",
      "Epoch 6, loss: 662.036636\n",
      "Epoch 7, loss: 659.831764\n",
      "Epoch 8, loss: 658.016307\n",
      "Epoch 9, loss: 655.982396\n",
      "Epoch 10, loss: 654.627841\n",
      "Epoch 11, loss: 653.148592\n",
      "Epoch 12, loss: 651.935977\n",
      "Epoch 13, loss: 650.884028\n",
      "Epoch 14, loss: 649.914529\n",
      "Epoch 15, loss: 648.948006\n",
      "Epoch 16, loss: 648.114874\n",
      "Epoch 17, loss: 647.375324\n",
      "Epoch 18, loss: 646.461731\n",
      "Epoch 19, loss: 645.665930\n",
      "Epoch 20, loss: 645.220515\n",
      "Epoch 21, loss: 644.495101\n",
      "Epoch 22, loss: 643.951457\n",
      "Epoch 23, loss: 643.395680\n",
      "Epoch 24, loss: 642.996125\n",
      "Epoch 25, loss: 642.388215\n",
      "Epoch 26, loss: 641.692363\n",
      "Epoch 27, loss: 641.473191\n",
      "Epoch 28, loss: 641.065861\n",
      "Epoch 29, loss: 640.631398\n",
      "Epoch 30, loss: 639.964954\n",
      "Epoch 31, loss: 639.759015\n",
      "Epoch 32, loss: 639.294784\n",
      "Epoch 33, loss: 639.023385\n",
      "Epoch 34, loss: 638.663639\n",
      "Epoch 35, loss: 638.062318\n",
      "Epoch 36, loss: 637.941302\n",
      "Epoch 37, loss: 637.901351\n",
      "Epoch 38, loss: 637.398504\n",
      "Epoch 39, loss: 637.116284\n",
      "Epoch 40, loss: 636.626760\n",
      "Epoch 41, loss: 636.495438\n",
      "Epoch 42, loss: 636.274131\n",
      "Epoch 43, loss: 635.901672\n",
      "Epoch 44, loss: 635.334345\n",
      "Epoch 45, loss: 635.354580\n",
      "Epoch 46, loss: 635.098602\n",
      "Epoch 47, loss: 634.733451\n",
      "Epoch 48, loss: 634.652301\n",
      "Epoch 49, loss: 634.397284\n",
      "Epoch 50, loss: 634.071229\n",
      "Epoch 51, loss: 633.717502\n",
      "Epoch 52, loss: 633.762573\n",
      "Epoch 53, loss: 633.582721\n",
      "Epoch 54, loss: 633.136647\n",
      "Epoch 55, loss: 633.046817\n",
      "Epoch 56, loss: 632.885007\n",
      "Epoch 57, loss: 632.720666\n",
      "Epoch 58, loss: 632.377018\n",
      "Epoch 59, loss: 632.226425\n",
      "Epoch 60, loss: 632.105274\n",
      "Epoch 61, loss: 631.704053\n",
      "Epoch 62, loss: 631.646870\n",
      "Epoch 63, loss: 631.156785\n",
      "Epoch 64, loss: 631.117252\n",
      "Epoch 65, loss: 630.934690\n",
      "Epoch 66, loss: 630.819071\n",
      "Epoch 67, loss: 630.637429\n",
      "Epoch 68, loss: 630.365823\n",
      "Epoch 69, loss: 630.420359\n",
      "Epoch 70, loss: 630.181117\n",
      "Epoch 71, loss: 629.833612\n",
      "Epoch 72, loss: 629.808599\n",
      "Epoch 73, loss: 629.490757\n",
      "Epoch 74, loss: 629.416896\n",
      "Epoch 75, loss: 629.333839\n",
      "Epoch 76, loss: 629.204593\n",
      "Epoch 77, loss: 629.030658\n",
      "Epoch 78, loss: 628.988611\n",
      "Epoch 79, loss: 628.674137\n",
      "Epoch 80, loss: 628.319915\n",
      "Epoch 81, loss: 628.323960\n",
      "Epoch 82, loss: 628.230717\n",
      "Epoch 83, loss: 627.891568\n",
      "Epoch 84, loss: 628.056739\n",
      "Epoch 85, loss: 627.806894\n",
      "Epoch 86, loss: 627.681229\n",
      "Epoch 87, loss: 627.479573\n",
      "Epoch 88, loss: 627.473095\n",
      "Epoch 89, loss: 627.323549\n",
      "Epoch 90, loss: 627.130342\n",
      "Epoch 91, loss: 627.029396\n",
      "Epoch 92, loss: 626.937814\n",
      "Epoch 93, loss: 626.765038\n",
      "Epoch 94, loss: 626.404791\n",
      "Epoch 95, loss: 626.664362\n",
      "Epoch 96, loss: 626.331212\n",
      "Epoch 97, loss: 626.190783\n",
      "Epoch 98, loss: 626.162671\n",
      "Epoch 99, loss: 625.952974\n",
      "Epoch 100, loss: 625.972645\n",
      "Epoch 101, loss: 625.806384\n",
      "Epoch 102, loss: 625.481170\n",
      "Epoch 103, loss: 625.702872\n",
      "Epoch 104, loss: 625.368793\n",
      "Epoch 105, loss: 625.160156\n",
      "Epoch 106, loss: 625.210001\n",
      "Epoch 107, loss: 625.102745\n",
      "Epoch 108, loss: 624.843980\n",
      "Epoch 109, loss: 624.855094\n",
      "Epoch 110, loss: 624.701953\n",
      "Epoch 111, loss: 624.716725\n",
      "Epoch 112, loss: 624.472495\n",
      "Epoch 113, loss: 624.474929\n",
      "Epoch 114, loss: 624.435783\n",
      "Epoch 115, loss: 624.229102\n",
      "Epoch 116, loss: 624.143337\n",
      "Epoch 117, loss: 624.141616\n",
      "Epoch 118, loss: 623.759772\n",
      "Epoch 119, loss: 623.544845\n",
      "Epoch 120, loss: 623.636727\n",
      "Epoch 121, loss: 623.644802\n",
      "Epoch 122, loss: 623.450002\n",
      "Epoch 123, loss: 623.371465\n",
      "Epoch 124, loss: 623.177816\n",
      "Epoch 125, loss: 623.341096\n",
      "Epoch 126, loss: 622.984785\n",
      "Epoch 127, loss: 622.929958\n",
      "Epoch 128, loss: 622.881868\n",
      "Epoch 129, loss: 622.785810\n",
      "Epoch 130, loss: 622.674987\n",
      "Epoch 131, loss: 622.585642\n",
      "Epoch 132, loss: 622.578268\n",
      "Epoch 133, loss: 622.563124\n",
      "Epoch 134, loss: 622.500114\n",
      "Epoch 135, loss: 622.199513\n",
      "Epoch 136, loss: 622.093613\n",
      "Epoch 137, loss: 622.021994\n",
      "Epoch 138, loss: 621.955513\n",
      "Epoch 139, loss: 621.893335\n",
      "Epoch 140, loss: 621.689599\n",
      "Epoch 141, loss: 621.668004\n",
      "Epoch 142, loss: 621.569541\n",
      "Epoch 143, loss: 621.442794\n",
      "Epoch 144, loss: 621.518124\n",
      "Epoch 145, loss: 621.403814\n",
      "Epoch 146, loss: 621.241605\n",
      "Epoch 147, loss: 621.121451\n",
      "Epoch 148, loss: 621.081250\n",
      "Epoch 149, loss: 621.061645\n",
      "Epoch 150, loss: 621.015292\n",
      "Epoch 151, loss: 620.812032\n",
      "Epoch 152, loss: 620.935856\n",
      "Epoch 153, loss: 620.748366\n",
      "Epoch 154, loss: 620.626074\n",
      "Epoch 155, loss: 620.291029\n",
      "Epoch 156, loss: 620.656358\n",
      "Epoch 157, loss: 620.351427\n",
      "Epoch 158, loss: 620.397009\n",
      "Epoch 159, loss: 620.412871\n",
      "Epoch 160, loss: 620.163723\n",
      "Epoch 161, loss: 619.929887\n",
      "Epoch 162, loss: 619.945595\n",
      "Epoch 163, loss: 619.699858\n",
      "Epoch 164, loss: 619.827198\n",
      "Epoch 165, loss: 619.595411\n",
      "Epoch 166, loss: 619.897341\n",
      "Epoch 167, loss: 619.596067\n",
      "Epoch 168, loss: 619.677705\n",
      "Epoch 169, loss: 619.370235\n",
      "Epoch 170, loss: 619.319220\n",
      "Epoch 171, loss: 619.330905\n",
      "Epoch 172, loss: 619.289083\n",
      "Epoch 173, loss: 619.265649\n",
      "Epoch 174, loss: 619.097885\n",
      "Epoch 175, loss: 618.878990\n",
      "Epoch 176, loss: 618.943289\n",
      "Epoch 177, loss: 618.865196\n",
      "Epoch 178, loss: 618.776154\n",
      "Epoch 179, loss: 618.775120\n",
      "Epoch 180, loss: 618.736462\n",
      "Epoch 181, loss: 618.549873\n",
      "Epoch 182, loss: 618.321593\n",
      "Epoch 183, loss: 618.599890\n",
      "Epoch 184, loss: 618.270104\n",
      "Epoch 185, loss: 618.081163\n",
      "Epoch 186, loss: 618.274196\n",
      "Epoch 187, loss: 618.145653\n",
      "Epoch 188, loss: 617.940399\n",
      "Epoch 189, loss: 617.948381\n",
      "Epoch 190, loss: 617.955172\n",
      "Epoch 191, loss: 617.722803\n",
      "Epoch 192, loss: 617.774350\n",
      "Epoch 193, loss: 617.569022\n",
      "Epoch 194, loss: 617.658668\n",
      "Epoch 195, loss: 617.483796\n",
      "Epoch 196, loss: 617.551201\n",
      "Epoch 197, loss: 617.271536\n",
      "Epoch 198, loss: 617.275887\n",
      "Epoch 199, loss: 617.277899\n",
      "This accuracy = 0.247\n",
      "Start learning_rates = 0.0001, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 688.347220\n",
      "Epoch 1, loss: 681.604865\n",
      "Epoch 2, loss: 676.258334\n",
      "Epoch 3, loss: 671.709241\n",
      "Epoch 4, loss: 668.171486\n",
      "Epoch 5, loss: 665.111737\n",
      "Epoch 6, loss: 662.161170\n",
      "Epoch 7, loss: 659.893281\n",
      "Epoch 8, loss: 657.883790\n",
      "Epoch 9, loss: 656.044099\n",
      "Epoch 10, loss: 654.545473\n",
      "Epoch 11, loss: 653.059044\n",
      "Epoch 12, loss: 651.876702\n",
      "Epoch 13, loss: 650.779239\n",
      "Epoch 14, loss: 649.670985\n",
      "Epoch 15, loss: 648.989541\n",
      "Epoch 16, loss: 648.020623\n",
      "Epoch 17, loss: 647.162859\n",
      "Epoch 18, loss: 646.616934\n",
      "Epoch 19, loss: 645.700437\n",
      "Epoch 20, loss: 645.066328\n",
      "Epoch 21, loss: 644.601942\n",
      "Epoch 22, loss: 643.826112\n",
      "Epoch 23, loss: 643.339800\n",
      "Epoch 24, loss: 642.895433\n",
      "Epoch 25, loss: 642.350897\n",
      "Epoch 26, loss: 641.863954\n",
      "Epoch 27, loss: 641.361879\n",
      "Epoch 28, loss: 640.835024\n",
      "Epoch 29, loss: 640.642414\n",
      "Epoch 30, loss: 640.005917\n",
      "Epoch 31, loss: 639.777268\n",
      "Epoch 32, loss: 639.292170\n",
      "Epoch 33, loss: 639.029965\n",
      "Epoch 34, loss: 638.594407\n",
      "Epoch 35, loss: 638.199110\n",
      "Epoch 36, loss: 637.911423\n",
      "Epoch 37, loss: 637.717357\n",
      "Epoch 38, loss: 637.285359\n",
      "Epoch 39, loss: 636.811262\n",
      "Epoch 40, loss: 636.728009\n",
      "Epoch 41, loss: 636.309445\n",
      "Epoch 42, loss: 636.054743\n",
      "Epoch 43, loss: 635.975689\n",
      "Epoch 44, loss: 635.363778\n",
      "Epoch 45, loss: 634.916728\n",
      "Epoch 46, loss: 635.120485\n",
      "Epoch 47, loss: 634.771431\n",
      "Epoch 48, loss: 634.551220\n",
      "Epoch 49, loss: 634.196090\n",
      "Epoch 50, loss: 634.056463\n",
      "Epoch 51, loss: 633.820848\n",
      "Epoch 52, loss: 633.385444\n",
      "Epoch 53, loss: 633.350865\n",
      "Epoch 54, loss: 632.984001\n",
      "Epoch 55, loss: 633.016173\n",
      "Epoch 56, loss: 632.607135\n",
      "Epoch 57, loss: 632.620132\n",
      "Epoch 58, loss: 632.207767\n",
      "Epoch 59, loss: 632.236508\n",
      "Epoch 60, loss: 631.868809\n",
      "Epoch 61, loss: 631.788683\n",
      "Epoch 62, loss: 631.679532\n",
      "Epoch 63, loss: 631.349916\n",
      "Epoch 64, loss: 631.170921\n",
      "Epoch 65, loss: 630.838208\n",
      "Epoch 66, loss: 630.769255\n",
      "Epoch 67, loss: 630.634860\n",
      "Epoch 68, loss: 630.459983\n",
      "Epoch 69, loss: 630.251240\n",
      "Epoch 70, loss: 630.090084\n",
      "Epoch 71, loss: 629.827777\n",
      "Epoch 72, loss: 629.801572\n",
      "Epoch 73, loss: 629.627687\n",
      "Epoch 74, loss: 629.520384\n",
      "Epoch 75, loss: 629.251916\n",
      "Epoch 76, loss: 629.178958\n",
      "Epoch 77, loss: 628.917933\n",
      "Epoch 78, loss: 628.601685\n",
      "Epoch 79, loss: 628.640311\n",
      "Epoch 80, loss: 628.522630\n",
      "Epoch 81, loss: 628.335326\n",
      "Epoch 82, loss: 628.230593\n",
      "Epoch 83, loss: 628.166596\n",
      "Epoch 84, loss: 627.886272\n",
      "Epoch 85, loss: 627.727084\n",
      "Epoch 86, loss: 627.657620\n",
      "Epoch 87, loss: 627.658234\n",
      "Epoch 88, loss: 627.433622\n",
      "Epoch 89, loss: 627.338199\n",
      "Epoch 90, loss: 627.175589\n",
      "Epoch 91, loss: 627.156752\n",
      "Epoch 92, loss: 626.881291\n",
      "Epoch 93, loss: 626.871926\n",
      "Epoch 94, loss: 626.442923\n",
      "Epoch 95, loss: 626.450855\n",
      "Epoch 96, loss: 626.466734\n",
      "Epoch 97, loss: 626.264508\n",
      "Epoch 98, loss: 626.083731\n",
      "Epoch 99, loss: 625.965506\n",
      "Epoch 100, loss: 625.949791\n",
      "Epoch 101, loss: 625.823985\n",
      "Epoch 102, loss: 625.647305\n",
      "Epoch 103, loss: 625.467115\n",
      "Epoch 104, loss: 625.471811\n",
      "Epoch 105, loss: 625.209323\n",
      "Epoch 106, loss: 625.180551\n",
      "Epoch 107, loss: 624.969841\n",
      "Epoch 108, loss: 624.923167\n",
      "Epoch 109, loss: 625.031158\n",
      "Epoch 110, loss: 624.668888\n",
      "Epoch 111, loss: 624.748725\n",
      "Epoch 112, loss: 624.383753\n",
      "Epoch 113, loss: 624.336904\n",
      "Epoch 114, loss: 624.457606\n",
      "Epoch 115, loss: 624.259921\n",
      "Epoch 116, loss: 623.998590\n",
      "Epoch 117, loss: 623.656811\n",
      "Epoch 118, loss: 623.874680\n",
      "Epoch 119, loss: 623.874700\n",
      "Epoch 120, loss: 623.590296\n",
      "Epoch 121, loss: 623.742656\n",
      "Epoch 122, loss: 623.380186\n",
      "Epoch 123, loss: 623.232119\n",
      "Epoch 124, loss: 623.302097\n",
      "Epoch 125, loss: 623.310820\n",
      "Epoch 126, loss: 623.044125\n",
      "Epoch 127, loss: 623.017229\n",
      "Epoch 128, loss: 622.924725\n",
      "Epoch 129, loss: 622.873076\n",
      "Epoch 130, loss: 622.755551\n",
      "Epoch 131, loss: 622.516695\n",
      "Epoch 132, loss: 622.568582\n",
      "Epoch 133, loss: 622.494430\n",
      "Epoch 134, loss: 622.403516\n",
      "Epoch 135, loss: 622.385010\n",
      "Epoch 136, loss: 622.287004\n",
      "Epoch 137, loss: 622.247403\n",
      "Epoch 138, loss: 621.829375\n",
      "Epoch 139, loss: 621.942200\n",
      "Epoch 140, loss: 621.851941\n",
      "Epoch 141, loss: 621.591080\n",
      "Epoch 142, loss: 621.602886\n",
      "Epoch 143, loss: 621.623959\n",
      "Epoch 144, loss: 621.422894\n",
      "Epoch 145, loss: 621.283666\n",
      "Epoch 146, loss: 621.127546\n",
      "Epoch 147, loss: 621.136923\n",
      "Epoch 148, loss: 620.991275\n",
      "Epoch 149, loss: 620.989396\n",
      "Epoch 150, loss: 620.858470\n",
      "Epoch 151, loss: 620.922065\n",
      "Epoch 152, loss: 620.874132\n",
      "Epoch 153, loss: 620.422667\n",
      "Epoch 154, loss: 620.371859\n",
      "Epoch 155, loss: 620.744464\n",
      "Epoch 156, loss: 620.558963\n",
      "Epoch 157, loss: 620.321390\n",
      "Epoch 158, loss: 620.402208\n",
      "Epoch 159, loss: 620.155775\n",
      "Epoch 160, loss: 620.213277\n",
      "Epoch 161, loss: 620.069168\n",
      "Epoch 162, loss: 619.781967\n",
      "Epoch 163, loss: 619.893297\n",
      "Epoch 164, loss: 619.677173\n",
      "Epoch 165, loss: 619.652024\n",
      "Epoch 166, loss: 619.606489\n",
      "Epoch 167, loss: 619.554850\n",
      "Epoch 168, loss: 619.361034\n",
      "Epoch 169, loss: 619.452559\n",
      "Epoch 170, loss: 619.270115\n",
      "Epoch 171, loss: 619.357160\n",
      "Epoch 172, loss: 619.379598\n",
      "Epoch 173, loss: 619.125360\n",
      "Epoch 174, loss: 618.888501\n",
      "Epoch 175, loss: 618.853335\n",
      "Epoch 176, loss: 618.733098\n",
      "Epoch 177, loss: 618.705286\n",
      "Epoch 178, loss: 618.672864\n",
      "Epoch 179, loss: 618.507399\n",
      "Epoch 180, loss: 618.731419\n",
      "Epoch 181, loss: 618.508646\n",
      "Epoch 182, loss: 618.518040\n",
      "Epoch 183, loss: 618.513316\n",
      "Epoch 184, loss: 618.435155\n",
      "Epoch 185, loss: 618.454539\n",
      "Epoch 186, loss: 618.084769\n",
      "Epoch 187, loss: 618.233342\n",
      "Epoch 188, loss: 618.222070\n",
      "Epoch 189, loss: 618.015068\n",
      "Epoch 190, loss: 617.886331\n",
      "Epoch 191, loss: 617.859249\n",
      "Epoch 192, loss: 617.855314\n",
      "Epoch 193, loss: 617.728486\n",
      "Epoch 194, loss: 617.723381\n",
      "Epoch 195, loss: 617.441106\n",
      "Epoch 196, loss: 617.444198\n",
      "Epoch 197, loss: 617.364862\n",
      "Epoch 198, loss: 617.336235\n",
      "Epoch 199, loss: 617.347155\n",
      "This accuracy = 0.244\n",
      "Start learning_rates = 0.0001, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 688.170259\n",
      "Epoch 1, loss: 681.505088\n",
      "Epoch 2, loss: 676.476285\n",
      "Epoch 3, loss: 671.887428\n",
      "Epoch 4, loss: 668.083386\n",
      "Epoch 5, loss: 665.019209\n",
      "Epoch 6, loss: 662.215979\n",
      "Epoch 7, loss: 659.917858\n",
      "Epoch 8, loss: 657.941243\n",
      "Epoch 9, loss: 655.938417\n",
      "Epoch 10, loss: 654.692812\n",
      "Epoch 11, loss: 653.176154\n",
      "Epoch 12, loss: 652.005909\n",
      "Epoch 13, loss: 650.655823\n",
      "Epoch 14, loss: 649.875649\n",
      "Epoch 15, loss: 648.687129\n",
      "Epoch 16, loss: 648.093479\n",
      "Epoch 17, loss: 647.241320\n",
      "Epoch 18, loss: 646.419916\n",
      "Epoch 19, loss: 645.610390\n",
      "Epoch 20, loss: 645.122923\n",
      "Epoch 21, loss: 644.475065\n",
      "Epoch 22, loss: 643.839120\n",
      "Epoch 23, loss: 643.478894\n",
      "Epoch 24, loss: 642.732961\n",
      "Epoch 25, loss: 642.378153\n",
      "Epoch 26, loss: 641.755033\n",
      "Epoch 27, loss: 641.483267\n",
      "Epoch 28, loss: 641.003110\n",
      "Epoch 29, loss: 640.592177\n",
      "Epoch 30, loss: 640.279462\n",
      "Epoch 31, loss: 639.668364\n",
      "Epoch 32, loss: 639.363851\n",
      "Epoch 33, loss: 639.077557\n",
      "Epoch 34, loss: 638.600730\n",
      "Epoch 35, loss: 638.288375\n",
      "Epoch 36, loss: 637.883829\n",
      "Epoch 37, loss: 637.732847\n",
      "Epoch 38, loss: 637.374897\n",
      "Epoch 39, loss: 636.950472\n",
      "Epoch 40, loss: 636.618226\n",
      "Epoch 41, loss: 636.406610\n",
      "Epoch 42, loss: 635.963193\n",
      "Epoch 43, loss: 635.761694\n",
      "Epoch 44, loss: 635.646922\n",
      "Epoch 45, loss: 635.166382\n",
      "Epoch 46, loss: 635.082273\n",
      "Epoch 47, loss: 635.053962\n",
      "Epoch 48, loss: 634.557104\n",
      "Epoch 49, loss: 634.199935\n",
      "Epoch 50, loss: 633.801098\n",
      "Epoch 51, loss: 633.814126\n",
      "Epoch 52, loss: 633.573433\n",
      "Epoch 53, loss: 633.566804\n",
      "Epoch 54, loss: 632.991910\n",
      "Epoch 55, loss: 633.000989\n",
      "Epoch 56, loss: 632.610583\n",
      "Epoch 57, loss: 632.657130\n",
      "Epoch 58, loss: 632.088608\n",
      "Epoch 59, loss: 632.197043\n",
      "Epoch 60, loss: 631.927993\n",
      "Epoch 61, loss: 631.850650\n",
      "Epoch 62, loss: 631.510629\n",
      "Epoch 63, loss: 631.377297\n",
      "Epoch 64, loss: 630.883194\n",
      "Epoch 65, loss: 631.120359\n",
      "Epoch 66, loss: 630.876179\n",
      "Epoch 67, loss: 630.458684\n",
      "Epoch 68, loss: 630.420485\n",
      "Epoch 69, loss: 630.176857\n",
      "Epoch 70, loss: 630.255376\n",
      "Epoch 71, loss: 629.788620\n",
      "Epoch 72, loss: 629.706454\n",
      "Epoch 73, loss: 629.691135\n",
      "Epoch 74, loss: 629.372623\n",
      "Epoch 75, loss: 629.170328\n",
      "Epoch 76, loss: 629.025662\n",
      "Epoch 77, loss: 628.815558\n",
      "Epoch 78, loss: 628.840509\n",
      "Epoch 79, loss: 628.673509\n",
      "Epoch 80, loss: 628.667445\n",
      "Epoch 81, loss: 628.486981\n",
      "Epoch 82, loss: 628.275430\n",
      "Epoch 83, loss: 628.062508\n",
      "Epoch 84, loss: 628.117403\n",
      "Epoch 85, loss: 627.853853\n",
      "Epoch 86, loss: 627.547960\n",
      "Epoch 87, loss: 627.618446\n",
      "Epoch 88, loss: 627.281806\n",
      "Epoch 89, loss: 627.213454\n",
      "Epoch 90, loss: 627.341250\n",
      "Epoch 91, loss: 627.015573\n",
      "Epoch 92, loss: 627.017447\n",
      "Epoch 93, loss: 626.700992\n",
      "Epoch 94, loss: 626.770610\n",
      "Epoch 95, loss: 626.463228\n",
      "Epoch 96, loss: 626.503656\n",
      "Epoch 97, loss: 626.412717\n",
      "Epoch 98, loss: 626.133036\n",
      "Epoch 99, loss: 625.939038\n",
      "Epoch 100, loss: 625.834378\n",
      "Epoch 101, loss: 625.702230\n",
      "Epoch 102, loss: 625.722837\n",
      "Epoch 103, loss: 625.595581\n",
      "Epoch 104, loss: 625.650160\n",
      "Epoch 105, loss: 625.408836\n",
      "Epoch 106, loss: 625.152692\n",
      "Epoch 107, loss: 625.178790\n",
      "Epoch 108, loss: 624.993348\n",
      "Epoch 109, loss: 624.878691\n",
      "Epoch 110, loss: 624.741193\n",
      "Epoch 111, loss: 624.537414\n",
      "Epoch 112, loss: 624.700851\n",
      "Epoch 113, loss: 624.511111\n",
      "Epoch 114, loss: 624.282211\n",
      "Epoch 115, loss: 624.253220\n",
      "Epoch 116, loss: 624.090096\n",
      "Epoch 117, loss: 623.977886\n",
      "Epoch 118, loss: 623.968671\n",
      "Epoch 119, loss: 623.653014\n",
      "Epoch 120, loss: 623.568636\n",
      "Epoch 121, loss: 623.463707\n",
      "Epoch 122, loss: 623.534760\n",
      "Epoch 123, loss: 623.202763\n",
      "Epoch 124, loss: 623.239905\n",
      "Epoch 125, loss: 623.168531\n",
      "Epoch 126, loss: 623.097317\n",
      "Epoch 127, loss: 622.758321\n",
      "Epoch 128, loss: 622.936899\n",
      "Epoch 129, loss: 622.869182\n",
      "Epoch 130, loss: 622.490261\n",
      "Epoch 131, loss: 622.692201\n",
      "Epoch 132, loss: 622.481953\n",
      "Epoch 133, loss: 622.425578\n",
      "Epoch 134, loss: 622.510851\n",
      "Epoch 135, loss: 622.125943\n",
      "Epoch 136, loss: 622.020723\n",
      "Epoch 137, loss: 622.178033\n",
      "Epoch 138, loss: 621.891950\n",
      "Epoch 139, loss: 621.756396\n",
      "Epoch 140, loss: 621.737211\n",
      "Epoch 141, loss: 621.762347\n",
      "Epoch 142, loss: 621.620096\n",
      "Epoch 143, loss: 621.544938\n",
      "Epoch 144, loss: 621.324572\n",
      "Epoch 145, loss: 621.349720\n",
      "Epoch 146, loss: 621.123382\n",
      "Epoch 147, loss: 621.177181\n",
      "Epoch 148, loss: 621.212237\n",
      "Epoch 149, loss: 621.131061\n",
      "Epoch 150, loss: 620.870015\n",
      "Epoch 151, loss: 620.926051\n",
      "Epoch 152, loss: 620.748922\n",
      "Epoch 153, loss: 620.556179\n",
      "Epoch 154, loss: 620.667843\n",
      "Epoch 155, loss: 620.591065\n",
      "Epoch 156, loss: 620.338579\n",
      "Epoch 157, loss: 620.299111\n",
      "Epoch 158, loss: 620.140930\n",
      "Epoch 159, loss: 620.045107\n",
      "Epoch 160, loss: 620.196313\n",
      "Epoch 161, loss: 620.022144\n",
      "Epoch 162, loss: 619.916674\n",
      "Epoch 163, loss: 619.853751\n",
      "Epoch 164, loss: 619.859290\n",
      "Epoch 165, loss: 619.738646\n",
      "Epoch 166, loss: 619.488987\n",
      "Epoch 167, loss: 619.778095\n",
      "Epoch 168, loss: 619.406132\n",
      "Epoch 169, loss: 619.248295\n",
      "Epoch 170, loss: 619.388601\n",
      "Epoch 171, loss: 619.189540\n",
      "Epoch 172, loss: 618.978700\n",
      "Epoch 173, loss: 619.199086\n",
      "Epoch 174, loss: 619.139864\n",
      "Epoch 175, loss: 618.906446\n",
      "Epoch 176, loss: 618.814619\n",
      "Epoch 177, loss: 618.797509\n",
      "Epoch 178, loss: 618.694980\n",
      "Epoch 179, loss: 618.571533\n",
      "Epoch 180, loss: 618.487333\n",
      "Epoch 181, loss: 618.631418\n",
      "Epoch 182, loss: 618.402112\n",
      "Epoch 183, loss: 618.248207\n",
      "Epoch 184, loss: 618.176307\n",
      "Epoch 185, loss: 618.352150\n",
      "Epoch 186, loss: 618.193350\n",
      "Epoch 187, loss: 618.085809\n",
      "Epoch 188, loss: 617.953052\n",
      "Epoch 189, loss: 618.066474\n",
      "Epoch 190, loss: 617.789314\n",
      "Epoch 191, loss: 617.965739\n",
      "Epoch 192, loss: 617.791090\n",
      "Epoch 193, loss: 617.798041\n",
      "Epoch 194, loss: 617.518683\n",
      "Epoch 195, loss: 617.375242\n",
      "Epoch 196, loss: 617.417478\n",
      "Epoch 197, loss: 617.357520\n",
      "Epoch 198, loss: 617.222028\n",
      "Epoch 199, loss: 617.403394\n",
      "This accuracy = 0.254\n",
      "Start learning_rates = 1e-05, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 690.495795\n",
      "Epoch 1, loss: 689.634005\n",
      "Epoch 2, loss: 688.835255\n",
      "Epoch 3, loss: 688.053207\n",
      "Epoch 4, loss: 687.308886\n",
      "Epoch 5, loss: 686.579900\n",
      "Epoch 6, loss: 685.873541\n",
      "Epoch 7, loss: 685.170922\n",
      "Epoch 8, loss: 684.479576\n",
      "Epoch 9, loss: 683.851230\n",
      "Epoch 10, loss: 683.174818\n",
      "Epoch 11, loss: 682.546118\n",
      "Epoch 12, loss: 681.934582\n",
      "Epoch 13, loss: 681.313552\n",
      "Epoch 14, loss: 680.715789\n",
      "Epoch 15, loss: 680.161086\n",
      "Epoch 16, loss: 679.556798\n",
      "Epoch 17, loss: 679.018836\n",
      "Epoch 18, loss: 678.448393\n",
      "Epoch 19, loss: 677.928279\n",
      "Epoch 20, loss: 677.380865\n",
      "Epoch 21, loss: 676.850582\n",
      "Epoch 22, loss: 676.360226\n",
      "Epoch 23, loss: 675.845051\n",
      "Epoch 24, loss: 675.339432\n",
      "Epoch 25, loss: 674.866972\n",
      "Epoch 26, loss: 674.404704\n",
      "Epoch 27, loss: 673.927818\n",
      "Epoch 28, loss: 673.487256\n",
      "Epoch 29, loss: 673.030131\n",
      "Epoch 30, loss: 672.590057\n",
      "Epoch 31, loss: 672.146893\n",
      "Epoch 32, loss: 671.747680\n",
      "Epoch 33, loss: 671.308367\n",
      "Epoch 34, loss: 670.900428\n",
      "Epoch 35, loss: 670.489808\n",
      "Epoch 36, loss: 670.088002\n",
      "Epoch 37, loss: 669.719890\n",
      "Epoch 38, loss: 669.307348\n",
      "Epoch 39, loss: 668.934685\n",
      "Epoch 40, loss: 668.565832\n",
      "Epoch 41, loss: 668.191489\n",
      "Epoch 42, loss: 667.841787\n",
      "Epoch 43, loss: 667.464726\n",
      "Epoch 44, loss: 667.134758\n",
      "Epoch 45, loss: 666.804954\n",
      "Epoch 46, loss: 666.464906\n",
      "Epoch 47, loss: 666.127611\n",
      "Epoch 48, loss: 665.787281\n",
      "Epoch 49, loss: 665.480746\n",
      "Epoch 50, loss: 665.159692\n",
      "Epoch 51, loss: 664.846745\n",
      "Epoch 52, loss: 664.545399\n",
      "Epoch 53, loss: 664.235277\n",
      "Epoch 54, loss: 663.956151\n",
      "Epoch 55, loss: 663.651250\n",
      "Epoch 56, loss: 663.357781\n",
      "Epoch 57, loss: 663.078795\n",
      "Epoch 58, loss: 662.797615\n",
      "Epoch 59, loss: 662.532077\n",
      "Epoch 60, loss: 662.265771\n",
      "Epoch 61, loss: 661.998152\n",
      "Epoch 62, loss: 661.742100\n",
      "Epoch 63, loss: 661.483648\n",
      "Epoch 64, loss: 661.234032\n",
      "Epoch 65, loss: 660.979340\n",
      "Epoch 66, loss: 660.718390\n",
      "Epoch 67, loss: 660.499059\n",
      "Epoch 68, loss: 660.242257\n",
      "Epoch 69, loss: 660.015438\n",
      "Epoch 70, loss: 659.776801\n",
      "Epoch 71, loss: 659.540654\n",
      "Epoch 72, loss: 659.333556\n",
      "Epoch 73, loss: 659.089227\n",
      "Epoch 74, loss: 658.885107\n",
      "Epoch 75, loss: 658.655961\n",
      "Epoch 76, loss: 658.473169\n",
      "Epoch 77, loss: 658.252685\n",
      "Epoch 78, loss: 658.030953\n",
      "Epoch 79, loss: 657.845478\n",
      "Epoch 80, loss: 657.620971\n",
      "Epoch 81, loss: 657.436894\n",
      "Epoch 82, loss: 657.234838\n",
      "Epoch 83, loss: 657.030476\n",
      "Epoch 84, loss: 656.856695\n",
      "Epoch 85, loss: 656.658753\n",
      "Epoch 86, loss: 656.471217\n",
      "Epoch 87, loss: 656.308913\n",
      "Epoch 88, loss: 656.127192\n",
      "Epoch 89, loss: 655.921218\n",
      "Epoch 90, loss: 655.760517\n",
      "Epoch 91, loss: 655.577127\n",
      "Epoch 92, loss: 655.434325\n",
      "Epoch 93, loss: 655.261264\n",
      "Epoch 94, loss: 655.094088\n",
      "Epoch 95, loss: 654.919564\n",
      "Epoch 96, loss: 654.763833\n",
      "Epoch 97, loss: 654.591734\n",
      "Epoch 98, loss: 654.439595\n",
      "Epoch 99, loss: 654.281125\n",
      "Epoch 100, loss: 654.140427\n",
      "Epoch 101, loss: 653.965457\n",
      "Epoch 102, loss: 653.826111\n",
      "Epoch 103, loss: 653.680740\n",
      "Epoch 104, loss: 653.524290\n",
      "Epoch 105, loss: 653.374269\n",
      "Epoch 106, loss: 653.231085\n",
      "Epoch 107, loss: 653.084688\n",
      "Epoch 108, loss: 652.952704\n",
      "Epoch 109, loss: 652.814157\n",
      "Epoch 110, loss: 652.675892\n",
      "Epoch 111, loss: 652.554442\n",
      "Epoch 112, loss: 652.397319\n",
      "Epoch 113, loss: 652.265596\n",
      "Epoch 114, loss: 652.144134\n",
      "Epoch 115, loss: 652.015125\n",
      "Epoch 116, loss: 651.890741\n",
      "Epoch 117, loss: 651.739573\n",
      "Epoch 118, loss: 651.632855\n",
      "Epoch 119, loss: 651.502124\n",
      "Epoch 120, loss: 651.381093\n",
      "Epoch 121, loss: 651.259195\n",
      "Epoch 122, loss: 651.137624\n",
      "Epoch 123, loss: 651.008695\n",
      "Epoch 124, loss: 650.894144\n",
      "Epoch 125, loss: 650.768399\n",
      "Epoch 126, loss: 650.672368\n",
      "Epoch 127, loss: 650.544797\n",
      "Epoch 128, loss: 650.436395\n",
      "Epoch 129, loss: 650.335421\n",
      "Epoch 130, loss: 650.219106\n",
      "Epoch 131, loss: 650.104978\n",
      "Epoch 132, loss: 649.982200\n",
      "Epoch 133, loss: 649.892789\n",
      "Epoch 134, loss: 649.772972\n",
      "Epoch 135, loss: 649.658971\n",
      "Epoch 136, loss: 649.579552\n",
      "Epoch 137, loss: 649.459923\n",
      "Epoch 138, loss: 649.351340\n",
      "Epoch 139, loss: 649.248511\n",
      "Epoch 140, loss: 649.140653\n",
      "Epoch 141, loss: 649.065186\n",
      "Epoch 142, loss: 648.950277\n",
      "Epoch 143, loss: 648.870325\n",
      "Epoch 144, loss: 648.763040\n",
      "Epoch 145, loss: 648.670009\n",
      "Epoch 146, loss: 648.567719\n",
      "Epoch 147, loss: 648.469051\n",
      "Epoch 148, loss: 648.376067\n",
      "Epoch 149, loss: 648.271853\n",
      "Epoch 150, loss: 648.185619\n",
      "Epoch 151, loss: 648.101786\n",
      "Epoch 152, loss: 647.990506\n",
      "Epoch 153, loss: 647.911528\n",
      "Epoch 154, loss: 647.823020\n",
      "Epoch 155, loss: 647.737965\n",
      "Epoch 156, loss: 647.647465\n",
      "Epoch 157, loss: 647.569659\n",
      "Epoch 158, loss: 647.465583\n",
      "Epoch 159, loss: 647.394713\n",
      "Epoch 160, loss: 647.304228\n",
      "Epoch 161, loss: 647.231967\n",
      "Epoch 162, loss: 647.138844\n",
      "Epoch 163, loss: 647.034413\n",
      "Epoch 164, loss: 646.962221\n",
      "Epoch 165, loss: 646.891324\n",
      "Epoch 166, loss: 646.795023\n",
      "Epoch 167, loss: 646.711523\n",
      "Epoch 168, loss: 646.633782\n",
      "Epoch 169, loss: 646.574508\n",
      "Epoch 170, loss: 646.477890\n",
      "Epoch 171, loss: 646.417197\n",
      "Epoch 172, loss: 646.316262\n",
      "Epoch 173, loss: 646.255712\n",
      "Epoch 174, loss: 646.174136\n",
      "Epoch 175, loss: 646.082765\n",
      "Epoch 176, loss: 646.016726\n",
      "Epoch 177, loss: 645.947281\n",
      "Epoch 178, loss: 645.871952\n",
      "Epoch 179, loss: 645.786613\n",
      "Epoch 180, loss: 645.731492\n",
      "Epoch 181, loss: 645.653957\n",
      "Epoch 182, loss: 645.553176\n",
      "Epoch 183, loss: 645.505132\n",
      "Epoch 184, loss: 645.427112\n",
      "Epoch 185, loss: 645.340943\n",
      "Epoch 186, loss: 645.285554\n",
      "Epoch 187, loss: 645.211597\n",
      "Epoch 188, loss: 645.146539\n",
      "Epoch 189, loss: 645.062591\n",
      "Epoch 190, loss: 645.013114\n",
      "Epoch 191, loss: 644.933941\n",
      "Epoch 192, loss: 644.869575\n",
      "Epoch 193, loss: 644.799031\n",
      "Epoch 194, loss: 644.730515\n",
      "Epoch 195, loss: 644.669475\n",
      "Epoch 196, loss: 644.602185\n",
      "Epoch 197, loss: 644.530227\n",
      "Epoch 198, loss: 644.465918\n",
      "Epoch 199, loss: 644.395310\n",
      "This accuracy = 0.231\n",
      "Start learning_rates = 1e-05, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 690.481198\n",
      "Epoch 1, loss: 689.615207\n",
      "Epoch 2, loss: 688.800395\n",
      "Epoch 3, loss: 688.030354\n",
      "Epoch 4, loss: 687.280420\n",
      "Epoch 5, loss: 686.569880\n",
      "Epoch 6, loss: 685.834132\n",
      "Epoch 7, loss: 685.164488\n",
      "Epoch 8, loss: 684.466169\n",
      "Epoch 9, loss: 683.817242\n",
      "Epoch 10, loss: 683.157419\n",
      "Epoch 11, loss: 682.526957\n",
      "Epoch 12, loss: 681.894483\n",
      "Epoch 13, loss: 681.287182\n",
      "Epoch 14, loss: 680.683980\n",
      "Epoch 15, loss: 680.108917\n",
      "Epoch 16, loss: 679.544359\n",
      "Epoch 17, loss: 678.988087\n",
      "Epoch 18, loss: 678.435359\n",
      "Epoch 19, loss: 677.875361\n",
      "Epoch 20, loss: 677.354956\n",
      "Epoch 21, loss: 676.839674\n",
      "Epoch 22, loss: 676.323980\n",
      "Epoch 23, loss: 675.814832\n",
      "Epoch 24, loss: 675.335067\n",
      "Epoch 25, loss: 674.854279\n",
      "Epoch 26, loss: 674.375842\n",
      "Epoch 27, loss: 673.902379\n",
      "Epoch 28, loss: 673.446964\n",
      "Epoch 29, loss: 672.999997\n",
      "Epoch 30, loss: 672.556356\n",
      "Epoch 31, loss: 672.128368\n",
      "Epoch 32, loss: 671.706197\n",
      "Epoch 33, loss: 671.277274\n",
      "Epoch 34, loss: 670.869185\n",
      "Epoch 35, loss: 670.450340\n",
      "Epoch 36, loss: 670.051988\n",
      "Epoch 37, loss: 669.667769\n",
      "Epoch 38, loss: 669.290300\n",
      "Epoch 39, loss: 668.903323\n",
      "Epoch 40, loss: 668.532222\n",
      "Epoch 41, loss: 668.163066\n",
      "Epoch 42, loss: 667.811284\n",
      "Epoch 43, loss: 667.449183\n",
      "Epoch 44, loss: 667.106742\n",
      "Epoch 45, loss: 666.766639\n",
      "Epoch 46, loss: 666.414725\n",
      "Epoch 47, loss: 666.101149\n",
      "Epoch 48, loss: 665.749973\n",
      "Epoch 49, loss: 665.452366\n",
      "Epoch 50, loss: 665.119177\n",
      "Epoch 51, loss: 664.825731\n",
      "Epoch 52, loss: 664.519573\n",
      "Epoch 53, loss: 664.219538\n",
      "Epoch 54, loss: 663.907971\n",
      "Epoch 55, loss: 663.628549\n",
      "Epoch 56, loss: 663.331914\n",
      "Epoch 57, loss: 663.043493\n",
      "Epoch 58, loss: 662.785125\n",
      "Epoch 59, loss: 662.509669\n",
      "Epoch 60, loss: 662.227990\n",
      "Epoch 61, loss: 661.956193\n",
      "Epoch 62, loss: 661.720654\n",
      "Epoch 63, loss: 661.442923\n",
      "Epoch 64, loss: 661.191068\n",
      "Epoch 65, loss: 660.932934\n",
      "Epoch 66, loss: 660.692689\n",
      "Epoch 67, loss: 660.453288\n",
      "Epoch 68, loss: 660.197891\n",
      "Epoch 69, loss: 659.975364\n",
      "Epoch 70, loss: 659.746887\n",
      "Epoch 71, loss: 659.508116\n",
      "Epoch 72, loss: 659.278613\n",
      "Epoch 73, loss: 659.062381\n",
      "Epoch 74, loss: 658.851469\n",
      "Epoch 75, loss: 658.636329\n",
      "Epoch 76, loss: 658.440191\n",
      "Epoch 77, loss: 658.211631\n",
      "Epoch 78, loss: 658.014659\n",
      "Epoch 79, loss: 657.819224\n",
      "Epoch 80, loss: 657.598844\n",
      "Epoch 81, loss: 657.388988\n",
      "Epoch 82, loss: 657.192519\n",
      "Epoch 83, loss: 657.005943\n",
      "Epoch 84, loss: 656.824423\n",
      "Epoch 85, loss: 656.635523\n",
      "Epoch 86, loss: 656.434238\n",
      "Epoch 87, loss: 656.246286\n",
      "Epoch 88, loss: 656.092567\n",
      "Epoch 89, loss: 655.897061\n",
      "Epoch 90, loss: 655.719654\n",
      "Epoch 91, loss: 655.564550\n",
      "Epoch 92, loss: 655.391425\n",
      "Epoch 93, loss: 655.209700\n",
      "Epoch 94, loss: 655.040554\n",
      "Epoch 95, loss: 654.872049\n",
      "Epoch 96, loss: 654.715665\n",
      "Epoch 97, loss: 654.550937\n",
      "Epoch 98, loss: 654.395075\n",
      "Epoch 99, loss: 654.248409\n",
      "Epoch 100, loss: 654.098662\n",
      "Epoch 101, loss: 653.920530\n",
      "Epoch 102, loss: 653.783260\n",
      "Epoch 103, loss: 653.637495\n",
      "Epoch 104, loss: 653.480977\n",
      "Epoch 105, loss: 653.344824\n",
      "Epoch 106, loss: 653.203249\n",
      "Epoch 107, loss: 653.068757\n",
      "Epoch 108, loss: 652.917859\n",
      "Epoch 109, loss: 652.769765\n",
      "Epoch 110, loss: 652.646013\n",
      "Epoch 111, loss: 652.508469\n",
      "Epoch 112, loss: 652.349593\n",
      "Epoch 113, loss: 652.228718\n",
      "Epoch 114, loss: 652.108585\n",
      "Epoch 115, loss: 651.975647\n",
      "Epoch 116, loss: 651.846829\n",
      "Epoch 117, loss: 651.699975\n",
      "Epoch 118, loss: 651.580818\n",
      "Epoch 119, loss: 651.474530\n",
      "Epoch 120, loss: 651.333143\n",
      "Epoch 121, loss: 651.212737\n",
      "Epoch 122, loss: 651.102067\n",
      "Epoch 123, loss: 650.968402\n",
      "Epoch 124, loss: 650.868258\n",
      "Epoch 125, loss: 650.734338\n",
      "Epoch 126, loss: 650.634853\n",
      "Epoch 127, loss: 650.517699\n",
      "Epoch 128, loss: 650.403479\n",
      "Epoch 129, loss: 650.280547\n",
      "Epoch 130, loss: 650.168272\n",
      "Epoch 131, loss: 650.059739\n",
      "Epoch 132, loss: 649.941910\n",
      "Epoch 133, loss: 649.844601\n",
      "Epoch 134, loss: 649.749628\n",
      "Epoch 135, loss: 649.638147\n",
      "Epoch 136, loss: 649.520162\n",
      "Epoch 137, loss: 649.428866\n",
      "Epoch 138, loss: 649.308472\n",
      "Epoch 139, loss: 649.218290\n",
      "Epoch 140, loss: 649.116806\n",
      "Epoch 141, loss: 649.020048\n",
      "Epoch 142, loss: 648.912930\n",
      "Epoch 143, loss: 648.803169\n",
      "Epoch 144, loss: 648.712202\n",
      "Epoch 145, loss: 648.625907\n",
      "Epoch 146, loss: 648.520630\n",
      "Epoch 147, loss: 648.431659\n",
      "Epoch 148, loss: 648.346856\n",
      "Epoch 149, loss: 648.264617\n",
      "Epoch 150, loss: 648.162013\n",
      "Epoch 151, loss: 648.067492\n",
      "Epoch 152, loss: 647.973407\n",
      "Epoch 153, loss: 647.901213\n",
      "Epoch 154, loss: 647.786963\n",
      "Epoch 155, loss: 647.707685\n",
      "Epoch 156, loss: 647.613936\n",
      "Epoch 157, loss: 647.536876\n",
      "Epoch 158, loss: 647.447314\n",
      "Epoch 159, loss: 647.348056\n",
      "Epoch 160, loss: 647.261469\n",
      "Epoch 161, loss: 647.176556\n",
      "Epoch 162, loss: 647.085017\n",
      "Epoch 163, loss: 647.023700\n",
      "Epoch 164, loss: 646.934100\n",
      "Epoch 165, loss: 646.852549\n",
      "Epoch 166, loss: 646.761432\n",
      "Epoch 167, loss: 646.695084\n",
      "Epoch 168, loss: 646.612824\n",
      "Epoch 169, loss: 646.525980\n",
      "Epoch 170, loss: 646.430012\n",
      "Epoch 171, loss: 646.363004\n",
      "Epoch 172, loss: 646.302571\n",
      "Epoch 173, loss: 646.207813\n",
      "Epoch 174, loss: 646.139404\n",
      "Epoch 175, loss: 646.052076\n",
      "Epoch 176, loss: 645.976035\n",
      "Epoch 177, loss: 645.907425\n",
      "Epoch 178, loss: 645.829781\n",
      "Epoch 179, loss: 645.752726\n",
      "Epoch 180, loss: 645.680962\n",
      "Epoch 181, loss: 645.608881\n",
      "Epoch 182, loss: 645.536546\n",
      "Epoch 183, loss: 645.467878\n",
      "Epoch 184, loss: 645.391556\n",
      "Epoch 185, loss: 645.319258\n",
      "Epoch 186, loss: 645.258256\n",
      "Epoch 187, loss: 645.166639\n",
      "Epoch 188, loss: 645.122637\n",
      "Epoch 189, loss: 645.036245\n",
      "Epoch 190, loss: 644.985267\n",
      "Epoch 191, loss: 644.916155\n",
      "Epoch 192, loss: 644.823042\n",
      "Epoch 193, loss: 644.749083\n",
      "Epoch 194, loss: 644.724783\n",
      "Epoch 195, loss: 644.641083\n",
      "Epoch 196, loss: 644.554671\n",
      "Epoch 197, loss: 644.499016\n",
      "Epoch 198, loss: 644.442580\n",
      "Epoch 199, loss: 644.356816\n",
      "This accuracy = 0.232\n",
      "Start learning_rates = 1e-05, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 690.476313\n",
      "Epoch 1, loss: 689.612672\n",
      "Epoch 2, loss: 688.808682\n",
      "Epoch 3, loss: 688.062429\n",
      "Epoch 4, loss: 687.297677\n",
      "Epoch 5, loss: 686.572722\n",
      "Epoch 6, loss: 685.868363\n",
      "Epoch 7, loss: 685.156660\n",
      "Epoch 8, loss: 684.488914\n",
      "Epoch 9, loss: 683.823026\n",
      "Epoch 10, loss: 683.185073\n",
      "Epoch 11, loss: 682.547915\n",
      "Epoch 12, loss: 681.927315\n",
      "Epoch 13, loss: 681.319356\n",
      "Epoch 14, loss: 680.732101\n",
      "Epoch 15, loss: 680.135205\n",
      "Epoch 16, loss: 679.582479\n",
      "Epoch 17, loss: 678.990494\n",
      "Epoch 18, loss: 678.438469\n",
      "Epoch 19, loss: 677.910741\n",
      "Epoch 20, loss: 677.387599\n",
      "Epoch 21, loss: 676.856773\n",
      "Epoch 22, loss: 676.358092\n",
      "Epoch 23, loss: 675.840277\n",
      "Epoch 24, loss: 675.348128\n",
      "Epoch 25, loss: 674.878782\n",
      "Epoch 26, loss: 674.402407\n",
      "Epoch 27, loss: 673.928812\n",
      "Epoch 28, loss: 673.470572\n",
      "Epoch 29, loss: 673.025662\n",
      "Epoch 30, loss: 672.582930\n",
      "Epoch 31, loss: 672.161087\n",
      "Epoch 32, loss: 671.730122\n",
      "Epoch 33, loss: 671.316941\n",
      "Epoch 34, loss: 670.890219\n",
      "Epoch 35, loss: 670.486110\n",
      "Epoch 36, loss: 670.081083\n",
      "Epoch 37, loss: 669.696850\n",
      "Epoch 38, loss: 669.309860\n",
      "Epoch 39, loss: 668.913582\n",
      "Epoch 40, loss: 668.550236\n",
      "Epoch 41, loss: 668.183311\n",
      "Epoch 42, loss: 667.831223\n",
      "Epoch 43, loss: 667.462218\n",
      "Epoch 44, loss: 667.124324\n",
      "Epoch 45, loss: 666.783238\n",
      "Epoch 46, loss: 666.443303\n",
      "Epoch 47, loss: 666.116946\n",
      "Epoch 48, loss: 665.785385\n",
      "Epoch 49, loss: 665.456895\n",
      "Epoch 50, loss: 665.145332\n",
      "Epoch 51, loss: 664.831593\n",
      "Epoch 52, loss: 664.519108\n",
      "Epoch 53, loss: 664.217143\n",
      "Epoch 54, loss: 663.934842\n",
      "Epoch 55, loss: 663.653082\n",
      "Epoch 56, loss: 663.355612\n",
      "Epoch 57, loss: 663.072270\n",
      "Epoch 58, loss: 662.799849\n",
      "Epoch 59, loss: 662.521341\n",
      "Epoch 60, loss: 662.258863\n",
      "Epoch 61, loss: 661.978839\n",
      "Epoch 62, loss: 661.720915\n",
      "Epoch 63, loss: 661.471912\n",
      "Epoch 64, loss: 661.205961\n",
      "Epoch 65, loss: 660.954165\n",
      "Epoch 66, loss: 660.697747\n",
      "Epoch 67, loss: 660.466857\n",
      "Epoch 68, loss: 660.224909\n",
      "Epoch 69, loss: 659.996679\n",
      "Epoch 70, loss: 659.759825\n",
      "Epoch 71, loss: 659.541496\n",
      "Epoch 72, loss: 659.294989\n",
      "Epoch 73, loss: 659.072946\n",
      "Epoch 74, loss: 658.873167\n",
      "Epoch 75, loss: 658.642225\n",
      "Epoch 76, loss: 658.431299\n",
      "Epoch 77, loss: 658.239526\n",
      "Epoch 78, loss: 658.013271\n",
      "Epoch 79, loss: 657.799557\n",
      "Epoch 80, loss: 657.625485\n",
      "Epoch 81, loss: 657.414768\n",
      "Epoch 82, loss: 657.212938\n",
      "Epoch 83, loss: 657.020459\n",
      "Epoch 84, loss: 656.830983\n",
      "Epoch 85, loss: 656.646876\n",
      "Epoch 86, loss: 656.475559\n",
      "Epoch 87, loss: 656.271999\n",
      "Epoch 88, loss: 656.106846\n",
      "Epoch 89, loss: 655.929607\n",
      "Epoch 90, loss: 655.743806\n",
      "Epoch 91, loss: 655.563885\n",
      "Epoch 92, loss: 655.400851\n",
      "Epoch 93, loss: 655.235944\n",
      "Epoch 94, loss: 655.077843\n",
      "Epoch 95, loss: 654.892434\n",
      "Epoch 96, loss: 654.738222\n",
      "Epoch 97, loss: 654.581978\n",
      "Epoch 98, loss: 654.418974\n",
      "Epoch 99, loss: 654.264115\n",
      "Epoch 100, loss: 654.107652\n",
      "Epoch 101, loss: 653.938334\n",
      "Epoch 102, loss: 653.805916\n",
      "Epoch 103, loss: 653.655365\n",
      "Epoch 104, loss: 653.503664\n",
      "Epoch 105, loss: 653.356556\n",
      "Epoch 106, loss: 653.227099\n",
      "Epoch 107, loss: 653.072155\n",
      "Epoch 108, loss: 652.927737\n",
      "Epoch 109, loss: 652.793690\n",
      "Epoch 110, loss: 652.647288\n",
      "Epoch 111, loss: 652.523173\n",
      "Epoch 112, loss: 652.384954\n",
      "Epoch 113, loss: 652.253529\n",
      "Epoch 114, loss: 652.129998\n",
      "Epoch 115, loss: 651.992210\n",
      "Epoch 116, loss: 651.855286\n",
      "Epoch 117, loss: 651.734682\n",
      "Epoch 118, loss: 651.611694\n",
      "Epoch 119, loss: 651.486226\n",
      "Epoch 120, loss: 651.348878\n",
      "Epoch 121, loss: 651.233154\n",
      "Epoch 122, loss: 651.105735\n",
      "Epoch 123, loss: 650.991453\n",
      "Epoch 124, loss: 650.870944\n",
      "Epoch 125, loss: 650.754349\n",
      "Epoch 126, loss: 650.648828\n",
      "Epoch 127, loss: 650.520797\n",
      "Epoch 128, loss: 650.403721\n",
      "Epoch 129, loss: 650.307209\n",
      "Epoch 130, loss: 650.174455\n",
      "Epoch 131, loss: 650.086608\n",
      "Epoch 132, loss: 649.971455\n",
      "Epoch 133, loss: 649.858056\n",
      "Epoch 134, loss: 649.745645\n",
      "Epoch 135, loss: 649.637534\n",
      "Epoch 136, loss: 649.558635\n",
      "Epoch 137, loss: 649.431799\n",
      "Epoch 138, loss: 649.338224\n",
      "Epoch 139, loss: 649.244125\n",
      "Epoch 140, loss: 649.121486\n",
      "Epoch 141, loss: 649.037014\n",
      "Epoch 142, loss: 648.951256\n",
      "Epoch 143, loss: 648.835035\n",
      "Epoch 144, loss: 648.730233\n",
      "Epoch 145, loss: 648.640111\n",
      "Epoch 146, loss: 648.538786\n",
      "Epoch 147, loss: 648.462603\n",
      "Epoch 148, loss: 648.377778\n",
      "Epoch 149, loss: 648.254293\n",
      "Epoch 150, loss: 648.170397\n",
      "Epoch 151, loss: 648.074300\n",
      "Epoch 152, loss: 647.990139\n",
      "Epoch 153, loss: 647.893764\n",
      "Epoch 154, loss: 647.804825\n",
      "Epoch 155, loss: 647.719984\n",
      "Epoch 156, loss: 647.637768\n",
      "Epoch 157, loss: 647.529274\n",
      "Epoch 158, loss: 647.440592\n",
      "Epoch 159, loss: 647.365815\n",
      "Epoch 160, loss: 647.265726\n",
      "Epoch 161, loss: 647.197027\n",
      "Epoch 162, loss: 647.113466\n",
      "Epoch 163, loss: 647.027027\n",
      "Epoch 164, loss: 646.946938\n",
      "Epoch 165, loss: 646.858442\n",
      "Epoch 166, loss: 646.775996\n",
      "Epoch 167, loss: 646.698050\n",
      "Epoch 168, loss: 646.625149\n",
      "Epoch 169, loss: 646.553678\n",
      "Epoch 170, loss: 646.460494\n",
      "Epoch 171, loss: 646.383888\n",
      "Epoch 172, loss: 646.291706\n",
      "Epoch 173, loss: 646.231418\n",
      "Epoch 174, loss: 646.146843\n",
      "Epoch 175, loss: 646.089119\n",
      "Epoch 176, loss: 645.980784\n",
      "Epoch 177, loss: 645.918617\n",
      "Epoch 178, loss: 645.834812\n",
      "Epoch 179, loss: 645.771867\n",
      "Epoch 180, loss: 645.708701\n",
      "Epoch 181, loss: 645.640212\n",
      "Epoch 182, loss: 645.549613\n",
      "Epoch 183, loss: 645.484322\n",
      "Epoch 184, loss: 645.420391\n",
      "Epoch 185, loss: 645.349303\n",
      "Epoch 186, loss: 645.259561\n",
      "Epoch 187, loss: 645.188929\n",
      "Epoch 188, loss: 645.135777\n",
      "Epoch 189, loss: 645.059850\n",
      "Epoch 190, loss: 644.985327\n",
      "Epoch 191, loss: 644.920790\n",
      "Epoch 192, loss: 644.851050\n",
      "Epoch 193, loss: 644.774911\n",
      "Epoch 194, loss: 644.703737\n",
      "Epoch 195, loss: 644.661183\n",
      "Epoch 196, loss: 644.575163\n",
      "Epoch 197, loss: 644.514412\n",
      "Epoch 198, loss: 644.447041\n",
      "Epoch 199, loss: 644.383262\n",
      "This accuracy = 0.229\n",
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        print(f'Start learning_rates = {lr}, reg_strengths = {rs}')\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        print(f'This accuracy = {accuracy}')\n",
    "        if (best_val_accuracy is None) or (accuracy > best_val_accuracy):\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
