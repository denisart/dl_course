{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import (load_svhn, random_split_train_val)\n",
    "from gradient_check import (check_gradient)\n",
    "from metrics import (multiclass_accuracy) \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return (train_flat_with_ones, test_flat_with_ones)\n",
    "    \n",
    "(train_X, train_y, test_X, test_y) = load_svhn('data', max_train=10000, max_test=1000)    \n",
    "(train_X, test_X) = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "(train_X, train_y, val_X, val_y) = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return (float(x*x), 2*x)\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return (np.sum(x), np.ones_like(x))\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return (np.sum(x), np.ones_like(x))\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "(loss, grad) = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "(loss, dW) = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.484015\n",
      "Epoch 1, loss: 2.356050\n",
      "Epoch 2, loss: 2.317979\n",
      "Epoch 3, loss: 2.306623\n",
      "Epoch 4, loss: 2.303297\n",
      "Epoch 5, loss: 2.302304\n",
      "Epoch 6, loss: 2.301991\n",
      "Epoch 7, loss: 2.301899\n",
      "Epoch 8, loss: 2.301881\n",
      "Epoch 9, loss: 2.301857\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6fca601850>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301890\n",
      "Epoch 1, loss: 2.301866\n",
      "Epoch 2, loss: 2.301834\n",
      "Epoch 3, loss: 2.301874\n",
      "Epoch 4, loss: 2.301892\n",
      "Epoch 5, loss: 2.301864\n",
      "Epoch 6, loss: 2.301880\n",
      "Epoch 7, loss: 2.301844\n",
      "Epoch 8, loss: 2.301878\n",
      "Epoch 9, loss: 2.301869\n",
      "Epoch 10, loss: 2.301858\n",
      "Epoch 11, loss: 2.301852\n",
      "Epoch 12, loss: 2.301887\n",
      "Epoch 13, loss: 2.301867\n",
      "Epoch 14, loss: 2.301868\n",
      "Epoch 15, loss: 2.301866\n",
      "Epoch 16, loss: 2.301873\n",
      "Epoch 17, loss: 2.301863\n",
      "Epoch 18, loss: 2.301867\n",
      "Epoch 19, loss: 2.301881\n",
      "Epoch 20, loss: 2.301849\n",
      "Epoch 21, loss: 2.301864\n",
      "Epoch 22, loss: 2.301849\n",
      "Epoch 23, loss: 2.301870\n",
      "Epoch 24, loss: 2.301869\n",
      "Epoch 25, loss: 2.301857\n",
      "Epoch 26, loss: 2.301877\n",
      "Epoch 27, loss: 2.301867\n",
      "Epoch 28, loss: 2.301849\n",
      "Epoch 29, loss: 2.301854\n",
      "Epoch 30, loss: 2.301862\n",
      "Epoch 31, loss: 2.301862\n",
      "Epoch 32, loss: 2.301853\n",
      "Epoch 33, loss: 2.301877\n",
      "Epoch 34, loss: 2.301866\n",
      "Epoch 35, loss: 2.301865\n",
      "Epoch 36, loss: 2.301858\n",
      "Epoch 37, loss: 2.301851\n",
      "Epoch 38, loss: 2.301872\n",
      "Epoch 39, loss: 2.301859\n",
      "Epoch 40, loss: 2.301866\n",
      "Epoch 41, loss: 2.301858\n",
      "Epoch 42, loss: 2.301863\n",
      "Epoch 43, loss: 2.301867\n",
      "Epoch 44, loss: 2.301864\n",
      "Epoch 45, loss: 2.301872\n",
      "Epoch 46, loss: 2.301868\n",
      "Epoch 47, loss: 2.301883\n",
      "Epoch 48, loss: 2.301870\n",
      "Epoch 49, loss: 2.301858\n",
      "Epoch 50, loss: 2.301856\n",
      "Epoch 51, loss: 2.301865\n",
      "Epoch 52, loss: 2.301870\n",
      "Epoch 53, loss: 2.301870\n",
      "Epoch 54, loss: 2.301863\n",
      "Epoch 55, loss: 2.301847\n",
      "Epoch 56, loss: 2.301873\n",
      "Epoch 57, loss: 2.301872\n",
      "Epoch 58, loss: 2.301868\n",
      "Epoch 59, loss: 2.301851\n",
      "Epoch 60, loss: 2.301860\n",
      "Epoch 61, loss: 2.301859\n",
      "Epoch 62, loss: 2.301867\n",
      "Epoch 63, loss: 2.301865\n",
      "Epoch 64, loss: 2.301871\n",
      "Epoch 65, loss: 2.301869\n",
      "Epoch 66, loss: 2.301848\n",
      "Epoch 67, loss: 2.301860\n",
      "Epoch 68, loss: 2.301864\n",
      "Epoch 69, loss: 2.301860\n",
      "Epoch 70, loss: 2.301864\n",
      "Epoch 71, loss: 2.301873\n",
      "Epoch 72, loss: 2.301851\n",
      "Epoch 73, loss: 2.301871\n",
      "Epoch 74, loss: 2.301889\n",
      "Epoch 75, loss: 2.301869\n",
      "Epoch 76, loss: 2.301877\n",
      "Epoch 77, loss: 2.301871\n",
      "Epoch 78, loss: 2.301864\n",
      "Epoch 79, loss: 2.301866\n",
      "Epoch 80, loss: 2.301871\n",
      "Epoch 81, loss: 2.301868\n",
      "Epoch 82, loss: 2.301862\n",
      "Epoch 83, loss: 2.301852\n",
      "Epoch 84, loss: 2.301876\n",
      "Epoch 85, loss: 2.301856\n",
      "Epoch 86, loss: 2.301868\n",
      "Epoch 87, loss: 2.301865\n",
      "Epoch 88, loss: 2.301863\n",
      "Epoch 89, loss: 2.301878\n",
      "Epoch 90, loss: 2.301864\n",
      "Epoch 91, loss: 2.301873\n",
      "Epoch 92, loss: 2.301861\n",
      "Epoch 93, loss: 2.301872\n",
      "Epoch 94, loss: 2.301884\n",
      "Epoch 95, loss: 2.301877\n",
      "Epoch 96, loss: 2.301870\n",
      "Epoch 97, loss: 2.301869\n",
      "Epoch 98, loss: 2.301898\n",
      "Epoch 99, loss: 2.301862\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning_rates = 0.001, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 2.301863\n",
      "Epoch 1, loss: 2.300808\n",
      "Epoch 2, loss: 2.299820\n",
      "Epoch 3, loss: 2.298862\n",
      "Epoch 4, loss: 2.297936\n",
      "Epoch 5, loss: 2.297027\n",
      "Epoch 6, loss: 2.296162\n",
      "Epoch 7, loss: 2.295287\n",
      "Epoch 8, loss: 2.294424\n",
      "Epoch 9, loss: 2.293574\n",
      "Epoch 10, loss: 2.292731\n",
      "Epoch 11, loss: 2.291898\n",
      "Epoch 12, loss: 2.291062\n",
      "Epoch 13, loss: 2.290261\n",
      "Epoch 14, loss: 2.289447\n",
      "Epoch 15, loss: 2.288644\n",
      "Epoch 16, loss: 2.287842\n",
      "Epoch 17, loss: 2.287044\n",
      "Epoch 18, loss: 2.286278\n",
      "Epoch 19, loss: 2.285482\n",
      "Epoch 20, loss: 2.284702\n",
      "Epoch 21, loss: 2.283942\n",
      "Epoch 22, loss: 2.283186\n",
      "Epoch 23, loss: 2.282423\n",
      "Epoch 24, loss: 2.281661\n",
      "Epoch 25, loss: 2.280938\n",
      "Epoch 26, loss: 2.280188\n",
      "Epoch 27, loss: 2.279446\n",
      "Epoch 28, loss: 2.278733\n",
      "Epoch 29, loss: 2.277991\n",
      "Epoch 30, loss: 2.277300\n",
      "Epoch 31, loss: 2.276569\n",
      "Epoch 32, loss: 2.275855\n",
      "Epoch 33, loss: 2.275155\n",
      "Epoch 34, loss: 2.274458\n",
      "Epoch 35, loss: 2.273788\n",
      "Epoch 36, loss: 2.273090\n",
      "Epoch 37, loss: 2.272401\n",
      "Epoch 38, loss: 2.271735\n",
      "Epoch 39, loss: 2.271068\n",
      "Epoch 40, loss: 2.270399\n",
      "Epoch 41, loss: 2.269727\n",
      "Epoch 42, loss: 2.269065\n",
      "Epoch 43, loss: 2.268411\n",
      "Epoch 44, loss: 2.267753\n",
      "Epoch 45, loss: 2.267127\n",
      "Epoch 46, loss: 2.266485\n",
      "Epoch 47, loss: 2.265835\n",
      "Epoch 48, loss: 2.265205\n",
      "Epoch 49, loss: 2.264569\n",
      "Epoch 50, loss: 2.263933\n",
      "Epoch 51, loss: 2.263322\n",
      "Epoch 52, loss: 2.262702\n",
      "Epoch 53, loss: 2.262099\n",
      "Epoch 54, loss: 2.261478\n",
      "Epoch 55, loss: 2.260874\n",
      "Epoch 56, loss: 2.260283\n",
      "Epoch 57, loss: 2.259663\n",
      "Epoch 58, loss: 2.259090\n",
      "Epoch 59, loss: 2.258490\n",
      "Epoch 60, loss: 2.257903\n",
      "Epoch 61, loss: 2.257348\n",
      "Epoch 62, loss: 2.256757\n",
      "Epoch 63, loss: 2.256175\n",
      "Epoch 64, loss: 2.255600\n",
      "Epoch 65, loss: 2.255026\n",
      "Epoch 66, loss: 2.254477\n",
      "Epoch 67, loss: 2.253897\n",
      "Epoch 68, loss: 2.253347\n",
      "Epoch 69, loss: 2.252782\n",
      "Epoch 70, loss: 2.252244\n",
      "Epoch 71, loss: 2.251683\n",
      "Epoch 72, loss: 2.251160\n",
      "Epoch 73, loss: 2.250610\n",
      "Epoch 74, loss: 2.250050\n",
      "Epoch 75, loss: 2.249527\n",
      "Epoch 76, loss: 2.248991\n",
      "Epoch 77, loss: 2.248469\n",
      "Epoch 78, loss: 2.247943\n",
      "Epoch 79, loss: 2.247425\n",
      "Epoch 80, loss: 2.246920\n",
      "Epoch 81, loss: 2.246385\n",
      "Epoch 82, loss: 2.245883\n",
      "Epoch 83, loss: 2.245354\n",
      "Epoch 84, loss: 2.244863\n",
      "Epoch 85, loss: 2.244364\n",
      "Epoch 86, loss: 2.243861\n",
      "Epoch 87, loss: 2.243384\n",
      "Epoch 88, loss: 2.242841\n",
      "Epoch 89, loss: 2.242383\n",
      "Epoch 90, loss: 2.241883\n",
      "Epoch 91, loss: 2.241391\n",
      "Epoch 92, loss: 2.240923\n",
      "Epoch 93, loss: 2.240430\n",
      "Epoch 94, loss: 2.239965\n",
      "Epoch 95, loss: 2.239472\n",
      "Epoch 96, loss: 2.239001\n",
      "Epoch 97, loss: 2.238543\n",
      "Epoch 98, loss: 2.238073\n",
      "Epoch 99, loss: 2.237614\n",
      "Epoch 100, loss: 2.237152\n",
      "Epoch 101, loss: 2.236675\n",
      "Epoch 102, loss: 2.236241\n",
      "Epoch 103, loss: 2.235763\n",
      "Epoch 104, loss: 2.235328\n",
      "Epoch 105, loss: 2.234885\n",
      "Epoch 106, loss: 2.234411\n",
      "Epoch 107, loss: 2.233982\n",
      "Epoch 108, loss: 2.233550\n",
      "Epoch 109, loss: 2.233119\n",
      "Epoch 110, loss: 2.232670\n",
      "Epoch 111, loss: 2.232243\n",
      "Epoch 112, loss: 2.231805\n",
      "Epoch 113, loss: 2.231377\n",
      "Epoch 114, loss: 2.230929\n",
      "Epoch 115, loss: 2.230529\n",
      "Epoch 116, loss: 2.230095\n",
      "Epoch 117, loss: 2.229684\n",
      "Epoch 118, loss: 2.229270\n",
      "Epoch 119, loss: 2.228859\n",
      "Epoch 120, loss: 2.228457\n",
      "Epoch 121, loss: 2.228036\n",
      "Epoch 122, loss: 2.227624\n",
      "Epoch 123, loss: 2.227224\n",
      "Epoch 124, loss: 2.226821\n",
      "Epoch 125, loss: 2.226417\n",
      "Epoch 126, loss: 2.226000\n",
      "Epoch 127, loss: 2.225624\n",
      "Epoch 128, loss: 2.225236\n",
      "Epoch 129, loss: 2.224845\n",
      "Epoch 130, loss: 2.224447\n",
      "Epoch 131, loss: 2.224061\n",
      "Epoch 132, loss: 2.223658\n",
      "Epoch 133, loss: 2.223290\n",
      "Epoch 134, loss: 2.222899\n",
      "Epoch 135, loss: 2.222521\n",
      "Epoch 136, loss: 2.222149\n",
      "Epoch 137, loss: 2.221783\n",
      "Epoch 138, loss: 2.221402\n",
      "Epoch 139, loss: 2.221032\n",
      "Epoch 140, loss: 2.220657\n",
      "Epoch 141, loss: 2.220285\n",
      "Epoch 142, loss: 2.219934\n",
      "Epoch 143, loss: 2.219561\n",
      "Epoch 144, loss: 2.219190\n",
      "Epoch 145, loss: 2.218843\n",
      "Epoch 146, loss: 2.218476\n",
      "Epoch 147, loss: 2.218117\n",
      "Epoch 148, loss: 2.217780\n",
      "Epoch 149, loss: 2.217432\n",
      "Epoch 150, loss: 2.217059\n",
      "Epoch 151, loss: 2.216730\n",
      "Epoch 152, loss: 2.216362\n",
      "Epoch 153, loss: 2.216045\n",
      "Epoch 154, loss: 2.215684\n",
      "Epoch 155, loss: 2.215374\n",
      "Epoch 156, loss: 2.215015\n",
      "Epoch 157, loss: 2.214686\n",
      "Epoch 158, loss: 2.214344\n",
      "Epoch 159, loss: 2.214016\n",
      "Epoch 160, loss: 2.213669\n",
      "Epoch 161, loss: 2.213355\n",
      "Epoch 162, loss: 2.213025\n",
      "Epoch 163, loss: 2.212688\n",
      "Epoch 164, loss: 2.212377\n",
      "Epoch 165, loss: 2.212043\n",
      "Epoch 166, loss: 2.211734\n",
      "Epoch 167, loss: 2.211408\n",
      "Epoch 168, loss: 2.211090\n",
      "Epoch 169, loss: 2.210770\n",
      "Epoch 170, loss: 2.210445\n",
      "Epoch 171, loss: 2.210133\n",
      "Epoch 172, loss: 2.209831\n",
      "Epoch 173, loss: 2.209515\n",
      "Epoch 174, loss: 2.209237\n",
      "Epoch 175, loss: 2.208903\n",
      "Epoch 176, loss: 2.208604\n",
      "Epoch 177, loss: 2.208306\n",
      "Epoch 178, loss: 2.208004\n",
      "Epoch 179, loss: 2.207708\n",
      "Epoch 180, loss: 2.207393\n",
      "Epoch 181, loss: 2.207115\n",
      "Epoch 182, loss: 2.206817\n",
      "Epoch 183, loss: 2.206526\n",
      "Epoch 184, loss: 2.206227\n",
      "Epoch 185, loss: 2.205947\n",
      "Epoch 186, loss: 2.205656\n",
      "Epoch 187, loss: 2.205356\n",
      "Epoch 188, loss: 2.205083\n",
      "Epoch 189, loss: 2.204781\n",
      "Epoch 190, loss: 2.204486\n",
      "Epoch 191, loss: 2.204213\n",
      "Epoch 192, loss: 2.203938\n",
      "Epoch 193, loss: 2.203653\n",
      "Epoch 194, loss: 2.203376\n",
      "Epoch 195, loss: 2.203093\n",
      "Epoch 196, loss: 2.202830\n",
      "Epoch 197, loss: 2.202550\n",
      "Epoch 198, loss: 2.202287\n",
      "Epoch 199, loss: 2.202003\n",
      "This accuracy = 0.23\n",
      "Start learning_rates = 0.001, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 2.302460\n",
      "Epoch 1, loss: 2.301368\n",
      "Epoch 2, loss: 2.300382\n",
      "Epoch 3, loss: 2.299435\n",
      "Epoch 4, loss: 2.298486\n",
      "Epoch 5, loss: 2.297604\n",
      "Epoch 6, loss: 2.296700\n",
      "Epoch 7, loss: 2.295834\n",
      "Epoch 8, loss: 2.294967\n",
      "Epoch 9, loss: 2.294115\n",
      "Epoch 10, loss: 2.293272\n",
      "Epoch 11, loss: 2.292434\n",
      "Epoch 12, loss: 2.291619\n",
      "Epoch 13, loss: 2.290783\n",
      "Epoch 14, loss: 2.289955\n",
      "Epoch 15, loss: 2.289164\n",
      "Epoch 16, loss: 2.288362\n",
      "Epoch 17, loss: 2.287577\n",
      "Epoch 18, loss: 2.286776\n",
      "Epoch 19, loss: 2.286000\n",
      "Epoch 20, loss: 2.285228\n",
      "Epoch 21, loss: 2.284462\n",
      "Epoch 22, loss: 2.283699\n",
      "Epoch 23, loss: 2.282930\n",
      "Epoch 24, loss: 2.282166\n",
      "Epoch 25, loss: 2.281431\n",
      "Epoch 26, loss: 2.280697\n",
      "Epoch 27, loss: 2.279947\n",
      "Epoch 28, loss: 2.279238\n",
      "Epoch 29, loss: 2.278489\n",
      "Epoch 30, loss: 2.277792\n",
      "Epoch 31, loss: 2.277064\n",
      "Epoch 32, loss: 2.276357\n",
      "Epoch 33, loss: 2.275656\n",
      "Epoch 34, loss: 2.274948\n",
      "Epoch 35, loss: 2.274253\n",
      "Epoch 36, loss: 2.273572\n",
      "Epoch 37, loss: 2.272886\n",
      "Epoch 38, loss: 2.272206\n",
      "Epoch 39, loss: 2.271534\n",
      "Epoch 40, loss: 2.270873\n",
      "Epoch 41, loss: 2.270200\n",
      "Epoch 42, loss: 2.269543\n",
      "Epoch 43, loss: 2.268883\n",
      "Epoch 44, loss: 2.268205\n",
      "Epoch 45, loss: 2.267576\n",
      "Epoch 46, loss: 2.266935\n",
      "Epoch 47, loss: 2.266297\n",
      "Epoch 48, loss: 2.265653\n",
      "Epoch 49, loss: 2.265028\n",
      "Epoch 50, loss: 2.264391\n",
      "Epoch 51, loss: 2.263783\n",
      "Epoch 52, loss: 2.263153\n",
      "Epoch 53, loss: 2.262559\n",
      "Epoch 54, loss: 2.261926\n",
      "Epoch 55, loss: 2.261300\n",
      "Epoch 56, loss: 2.260716\n",
      "Epoch 57, loss: 2.260109\n",
      "Epoch 58, loss: 2.259490\n",
      "Epoch 59, loss: 2.258914\n",
      "Epoch 60, loss: 2.258341\n",
      "Epoch 61, loss: 2.257746\n",
      "Epoch 62, loss: 2.257178\n",
      "Epoch 63, loss: 2.256578\n",
      "Epoch 64, loss: 2.256019\n",
      "Epoch 65, loss: 2.255448\n",
      "Epoch 66, loss: 2.254866\n",
      "Epoch 67, loss: 2.254307\n",
      "Epoch 68, loss: 2.253759\n",
      "Epoch 69, loss: 2.253191\n",
      "Epoch 70, loss: 2.252640\n",
      "Epoch 71, loss: 2.252083\n",
      "Epoch 72, loss: 2.251543\n",
      "Epoch 73, loss: 2.250997\n",
      "Epoch 74, loss: 2.250455\n",
      "Epoch 75, loss: 2.249914\n",
      "Epoch 76, loss: 2.249381\n",
      "Epoch 77, loss: 2.248847\n",
      "Epoch 78, loss: 2.248326\n",
      "Epoch 79, loss: 2.247815\n",
      "Epoch 80, loss: 2.247268\n",
      "Epoch 81, loss: 2.246764\n",
      "Epoch 82, loss: 2.246251\n",
      "Epoch 83, loss: 2.245729\n",
      "Epoch 84, loss: 2.245234\n",
      "Epoch 85, loss: 2.244727\n",
      "Epoch 86, loss: 2.244225\n",
      "Epoch 87, loss: 2.243725\n",
      "Epoch 88, loss: 2.243248\n",
      "Epoch 89, loss: 2.242725\n",
      "Epoch 90, loss: 2.242249\n",
      "Epoch 91, loss: 2.241743\n",
      "Epoch 92, loss: 2.241259\n",
      "Epoch 93, loss: 2.240784\n",
      "Epoch 94, loss: 2.240294\n",
      "Epoch 95, loss: 2.239826\n",
      "Epoch 96, loss: 2.239347\n",
      "Epoch 97, loss: 2.238867\n",
      "Epoch 98, loss: 2.238408\n",
      "Epoch 99, loss: 2.237935\n",
      "Epoch 100, loss: 2.237470\n",
      "Epoch 101, loss: 2.237027\n",
      "Epoch 102, loss: 2.236567\n",
      "Epoch 103, loss: 2.236076\n",
      "Epoch 104, loss: 2.235632\n",
      "Epoch 105, loss: 2.235189\n",
      "Epoch 106, loss: 2.234759\n",
      "Epoch 107, loss: 2.234306\n",
      "Epoch 108, loss: 2.233852\n",
      "Epoch 109, loss: 2.233414\n",
      "Epoch 110, loss: 2.232975\n",
      "Epoch 111, loss: 2.232551\n",
      "Epoch 112, loss: 2.232109\n",
      "Epoch 113, loss: 2.231685\n",
      "Epoch 114, loss: 2.231248\n",
      "Epoch 115, loss: 2.230823\n",
      "Epoch 116, loss: 2.230414\n",
      "Epoch 117, loss: 2.229978\n",
      "Epoch 118, loss: 2.229558\n",
      "Epoch 119, loss: 2.229141\n",
      "Epoch 120, loss: 2.228719\n",
      "Epoch 121, loss: 2.228315\n",
      "Epoch 122, loss: 2.227897\n",
      "Epoch 123, loss: 2.227502\n",
      "Epoch 124, loss: 2.227113\n",
      "Epoch 125, loss: 2.226703\n",
      "Epoch 126, loss: 2.226301\n",
      "Epoch 127, loss: 2.225894\n",
      "Epoch 128, loss: 2.225493\n",
      "Epoch 129, loss: 2.225092\n",
      "Epoch 130, loss: 2.224719\n",
      "Epoch 131, loss: 2.224314\n",
      "Epoch 132, loss: 2.223929\n",
      "Epoch 133, loss: 2.223540\n",
      "Epoch 134, loss: 2.223148\n",
      "Epoch 135, loss: 2.222799\n",
      "Epoch 136, loss: 2.222397\n",
      "Epoch 137, loss: 2.222019\n",
      "Epoch 138, loss: 2.221657\n",
      "Epoch 139, loss: 2.221272\n",
      "Epoch 140, loss: 2.220906\n",
      "Epoch 141, loss: 2.220530\n",
      "Epoch 142, loss: 2.220170\n",
      "Epoch 143, loss: 2.219790\n",
      "Epoch 144, loss: 2.219449\n",
      "Epoch 145, loss: 2.219075\n",
      "Epoch 146, loss: 2.218720\n",
      "Epoch 147, loss: 2.218375\n",
      "Epoch 148, loss: 2.218008\n",
      "Epoch 149, loss: 2.217649\n",
      "Epoch 150, loss: 2.217304\n",
      "Epoch 151, loss: 2.216951\n",
      "Epoch 152, loss: 2.216612\n",
      "Epoch 153, loss: 2.216274\n",
      "Epoch 154, loss: 2.215924\n",
      "Epoch 155, loss: 2.215568\n",
      "Epoch 156, loss: 2.215237\n",
      "Epoch 157, loss: 2.214896\n",
      "Epoch 158, loss: 2.214554\n",
      "Epoch 159, loss: 2.214218\n",
      "Epoch 160, loss: 2.213892\n",
      "Epoch 161, loss: 2.213555\n",
      "Epoch 162, loss: 2.213244\n",
      "Epoch 163, loss: 2.212887\n",
      "Epoch 164, loss: 2.212576\n",
      "Epoch 165, loss: 2.212251\n",
      "Epoch 166, loss: 2.211939\n",
      "Epoch 167, loss: 2.211598\n",
      "Epoch 168, loss: 2.211275\n",
      "Epoch 169, loss: 2.210972\n",
      "Epoch 170, loss: 2.210656\n",
      "Epoch 171, loss: 2.210326\n",
      "Epoch 172, loss: 2.210044\n",
      "Epoch 173, loss: 2.209728\n",
      "Epoch 174, loss: 2.209404\n",
      "Epoch 175, loss: 2.209105\n",
      "Epoch 176, loss: 2.208789\n",
      "Epoch 177, loss: 2.208486\n",
      "Epoch 178, loss: 2.208187\n",
      "Epoch 179, loss: 2.207867\n",
      "Epoch 180, loss: 2.207582\n",
      "Epoch 181, loss: 2.207276\n",
      "Epoch 182, loss: 2.207000\n",
      "Epoch 183, loss: 2.206694\n",
      "Epoch 184, loss: 2.206402\n",
      "Epoch 185, loss: 2.206105\n",
      "Epoch 186, loss: 2.205837\n",
      "Epoch 187, loss: 2.205519\n",
      "Epoch 188, loss: 2.205236\n",
      "Epoch 189, loss: 2.204941\n",
      "Epoch 190, loss: 2.204666\n",
      "Epoch 191, loss: 2.204388\n",
      "Epoch 192, loss: 2.204095\n",
      "Epoch 193, loss: 2.203817\n",
      "Epoch 194, loss: 2.203538\n",
      "Epoch 195, loss: 2.203238\n",
      "Epoch 196, loss: 2.202970\n",
      "Epoch 197, loss: 2.202698\n",
      "Epoch 198, loss: 2.202445\n",
      "Epoch 199, loss: 2.202155\n",
      "This accuracy = 0.227\n",
      "Start learning_rates = 0.001, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 2.302269\n",
      "Epoch 1, loss: 2.301209\n",
      "Epoch 2, loss: 2.300223\n",
      "Epoch 3, loss: 2.299257\n",
      "Epoch 4, loss: 2.298322\n",
      "Epoch 5, loss: 2.297433\n",
      "Epoch 6, loss: 2.296558\n",
      "Epoch 7, loss: 2.295662\n",
      "Epoch 8, loss: 2.294806\n",
      "Epoch 9, loss: 2.293952\n",
      "Epoch 10, loss: 2.293107\n",
      "Epoch 11, loss: 2.292276\n",
      "Epoch 12, loss: 2.291458\n",
      "Epoch 13, loss: 2.290642\n",
      "Epoch 14, loss: 2.289815\n",
      "Epoch 15, loss: 2.289005\n",
      "Epoch 16, loss: 2.288210\n",
      "Epoch 17, loss: 2.287408\n",
      "Epoch 18, loss: 2.286647\n",
      "Epoch 19, loss: 2.285883\n",
      "Epoch 20, loss: 2.285078\n",
      "Epoch 21, loss: 2.284312\n",
      "Epoch 22, loss: 2.283554\n",
      "Epoch 23, loss: 2.282802\n",
      "Epoch 24, loss: 2.282032\n",
      "Epoch 25, loss: 2.281303\n",
      "Epoch 26, loss: 2.280565\n",
      "Epoch 27, loss: 2.279818\n",
      "Epoch 28, loss: 2.279107\n",
      "Epoch 29, loss: 2.278369\n",
      "Epoch 30, loss: 2.277664\n",
      "Epoch 31, loss: 2.276937\n",
      "Epoch 32, loss: 2.276242\n",
      "Epoch 33, loss: 2.275531\n",
      "Epoch 34, loss: 2.274838\n",
      "Epoch 35, loss: 2.274142\n",
      "Epoch 36, loss: 2.273452\n",
      "Epoch 37, loss: 2.272759\n",
      "Epoch 38, loss: 2.272094\n",
      "Epoch 39, loss: 2.271405\n",
      "Epoch 40, loss: 2.270753\n",
      "Epoch 41, loss: 2.270075\n",
      "Epoch 42, loss: 2.269419\n",
      "Epoch 43, loss: 2.268760\n",
      "Epoch 44, loss: 2.268099\n",
      "Epoch 45, loss: 2.267459\n",
      "Epoch 46, loss: 2.266816\n",
      "Epoch 47, loss: 2.266184\n",
      "Epoch 48, loss: 2.265540\n",
      "Epoch 49, loss: 2.264914\n",
      "Epoch 50, loss: 2.264283\n",
      "Epoch 51, loss: 2.263652\n",
      "Epoch 52, loss: 2.263059\n",
      "Epoch 53, loss: 2.262418\n",
      "Epoch 54, loss: 2.261827\n",
      "Epoch 55, loss: 2.261220\n",
      "Epoch 56, loss: 2.260605\n",
      "Epoch 57, loss: 2.259998\n",
      "Epoch 58, loss: 2.259418\n",
      "Epoch 59, loss: 2.258816\n",
      "Epoch 60, loss: 2.258225\n",
      "Epoch 61, loss: 2.257635\n",
      "Epoch 62, loss: 2.257059\n",
      "Epoch 63, loss: 2.256481\n",
      "Epoch 64, loss: 2.255908\n",
      "Epoch 65, loss: 2.255343\n",
      "Epoch 66, loss: 2.254772\n",
      "Epoch 67, loss: 2.254196\n",
      "Epoch 68, loss: 2.253632\n",
      "Epoch 69, loss: 2.253089\n",
      "Epoch 70, loss: 2.252542\n",
      "Epoch 71, loss: 2.251980\n",
      "Epoch 72, loss: 2.251453\n",
      "Epoch 73, loss: 2.250896\n",
      "Epoch 74, loss: 2.250367\n",
      "Epoch 75, loss: 2.249840\n",
      "Epoch 76, loss: 2.249309\n",
      "Epoch 77, loss: 2.248758\n",
      "Epoch 78, loss: 2.248237\n",
      "Epoch 79, loss: 2.247721\n",
      "Epoch 80, loss: 2.247192\n",
      "Epoch 81, loss: 2.246671\n",
      "Epoch 82, loss: 2.246149\n",
      "Epoch 83, loss: 2.245660\n",
      "Epoch 84, loss: 2.245146\n",
      "Epoch 85, loss: 2.244642\n",
      "Epoch 86, loss: 2.244147\n",
      "Epoch 87, loss: 2.243635\n",
      "Epoch 88, loss: 2.243139\n",
      "Epoch 89, loss: 2.242646\n",
      "Epoch 90, loss: 2.242153\n",
      "Epoch 91, loss: 2.241670\n",
      "Epoch 92, loss: 2.241166\n",
      "Epoch 93, loss: 2.240697\n",
      "Epoch 94, loss: 2.240223\n",
      "Epoch 95, loss: 2.239748\n",
      "Epoch 96, loss: 2.239277\n",
      "Epoch 97, loss: 2.238791\n",
      "Epoch 98, loss: 2.238327\n",
      "Epoch 99, loss: 2.237874\n",
      "Epoch 100, loss: 2.237408\n",
      "Epoch 101, loss: 2.236932\n",
      "Epoch 102, loss: 2.236486\n",
      "Epoch 103, loss: 2.236027\n",
      "Epoch 104, loss: 2.235569\n",
      "Epoch 105, loss: 2.235122\n",
      "Epoch 106, loss: 2.234668\n",
      "Epoch 107, loss: 2.234236\n",
      "Epoch 108, loss: 2.233803\n",
      "Epoch 109, loss: 2.233348\n",
      "Epoch 110, loss: 2.232892\n",
      "Epoch 111, loss: 2.232477\n",
      "Epoch 112, loss: 2.232036\n",
      "Epoch 113, loss: 2.231594\n",
      "Epoch 114, loss: 2.231175\n",
      "Epoch 115, loss: 2.230755\n",
      "Epoch 116, loss: 2.230321\n",
      "Epoch 117, loss: 2.229919\n",
      "Epoch 118, loss: 2.229477\n",
      "Epoch 119, loss: 2.229067\n",
      "Epoch 120, loss: 2.228664\n",
      "Epoch 121, loss: 2.228264\n",
      "Epoch 122, loss: 2.227837\n",
      "Epoch 123, loss: 2.227441\n",
      "Epoch 124, loss: 2.227034\n",
      "Epoch 125, loss: 2.226619\n",
      "Epoch 126, loss: 2.226224\n",
      "Epoch 127, loss: 2.225834\n",
      "Epoch 128, loss: 2.225439\n",
      "Epoch 129, loss: 2.225036\n",
      "Epoch 130, loss: 2.224629\n",
      "Epoch 131, loss: 2.224258\n",
      "Epoch 132, loss: 2.223878\n",
      "Epoch 133, loss: 2.223491\n",
      "Epoch 134, loss: 2.223099\n",
      "Epoch 135, loss: 2.222724\n",
      "Epoch 136, loss: 2.222339\n",
      "Epoch 137, loss: 2.221965\n",
      "Epoch 138, loss: 2.221583\n",
      "Epoch 139, loss: 2.221217\n",
      "Epoch 140, loss: 2.220844\n",
      "Epoch 141, loss: 2.220472\n",
      "Epoch 142, loss: 2.220107\n",
      "Epoch 143, loss: 2.219741\n",
      "Epoch 144, loss: 2.219379\n",
      "Epoch 145, loss: 2.219021\n",
      "Epoch 146, loss: 2.218664\n",
      "Epoch 147, loss: 2.218300\n",
      "Epoch 148, loss: 2.217969\n",
      "Epoch 149, loss: 2.217625\n",
      "Epoch 150, loss: 2.217253\n",
      "Epoch 151, loss: 2.216923\n",
      "Epoch 152, loss: 2.216559\n",
      "Epoch 153, loss: 2.216222\n",
      "Epoch 154, loss: 2.215864\n",
      "Epoch 155, loss: 2.215532\n",
      "Epoch 156, loss: 2.215185\n",
      "Epoch 157, loss: 2.214862\n",
      "Epoch 158, loss: 2.214507\n",
      "Epoch 159, loss: 2.214167\n",
      "Epoch 160, loss: 2.213849\n",
      "Epoch 161, loss: 2.213504\n",
      "Epoch 162, loss: 2.213166\n",
      "Epoch 163, loss: 2.212851\n",
      "Epoch 164, loss: 2.212507\n",
      "Epoch 165, loss: 2.212207\n",
      "Epoch 166, loss: 2.211881\n",
      "Epoch 167, loss: 2.211559\n",
      "Epoch 168, loss: 2.211214\n",
      "Epoch 169, loss: 2.210927\n",
      "Epoch 170, loss: 2.210623\n",
      "Epoch 171, loss: 2.210298\n",
      "Epoch 172, loss: 2.209971\n",
      "Epoch 173, loss: 2.209658\n",
      "Epoch 174, loss: 2.209366\n",
      "Epoch 175, loss: 2.209062\n",
      "Epoch 176, loss: 2.208747\n",
      "Epoch 177, loss: 2.208432\n",
      "Epoch 178, loss: 2.208130\n",
      "Epoch 179, loss: 2.207842\n",
      "Epoch 180, loss: 2.207531\n",
      "Epoch 181, loss: 2.207217\n",
      "Epoch 182, loss: 2.206922\n",
      "Epoch 183, loss: 2.206641\n",
      "Epoch 184, loss: 2.206356\n",
      "Epoch 185, loss: 2.206064\n",
      "Epoch 186, loss: 2.205776\n",
      "Epoch 187, loss: 2.205450\n",
      "Epoch 188, loss: 2.205200\n",
      "Epoch 189, loss: 2.204899\n",
      "Epoch 190, loss: 2.204624\n",
      "Epoch 191, loss: 2.204336\n",
      "Epoch 192, loss: 2.204051\n",
      "Epoch 193, loss: 2.203764\n",
      "Epoch 194, loss: 2.203496\n",
      "Epoch 195, loss: 2.203209\n",
      "Epoch 196, loss: 2.202934\n",
      "Epoch 197, loss: 2.202649\n",
      "Epoch 198, loss: 2.202391\n",
      "Epoch 199, loss: 2.202118\n",
      "This accuracy = 0.226\n",
      "Start learning_rates = 0.0001, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 2.302380\n",
      "Epoch 1, loss: 2.302271\n",
      "Epoch 2, loss: 2.302165\n",
      "Epoch 3, loss: 2.302054\n",
      "Epoch 4, loss: 2.301949\n",
      "Epoch 5, loss: 2.301843\n",
      "Epoch 6, loss: 2.301737\n",
      "Epoch 7, loss: 2.301634\n",
      "Epoch 8, loss: 2.301528\n",
      "Epoch 9, loss: 2.301425\n",
      "Epoch 10, loss: 2.301322\n",
      "Epoch 11, loss: 2.301219\n",
      "Epoch 12, loss: 2.301118\n",
      "Epoch 13, loss: 2.301017\n",
      "Epoch 14, loss: 2.300917\n",
      "Epoch 15, loss: 2.300816\n",
      "Epoch 16, loss: 2.300717\n",
      "Epoch 17, loss: 2.300618\n",
      "Epoch 18, loss: 2.300520\n",
      "Epoch 19, loss: 2.300420\n",
      "Epoch 20, loss: 2.300322\n",
      "Epoch 21, loss: 2.300223\n",
      "Epoch 22, loss: 2.300127\n",
      "Epoch 23, loss: 2.300030\n",
      "Epoch 24, loss: 2.299936\n",
      "Epoch 25, loss: 2.299838\n",
      "Epoch 26, loss: 2.299741\n",
      "Epoch 27, loss: 2.299647\n",
      "Epoch 28, loss: 2.299552\n",
      "Epoch 29, loss: 2.299457\n",
      "Epoch 30, loss: 2.299362\n",
      "Epoch 31, loss: 2.299271\n",
      "Epoch 32, loss: 2.299174\n",
      "Epoch 33, loss: 2.299082\n",
      "Epoch 34, loss: 2.298987\n",
      "Epoch 35, loss: 2.298893\n",
      "Epoch 36, loss: 2.298803\n",
      "Epoch 37, loss: 2.298712\n",
      "Epoch 38, loss: 2.298618\n",
      "Epoch 39, loss: 2.298527\n",
      "Epoch 40, loss: 2.298433\n",
      "Epoch 41, loss: 2.298343\n",
      "Epoch 42, loss: 2.298252\n",
      "Epoch 43, loss: 2.298161\n",
      "Epoch 44, loss: 2.298068\n",
      "Epoch 45, loss: 2.297979\n",
      "Epoch 46, loss: 2.297888\n",
      "Epoch 47, loss: 2.297796\n",
      "Epoch 48, loss: 2.297707\n",
      "Epoch 49, loss: 2.297618\n",
      "Epoch 50, loss: 2.297526\n",
      "Epoch 51, loss: 2.297437\n",
      "Epoch 52, loss: 2.297348\n",
      "Epoch 53, loss: 2.297258\n",
      "Epoch 54, loss: 2.297169\n",
      "Epoch 55, loss: 2.297079\n",
      "Epoch 56, loss: 2.296992\n",
      "Epoch 57, loss: 2.296903\n",
      "Epoch 58, loss: 2.296815\n",
      "Epoch 59, loss: 2.296724\n",
      "Epoch 60, loss: 2.296639\n",
      "Epoch 61, loss: 2.296547\n",
      "Epoch 62, loss: 2.296460\n",
      "Epoch 63, loss: 2.296372\n",
      "Epoch 64, loss: 2.296284\n",
      "Epoch 65, loss: 2.296195\n",
      "Epoch 66, loss: 2.296108\n",
      "Epoch 67, loss: 2.296022\n",
      "Epoch 68, loss: 2.295932\n",
      "Epoch 69, loss: 2.295847\n",
      "Epoch 70, loss: 2.295759\n",
      "Epoch 71, loss: 2.295674\n",
      "Epoch 72, loss: 2.295586\n",
      "Epoch 73, loss: 2.295499\n",
      "Epoch 74, loss: 2.295411\n",
      "Epoch 75, loss: 2.295329\n",
      "Epoch 76, loss: 2.295240\n",
      "Epoch 77, loss: 2.295153\n",
      "Epoch 78, loss: 2.295066\n",
      "Epoch 79, loss: 2.294981\n",
      "Epoch 80, loss: 2.294893\n",
      "Epoch 81, loss: 2.294807\n",
      "Epoch 82, loss: 2.294722\n",
      "Epoch 83, loss: 2.294635\n",
      "Epoch 84, loss: 2.294551\n",
      "Epoch 85, loss: 2.294465\n",
      "Epoch 86, loss: 2.294380\n",
      "Epoch 87, loss: 2.294295\n",
      "Epoch 88, loss: 2.294210\n",
      "Epoch 89, loss: 2.294124\n",
      "Epoch 90, loss: 2.294038\n",
      "Epoch 91, loss: 2.293954\n",
      "Epoch 92, loss: 2.293870\n",
      "Epoch 93, loss: 2.293785\n",
      "Epoch 94, loss: 2.293699\n",
      "Epoch 95, loss: 2.293614\n",
      "Epoch 96, loss: 2.293528\n",
      "Epoch 97, loss: 2.293445\n",
      "Epoch 98, loss: 2.293361\n",
      "Epoch 99, loss: 2.293277\n",
      "Epoch 100, loss: 2.293192\n",
      "Epoch 101, loss: 2.293108\n",
      "Epoch 102, loss: 2.293025\n",
      "Epoch 103, loss: 2.292942\n",
      "Epoch 104, loss: 2.292856\n",
      "Epoch 105, loss: 2.292773\n",
      "Epoch 106, loss: 2.292687\n",
      "Epoch 107, loss: 2.292604\n",
      "Epoch 108, loss: 2.292519\n",
      "Epoch 109, loss: 2.292437\n",
      "Epoch 110, loss: 2.292355\n",
      "Epoch 111, loss: 2.292271\n",
      "Epoch 112, loss: 2.292187\n",
      "Epoch 113, loss: 2.292106\n",
      "Epoch 114, loss: 2.292021\n",
      "Epoch 115, loss: 2.291938\n",
      "Epoch 116, loss: 2.291855\n",
      "Epoch 117, loss: 2.291772\n",
      "Epoch 118, loss: 2.291688\n",
      "Epoch 119, loss: 2.291605\n",
      "Epoch 120, loss: 2.291524\n",
      "Epoch 121, loss: 2.291440\n",
      "Epoch 122, loss: 2.291357\n",
      "Epoch 123, loss: 2.291275\n",
      "Epoch 124, loss: 2.291192\n",
      "Epoch 125, loss: 2.291111\n",
      "Epoch 126, loss: 2.291029\n",
      "Epoch 127, loss: 2.290945\n",
      "Epoch 128, loss: 2.290864\n",
      "Epoch 129, loss: 2.290783\n",
      "Epoch 130, loss: 2.290702\n",
      "Epoch 131, loss: 2.290618\n",
      "Epoch 132, loss: 2.290537\n",
      "Epoch 133, loss: 2.290456\n",
      "Epoch 134, loss: 2.290374\n",
      "Epoch 135, loss: 2.290291\n",
      "Epoch 136, loss: 2.290209\n",
      "Epoch 137, loss: 2.290128\n",
      "Epoch 138, loss: 2.290046\n",
      "Epoch 139, loss: 2.289965\n",
      "Epoch 140, loss: 2.289886\n",
      "Epoch 141, loss: 2.289803\n",
      "Epoch 142, loss: 2.289722\n",
      "Epoch 143, loss: 2.289639\n",
      "Epoch 144, loss: 2.289560\n",
      "Epoch 145, loss: 2.289479\n",
      "Epoch 146, loss: 2.289396\n",
      "Epoch 147, loss: 2.289317\n",
      "Epoch 148, loss: 2.289236\n",
      "Epoch 149, loss: 2.289154\n",
      "Epoch 150, loss: 2.289076\n",
      "Epoch 151, loss: 2.288994\n",
      "Epoch 152, loss: 2.288914\n",
      "Epoch 153, loss: 2.288833\n",
      "Epoch 154, loss: 2.288752\n",
      "Epoch 155, loss: 2.288669\n",
      "Epoch 156, loss: 2.288593\n",
      "Epoch 157, loss: 2.288511\n",
      "Epoch 158, loss: 2.288433\n",
      "Epoch 159, loss: 2.288353\n",
      "Epoch 160, loss: 2.288274\n",
      "Epoch 161, loss: 2.288191\n",
      "Epoch 162, loss: 2.288111\n",
      "Epoch 163, loss: 2.288032\n",
      "Epoch 164, loss: 2.287952\n",
      "Epoch 165, loss: 2.287873\n",
      "Epoch 166, loss: 2.287795\n",
      "Epoch 167, loss: 2.287714\n",
      "Epoch 168, loss: 2.287634\n",
      "Epoch 169, loss: 2.287553\n",
      "Epoch 170, loss: 2.287476\n",
      "Epoch 171, loss: 2.287397\n",
      "Epoch 172, loss: 2.287317\n",
      "Epoch 173, loss: 2.287238\n",
      "Epoch 174, loss: 2.287158\n",
      "Epoch 175, loss: 2.287081\n",
      "Epoch 176, loss: 2.286999\n",
      "Epoch 177, loss: 2.286922\n",
      "Epoch 178, loss: 2.286843\n",
      "Epoch 179, loss: 2.286765\n",
      "Epoch 180, loss: 2.286685\n",
      "Epoch 181, loss: 2.286608\n",
      "Epoch 182, loss: 2.286528\n",
      "Epoch 183, loss: 2.286452\n",
      "Epoch 184, loss: 2.286370\n",
      "Epoch 185, loss: 2.286291\n",
      "Epoch 186, loss: 2.286216\n",
      "Epoch 187, loss: 2.286135\n",
      "Epoch 188, loss: 2.286057\n",
      "Epoch 189, loss: 2.285980\n",
      "Epoch 190, loss: 2.285902\n",
      "Epoch 191, loss: 2.285824\n",
      "Epoch 192, loss: 2.285746\n",
      "Epoch 193, loss: 2.285669\n",
      "Epoch 194, loss: 2.285591\n",
      "Epoch 195, loss: 2.285512\n",
      "Epoch 196, loss: 2.285436\n",
      "Epoch 197, loss: 2.285356\n",
      "Epoch 198, loss: 2.285279\n",
      "Epoch 199, loss: 2.285204\n",
      "This accuracy = 0.168\n",
      "Start learning_rates = 0.0001, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 2.302610\n",
      "Epoch 1, loss: 2.302495\n",
      "Epoch 2, loss: 2.302383\n",
      "Epoch 3, loss: 2.302272\n",
      "Epoch 4, loss: 2.302161\n",
      "Epoch 5, loss: 2.302052\n",
      "Epoch 6, loss: 2.301941\n",
      "Epoch 7, loss: 2.301835\n",
      "Epoch 8, loss: 2.301727\n",
      "Epoch 9, loss: 2.301620\n",
      "Epoch 10, loss: 2.301514\n",
      "Epoch 11, loss: 2.301407\n",
      "Epoch 12, loss: 2.301303\n",
      "Epoch 13, loss: 2.301199\n",
      "Epoch 14, loss: 2.301094\n",
      "Epoch 15, loss: 2.300995\n",
      "Epoch 16, loss: 2.300891\n",
      "Epoch 17, loss: 2.300791\n",
      "Epoch 18, loss: 2.300691\n",
      "Epoch 19, loss: 2.300588\n",
      "Epoch 20, loss: 2.300488\n",
      "Epoch 21, loss: 2.300389\n",
      "Epoch 22, loss: 2.300290\n",
      "Epoch 23, loss: 2.300193\n",
      "Epoch 24, loss: 2.300094\n",
      "Epoch 25, loss: 2.299996\n",
      "Epoch 26, loss: 2.299899\n",
      "Epoch 27, loss: 2.299802\n",
      "Epoch 28, loss: 2.299707\n",
      "Epoch 29, loss: 2.299612\n",
      "Epoch 30, loss: 2.299514\n",
      "Epoch 31, loss: 2.299419\n",
      "Epoch 32, loss: 2.299324\n",
      "Epoch 33, loss: 2.299230\n",
      "Epoch 34, loss: 2.299135\n",
      "Epoch 35, loss: 2.299041\n",
      "Epoch 36, loss: 2.298948\n",
      "Epoch 37, loss: 2.298856\n",
      "Epoch 38, loss: 2.298761\n",
      "Epoch 39, loss: 2.298668\n",
      "Epoch 40, loss: 2.298575\n",
      "Epoch 41, loss: 2.298483\n",
      "Epoch 42, loss: 2.298389\n",
      "Epoch 43, loss: 2.298300\n",
      "Epoch 44, loss: 2.298206\n",
      "Epoch 45, loss: 2.298116\n",
      "Epoch 46, loss: 2.298025\n",
      "Epoch 47, loss: 2.297934\n",
      "Epoch 48, loss: 2.297842\n",
      "Epoch 49, loss: 2.297752\n",
      "Epoch 50, loss: 2.297662\n",
      "Epoch 51, loss: 2.297571\n",
      "Epoch 52, loss: 2.297482\n",
      "Epoch 53, loss: 2.297393\n",
      "Epoch 54, loss: 2.297304\n",
      "Epoch 55, loss: 2.297214\n",
      "Epoch 56, loss: 2.297123\n",
      "Epoch 57, loss: 2.297035\n",
      "Epoch 58, loss: 2.296945\n",
      "Epoch 59, loss: 2.296858\n",
      "Epoch 60, loss: 2.296767\n",
      "Epoch 61, loss: 2.296681\n",
      "Epoch 62, loss: 2.296593\n",
      "Epoch 63, loss: 2.296503\n",
      "Epoch 64, loss: 2.296415\n",
      "Epoch 65, loss: 2.296327\n",
      "Epoch 66, loss: 2.296239\n",
      "Epoch 67, loss: 2.296151\n",
      "Epoch 68, loss: 2.296064\n",
      "Epoch 69, loss: 2.295977\n",
      "Epoch 70, loss: 2.295889\n",
      "Epoch 71, loss: 2.295802\n",
      "Epoch 72, loss: 2.295715\n",
      "Epoch 73, loss: 2.295629\n",
      "Epoch 74, loss: 2.295542\n",
      "Epoch 75, loss: 2.295455\n",
      "Epoch 76, loss: 2.295368\n",
      "Epoch 77, loss: 2.295283\n",
      "Epoch 78, loss: 2.295196\n",
      "Epoch 79, loss: 2.295109\n",
      "Epoch 80, loss: 2.295024\n",
      "Epoch 81, loss: 2.294936\n",
      "Epoch 82, loss: 2.294852\n",
      "Epoch 83, loss: 2.294765\n",
      "Epoch 84, loss: 2.294680\n",
      "Epoch 85, loss: 2.294594\n",
      "Epoch 86, loss: 2.294509\n",
      "Epoch 87, loss: 2.294423\n",
      "Epoch 88, loss: 2.294339\n",
      "Epoch 89, loss: 2.294252\n",
      "Epoch 90, loss: 2.294169\n",
      "Epoch 91, loss: 2.294085\n",
      "Epoch 92, loss: 2.293998\n",
      "Epoch 93, loss: 2.293913\n",
      "Epoch 94, loss: 2.293827\n",
      "Epoch 95, loss: 2.293745\n",
      "Epoch 96, loss: 2.293661\n",
      "Epoch 97, loss: 2.293574\n",
      "Epoch 98, loss: 2.293491\n",
      "Epoch 99, loss: 2.293407\n",
      "Epoch 100, loss: 2.293321\n",
      "Epoch 101, loss: 2.293237\n",
      "Epoch 102, loss: 2.293153\n",
      "Epoch 103, loss: 2.293069\n",
      "Epoch 104, loss: 2.292986\n",
      "Epoch 105, loss: 2.292901\n",
      "Epoch 106, loss: 2.292819\n",
      "Epoch 107, loss: 2.292735\n",
      "Epoch 108, loss: 2.292652\n",
      "Epoch 109, loss: 2.292568\n",
      "Epoch 110, loss: 2.292484\n",
      "Epoch 111, loss: 2.292402\n",
      "Epoch 112, loss: 2.292317\n",
      "Epoch 113, loss: 2.292233\n",
      "Epoch 114, loss: 2.292153\n",
      "Epoch 115, loss: 2.292069\n",
      "Epoch 116, loss: 2.291983\n",
      "Epoch 117, loss: 2.291901\n",
      "Epoch 118, loss: 2.291820\n",
      "Epoch 119, loss: 2.291738\n",
      "Epoch 120, loss: 2.291654\n",
      "Epoch 121, loss: 2.291573\n",
      "Epoch 122, loss: 2.291488\n",
      "Epoch 123, loss: 2.291407\n",
      "Epoch 124, loss: 2.291324\n",
      "Epoch 125, loss: 2.291243\n",
      "Epoch 126, loss: 2.291159\n",
      "Epoch 127, loss: 2.291077\n",
      "Epoch 128, loss: 2.290997\n",
      "Epoch 129, loss: 2.290913\n",
      "Epoch 130, loss: 2.290832\n",
      "Epoch 131, loss: 2.290750\n",
      "Epoch 132, loss: 2.290667\n",
      "Epoch 133, loss: 2.290587\n",
      "Epoch 134, loss: 2.290504\n",
      "Epoch 135, loss: 2.290425\n",
      "Epoch 136, loss: 2.290342\n",
      "Epoch 137, loss: 2.290262\n",
      "Epoch 138, loss: 2.290176\n",
      "Epoch 139, loss: 2.290097\n",
      "Epoch 140, loss: 2.290017\n",
      "Epoch 141, loss: 2.289934\n",
      "Epoch 142, loss: 2.289852\n",
      "Epoch 143, loss: 2.289773\n",
      "Epoch 144, loss: 2.289691\n",
      "Epoch 145, loss: 2.289610\n",
      "Epoch 146, loss: 2.289529\n",
      "Epoch 147, loss: 2.289449\n",
      "Epoch 148, loss: 2.289367\n",
      "Epoch 149, loss: 2.289287\n",
      "Epoch 150, loss: 2.289207\n",
      "Epoch 151, loss: 2.289126\n",
      "Epoch 152, loss: 2.289047\n",
      "Epoch 153, loss: 2.288965\n",
      "Epoch 154, loss: 2.288885\n",
      "Epoch 155, loss: 2.288806\n",
      "Epoch 156, loss: 2.288727\n",
      "Epoch 157, loss: 2.288644\n",
      "Epoch 158, loss: 2.288566\n",
      "Epoch 159, loss: 2.288485\n",
      "Epoch 160, loss: 2.288405\n",
      "Epoch 161, loss: 2.288325\n",
      "Epoch 162, loss: 2.288245\n",
      "Epoch 163, loss: 2.288165\n",
      "Epoch 164, loss: 2.288087\n",
      "Epoch 165, loss: 2.288006\n",
      "Epoch 166, loss: 2.287925\n",
      "Epoch 167, loss: 2.287846\n",
      "Epoch 168, loss: 2.287766\n",
      "Epoch 169, loss: 2.287688\n",
      "Epoch 170, loss: 2.287609\n",
      "Epoch 171, loss: 2.287531\n",
      "Epoch 172, loss: 2.287452\n",
      "Epoch 173, loss: 2.287371\n",
      "Epoch 174, loss: 2.287293\n",
      "Epoch 175, loss: 2.287214\n",
      "Epoch 176, loss: 2.287135\n",
      "Epoch 177, loss: 2.287055\n",
      "Epoch 178, loss: 2.286977\n",
      "Epoch 179, loss: 2.286899\n",
      "Epoch 180, loss: 2.286821\n",
      "Epoch 181, loss: 2.286741\n",
      "Epoch 182, loss: 2.286663\n",
      "Epoch 183, loss: 2.286585\n",
      "Epoch 184, loss: 2.286506\n",
      "Epoch 185, loss: 2.286428\n",
      "Epoch 186, loss: 2.286349\n",
      "Epoch 187, loss: 2.286273\n",
      "Epoch 188, loss: 2.286194\n",
      "Epoch 189, loss: 2.286115\n",
      "Epoch 190, loss: 2.286036\n",
      "Epoch 191, loss: 2.285960\n",
      "Epoch 192, loss: 2.285882\n",
      "Epoch 193, loss: 2.285805\n",
      "Epoch 194, loss: 2.285726\n",
      "Epoch 195, loss: 2.285645\n",
      "Epoch 196, loss: 2.285570\n",
      "Epoch 197, loss: 2.285492\n",
      "Epoch 198, loss: 2.285413\n",
      "Epoch 199, loss: 2.285337\n",
      "This accuracy = 0.174\n",
      "Start learning_rates = 0.0001, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 2.302511\n",
      "Epoch 1, loss: 2.302396\n",
      "Epoch 2, loss: 2.302282\n",
      "Epoch 3, loss: 2.302170\n",
      "Epoch 4, loss: 2.302057\n",
      "Epoch 5, loss: 2.301946\n",
      "Epoch 6, loss: 2.301837\n",
      "Epoch 7, loss: 2.301727\n",
      "Epoch 8, loss: 2.301620\n",
      "Epoch 9, loss: 2.301512\n",
      "Epoch 10, loss: 2.301404\n",
      "Epoch 11, loss: 2.301299\n",
      "Epoch 12, loss: 2.301194\n",
      "Epoch 13, loss: 2.301088\n",
      "Epoch 14, loss: 2.300984\n",
      "Epoch 15, loss: 2.300880\n",
      "Epoch 16, loss: 2.300779\n",
      "Epoch 17, loss: 2.300677\n",
      "Epoch 18, loss: 2.300573\n",
      "Epoch 19, loss: 2.300471\n",
      "Epoch 20, loss: 2.300372\n",
      "Epoch 21, loss: 2.300273\n",
      "Epoch 22, loss: 2.300172\n",
      "Epoch 23, loss: 2.300075\n",
      "Epoch 24, loss: 2.299975\n",
      "Epoch 25, loss: 2.299877\n",
      "Epoch 26, loss: 2.299778\n",
      "Epoch 27, loss: 2.299684\n",
      "Epoch 28, loss: 2.299586\n",
      "Epoch 29, loss: 2.299488\n",
      "Epoch 30, loss: 2.299394\n",
      "Epoch 31, loss: 2.299296\n",
      "Epoch 32, loss: 2.299201\n",
      "Epoch 33, loss: 2.299107\n",
      "Epoch 34, loss: 2.299011\n",
      "Epoch 35, loss: 2.298917\n",
      "Epoch 36, loss: 2.298823\n",
      "Epoch 37, loss: 2.298730\n",
      "Epoch 38, loss: 2.298638\n",
      "Epoch 39, loss: 2.298541\n",
      "Epoch 40, loss: 2.298449\n",
      "Epoch 41, loss: 2.298356\n",
      "Epoch 42, loss: 2.298264\n",
      "Epoch 43, loss: 2.298173\n",
      "Epoch 44, loss: 2.298080\n",
      "Epoch 45, loss: 2.297988\n",
      "Epoch 46, loss: 2.297898\n",
      "Epoch 47, loss: 2.297809\n",
      "Epoch 48, loss: 2.297715\n",
      "Epoch 49, loss: 2.297624\n",
      "Epoch 50, loss: 2.297531\n",
      "Epoch 51, loss: 2.297442\n",
      "Epoch 52, loss: 2.297354\n",
      "Epoch 53, loss: 2.297264\n",
      "Epoch 54, loss: 2.297172\n",
      "Epoch 55, loss: 2.297084\n",
      "Epoch 56, loss: 2.296993\n",
      "Epoch 57, loss: 2.296904\n",
      "Epoch 58, loss: 2.296815\n",
      "Epoch 59, loss: 2.296726\n",
      "Epoch 60, loss: 2.296638\n",
      "Epoch 61, loss: 2.296549\n",
      "Epoch 62, loss: 2.296459\n",
      "Epoch 63, loss: 2.296372\n",
      "Epoch 64, loss: 2.296283\n",
      "Epoch 65, loss: 2.296194\n",
      "Epoch 66, loss: 2.296108\n",
      "Epoch 67, loss: 2.296018\n",
      "Epoch 68, loss: 2.295930\n",
      "Epoch 69, loss: 2.295845\n",
      "Epoch 70, loss: 2.295757\n",
      "Epoch 71, loss: 2.295670\n",
      "Epoch 72, loss: 2.295582\n",
      "Epoch 73, loss: 2.295495\n",
      "Epoch 74, loss: 2.295407\n",
      "Epoch 75, loss: 2.295320\n",
      "Epoch 76, loss: 2.295235\n",
      "Epoch 77, loss: 2.295147\n",
      "Epoch 78, loss: 2.295060\n",
      "Epoch 79, loss: 2.294976\n",
      "Epoch 80, loss: 2.294889\n",
      "Epoch 81, loss: 2.294803\n",
      "Epoch 82, loss: 2.294716\n",
      "Epoch 83, loss: 2.294629\n",
      "Epoch 84, loss: 2.294546\n",
      "Epoch 85, loss: 2.294458\n",
      "Epoch 86, loss: 2.294371\n",
      "Epoch 87, loss: 2.294287\n",
      "Epoch 88, loss: 2.294201\n",
      "Epoch 89, loss: 2.294116\n",
      "Epoch 90, loss: 2.294033\n",
      "Epoch 91, loss: 2.293947\n",
      "Epoch 92, loss: 2.293862\n",
      "Epoch 93, loss: 2.293777\n",
      "Epoch 94, loss: 2.293692\n",
      "Epoch 95, loss: 2.293605\n",
      "Epoch 96, loss: 2.293523\n",
      "Epoch 97, loss: 2.293438\n",
      "Epoch 98, loss: 2.293353\n",
      "Epoch 99, loss: 2.293268\n",
      "Epoch 100, loss: 2.293186\n",
      "Epoch 101, loss: 2.293098\n",
      "Epoch 102, loss: 2.293016\n",
      "Epoch 103, loss: 2.292932\n",
      "Epoch 104, loss: 2.292850\n",
      "Epoch 105, loss: 2.292765\n",
      "Epoch 106, loss: 2.292681\n",
      "Epoch 107, loss: 2.292598\n",
      "Epoch 108, loss: 2.292513\n",
      "Epoch 109, loss: 2.292429\n",
      "Epoch 110, loss: 2.292345\n",
      "Epoch 111, loss: 2.292261\n",
      "Epoch 112, loss: 2.292178\n",
      "Epoch 113, loss: 2.292096\n",
      "Epoch 114, loss: 2.292013\n",
      "Epoch 115, loss: 2.291931\n",
      "Epoch 116, loss: 2.291846\n",
      "Epoch 117, loss: 2.291764\n",
      "Epoch 118, loss: 2.291682\n",
      "Epoch 119, loss: 2.291596\n",
      "Epoch 120, loss: 2.291514\n",
      "Epoch 121, loss: 2.291432\n",
      "Epoch 122, loss: 2.291349\n",
      "Epoch 123, loss: 2.291266\n",
      "Epoch 124, loss: 2.291183\n",
      "Epoch 125, loss: 2.291102\n",
      "Epoch 126, loss: 2.291019\n",
      "Epoch 127, loss: 2.290937\n",
      "Epoch 128, loss: 2.290857\n",
      "Epoch 129, loss: 2.290772\n",
      "Epoch 130, loss: 2.290691\n",
      "Epoch 131, loss: 2.290610\n",
      "Epoch 132, loss: 2.290527\n",
      "Epoch 133, loss: 2.290444\n",
      "Epoch 134, loss: 2.290364\n",
      "Epoch 135, loss: 2.290281\n",
      "Epoch 136, loss: 2.290199\n",
      "Epoch 137, loss: 2.290118\n",
      "Epoch 138, loss: 2.290038\n",
      "Epoch 139, loss: 2.289957\n",
      "Epoch 140, loss: 2.289874\n",
      "Epoch 141, loss: 2.289794\n",
      "Epoch 142, loss: 2.289713\n",
      "Epoch 143, loss: 2.289632\n",
      "Epoch 144, loss: 2.289550\n",
      "Epoch 145, loss: 2.289469\n",
      "Epoch 146, loss: 2.289388\n",
      "Epoch 147, loss: 2.289307\n",
      "Epoch 148, loss: 2.289227\n",
      "Epoch 149, loss: 2.289146\n",
      "Epoch 150, loss: 2.289066\n",
      "Epoch 151, loss: 2.288985\n",
      "Epoch 152, loss: 2.288905\n",
      "Epoch 153, loss: 2.288825\n",
      "Epoch 154, loss: 2.288744\n",
      "Epoch 155, loss: 2.288662\n",
      "Epoch 156, loss: 2.288583\n",
      "Epoch 157, loss: 2.288503\n",
      "Epoch 158, loss: 2.288420\n",
      "Epoch 159, loss: 2.288342\n",
      "Epoch 160, loss: 2.288262\n",
      "Epoch 161, loss: 2.288183\n",
      "Epoch 162, loss: 2.288102\n",
      "Epoch 163, loss: 2.288022\n",
      "Epoch 164, loss: 2.287944\n",
      "Epoch 165, loss: 2.287863\n",
      "Epoch 166, loss: 2.287783\n",
      "Epoch 167, loss: 2.287706\n",
      "Epoch 168, loss: 2.287624\n",
      "Epoch 169, loss: 2.287546\n",
      "Epoch 170, loss: 2.287467\n",
      "Epoch 171, loss: 2.287386\n",
      "Epoch 172, loss: 2.287307\n",
      "Epoch 173, loss: 2.287229\n",
      "Epoch 174, loss: 2.287151\n",
      "Epoch 175, loss: 2.287070\n",
      "Epoch 176, loss: 2.286992\n",
      "Epoch 177, loss: 2.286911\n",
      "Epoch 178, loss: 2.286834\n",
      "Epoch 179, loss: 2.286754\n",
      "Epoch 180, loss: 2.286676\n",
      "Epoch 181, loss: 2.286598\n",
      "Epoch 182, loss: 2.286518\n",
      "Epoch 183, loss: 2.286439\n",
      "Epoch 184, loss: 2.286361\n",
      "Epoch 185, loss: 2.286283\n",
      "Epoch 186, loss: 2.286205\n",
      "Epoch 187, loss: 2.286127\n",
      "Epoch 188, loss: 2.286047\n",
      "Epoch 189, loss: 2.285972\n",
      "Epoch 190, loss: 2.285892\n",
      "Epoch 191, loss: 2.285816\n",
      "Epoch 192, loss: 2.285738\n",
      "Epoch 193, loss: 2.285659\n",
      "Epoch 194, loss: 2.285580\n",
      "Epoch 195, loss: 2.285503\n",
      "Epoch 196, loss: 2.285425\n",
      "Epoch 197, loss: 2.285347\n",
      "Epoch 198, loss: 2.285270\n",
      "Epoch 199, loss: 2.285193\n",
      "This accuracy = 0.172\n",
      "Start learning_rates = 1e-05, reg_strengths = 0.0001\n",
      "Epoch 0, loss: 2.302648\n",
      "Epoch 1, loss: 2.302637\n",
      "Epoch 2, loss: 2.302625\n",
      "Epoch 3, loss: 2.302614\n",
      "Epoch 4, loss: 2.302603\n",
      "Epoch 5, loss: 2.302592\n",
      "Epoch 6, loss: 2.302581\n",
      "Epoch 7, loss: 2.302570\n",
      "Epoch 8, loss: 2.302559\n",
      "Epoch 9, loss: 2.302548\n",
      "Epoch 10, loss: 2.302537\n",
      "Epoch 11, loss: 2.302526\n",
      "Epoch 12, loss: 2.302515\n",
      "Epoch 13, loss: 2.302503\n",
      "Epoch 14, loss: 2.302493\n",
      "Epoch 15, loss: 2.302482\n",
      "Epoch 16, loss: 2.302470\n",
      "Epoch 17, loss: 2.302460\n",
      "Epoch 18, loss: 2.302448\n",
      "Epoch 19, loss: 2.302438\n",
      "Epoch 20, loss: 2.302427\n",
      "Epoch 21, loss: 2.302416\n",
      "Epoch 22, loss: 2.302405\n",
      "Epoch 23, loss: 2.302394\n",
      "Epoch 24, loss: 2.302383\n",
      "Epoch 25, loss: 2.302372\n",
      "Epoch 26, loss: 2.302361\n",
      "Epoch 27, loss: 2.302350\n",
      "Epoch 28, loss: 2.302339\n",
      "Epoch 29, loss: 2.302328\n",
      "Epoch 30, loss: 2.302317\n",
      "Epoch 31, loss: 2.302306\n",
      "Epoch 32, loss: 2.302296\n",
      "Epoch 33, loss: 2.302285\n",
      "Epoch 34, loss: 2.302274\n",
      "Epoch 35, loss: 2.302263\n",
      "Epoch 36, loss: 2.302252\n",
      "Epoch 37, loss: 2.302242\n",
      "Epoch 38, loss: 2.302230\n",
      "Epoch 39, loss: 2.302220\n",
      "Epoch 40, loss: 2.302209\n",
      "Epoch 41, loss: 2.302198\n",
      "Epoch 42, loss: 2.302187\n",
      "Epoch 43, loss: 2.302176\n",
      "Epoch 44, loss: 2.302166\n",
      "Epoch 45, loss: 2.302155\n",
      "Epoch 46, loss: 2.302144\n",
      "Epoch 47, loss: 2.302133\n",
      "Epoch 48, loss: 2.302123\n",
      "Epoch 49, loss: 2.302112\n",
      "Epoch 50, loss: 2.302101\n",
      "Epoch 51, loss: 2.302090\n",
      "Epoch 52, loss: 2.302080\n",
      "Epoch 53, loss: 2.302069\n",
      "Epoch 54, loss: 2.302059\n",
      "Epoch 55, loss: 2.302048\n",
      "Epoch 56, loss: 2.302037\n",
      "Epoch 57, loss: 2.302026\n",
      "Epoch 58, loss: 2.302016\n",
      "Epoch 59, loss: 2.302005\n",
      "Epoch 60, loss: 2.301994\n",
      "Epoch 61, loss: 2.301984\n",
      "Epoch 62, loss: 2.301973\n",
      "Epoch 63, loss: 2.301962\n",
      "Epoch 64, loss: 2.301952\n",
      "Epoch 65, loss: 2.301941\n",
      "Epoch 66, loss: 2.301930\n",
      "Epoch 67, loss: 2.301920\n",
      "Epoch 68, loss: 2.301909\n",
      "Epoch 69, loss: 2.301899\n",
      "Epoch 70, loss: 2.301888\n",
      "Epoch 71, loss: 2.301877\n",
      "Epoch 72, loss: 2.301867\n",
      "Epoch 73, loss: 2.301856\n",
      "Epoch 74, loss: 2.301846\n",
      "Epoch 75, loss: 2.301835\n",
      "Epoch 76, loss: 2.301825\n",
      "Epoch 77, loss: 2.301814\n",
      "Epoch 78, loss: 2.301804\n",
      "Epoch 79, loss: 2.301793\n",
      "Epoch 80, loss: 2.301783\n",
      "Epoch 81, loss: 2.301772\n",
      "Epoch 82, loss: 2.301762\n",
      "Epoch 83, loss: 2.301751\n",
      "Epoch 84, loss: 2.301741\n",
      "Epoch 85, loss: 2.301730\n",
      "Epoch 86, loss: 2.301720\n",
      "Epoch 87, loss: 2.301709\n",
      "Epoch 88, loss: 2.301699\n",
      "Epoch 89, loss: 2.301688\n",
      "Epoch 90, loss: 2.301678\n",
      "Epoch 91, loss: 2.301667\n",
      "Epoch 92, loss: 2.301657\n",
      "Epoch 93, loss: 2.301647\n",
      "Epoch 94, loss: 2.301636\n",
      "Epoch 95, loss: 2.301626\n",
      "Epoch 96, loss: 2.301615\n",
      "Epoch 97, loss: 2.301605\n",
      "Epoch 98, loss: 2.301594\n",
      "Epoch 99, loss: 2.301584\n",
      "Epoch 100, loss: 2.301574\n",
      "Epoch 101, loss: 2.301563\n",
      "Epoch 102, loss: 2.301553\n",
      "Epoch 103, loss: 2.301543\n",
      "Epoch 104, loss: 2.301532\n",
      "Epoch 105, loss: 2.301522\n",
      "Epoch 106, loss: 2.301511\n",
      "Epoch 107, loss: 2.301501\n",
      "Epoch 108, loss: 2.301491\n",
      "Epoch 109, loss: 2.301480\n",
      "Epoch 110, loss: 2.301470\n",
      "Epoch 111, loss: 2.301460\n",
      "Epoch 112, loss: 2.301449\n",
      "Epoch 113, loss: 2.301439\n",
      "Epoch 114, loss: 2.301429\n",
      "Epoch 115, loss: 2.301419\n",
      "Epoch 116, loss: 2.301408\n",
      "Epoch 117, loss: 2.301398\n",
      "Epoch 118, loss: 2.301388\n",
      "Epoch 119, loss: 2.301377\n",
      "Epoch 120, loss: 2.301367\n",
      "Epoch 121, loss: 2.301357\n",
      "Epoch 122, loss: 2.301347\n",
      "Epoch 123, loss: 2.301336\n",
      "Epoch 124, loss: 2.301326\n",
      "Epoch 125, loss: 2.301316\n",
      "Epoch 126, loss: 2.301306\n",
      "Epoch 127, loss: 2.301295\n",
      "Epoch 128, loss: 2.301285\n",
      "Epoch 129, loss: 2.301275\n",
      "Epoch 130, loss: 2.301265\n",
      "Epoch 131, loss: 2.301254\n",
      "Epoch 132, loss: 2.301244\n",
      "Epoch 133, loss: 2.301234\n",
      "Epoch 134, loss: 2.301224\n",
      "Epoch 135, loss: 2.301214\n",
      "Epoch 136, loss: 2.301204\n",
      "Epoch 137, loss: 2.301193\n",
      "Epoch 138, loss: 2.301183\n",
      "Epoch 139, loss: 2.301173\n",
      "Epoch 140, loss: 2.301163\n",
      "Epoch 141, loss: 2.301153\n",
      "Epoch 142, loss: 2.301142\n",
      "Epoch 143, loss: 2.301133\n",
      "Epoch 144, loss: 2.301122\n",
      "Epoch 145, loss: 2.301112\n",
      "Epoch 146, loss: 2.301102\n",
      "Epoch 147, loss: 2.301092\n",
      "Epoch 148, loss: 2.301082\n",
      "Epoch 149, loss: 2.301072\n",
      "Epoch 150, loss: 2.301061\n",
      "Epoch 151, loss: 2.301051\n",
      "Epoch 152, loss: 2.301041\n",
      "Epoch 153, loss: 2.301031\n",
      "Epoch 154, loss: 2.301021\n",
      "Epoch 155, loss: 2.301011\n",
      "Epoch 156, loss: 2.301001\n",
      "Epoch 157, loss: 2.300991\n",
      "Epoch 158, loss: 2.300981\n",
      "Epoch 159, loss: 2.300971\n",
      "Epoch 160, loss: 2.300961\n",
      "Epoch 161, loss: 2.300951\n",
      "Epoch 162, loss: 2.300941\n",
      "Epoch 163, loss: 2.300930\n",
      "Epoch 164, loss: 2.300920\n",
      "Epoch 165, loss: 2.300911\n",
      "Epoch 166, loss: 2.300900\n",
      "Epoch 167, loss: 2.300890\n",
      "Epoch 168, loss: 2.300880\n",
      "Epoch 169, loss: 2.300871\n",
      "Epoch 170, loss: 2.300860\n",
      "Epoch 171, loss: 2.300851\n",
      "Epoch 172, loss: 2.300840\n",
      "Epoch 173, loss: 2.300830\n",
      "Epoch 174, loss: 2.300820\n",
      "Epoch 175, loss: 2.300810\n",
      "Epoch 176, loss: 2.300800\n",
      "Epoch 177, loss: 2.300791\n",
      "Epoch 178, loss: 2.300781\n",
      "Epoch 179, loss: 2.300771\n",
      "Epoch 180, loss: 2.300761\n",
      "Epoch 181, loss: 2.300751\n",
      "Epoch 182, loss: 2.300741\n",
      "Epoch 183, loss: 2.300731\n",
      "Epoch 184, loss: 2.300721\n",
      "Epoch 185, loss: 2.300711\n",
      "Epoch 186, loss: 2.300701\n",
      "Epoch 187, loss: 2.300691\n",
      "Epoch 188, loss: 2.300681\n",
      "Epoch 189, loss: 2.300671\n",
      "Epoch 190, loss: 2.300661\n",
      "Epoch 191, loss: 2.300651\n",
      "Epoch 192, loss: 2.300641\n",
      "Epoch 193, loss: 2.300631\n",
      "Epoch 194, loss: 2.300622\n",
      "Epoch 195, loss: 2.300612\n",
      "Epoch 196, loss: 2.300602\n",
      "Epoch 197, loss: 2.300592\n",
      "Epoch 198, loss: 2.300582\n",
      "Epoch 199, loss: 2.300572\n",
      "This accuracy = 0.116\n",
      "Start learning_rates = 1e-05, reg_strengths = 1e-05\n",
      "Epoch 0, loss: 2.302660\n",
      "Epoch 1, loss: 2.302649\n",
      "Epoch 2, loss: 2.302637\n",
      "Epoch 3, loss: 2.302625\n",
      "Epoch 4, loss: 2.302614\n",
      "Epoch 5, loss: 2.302603\n",
      "Epoch 6, loss: 2.302591\n",
      "Epoch 7, loss: 2.302580\n",
      "Epoch 8, loss: 2.302568\n",
      "Epoch 9, loss: 2.302557\n",
      "Epoch 10, loss: 2.302545\n",
      "Epoch 11, loss: 2.302534\n",
      "Epoch 12, loss: 2.302522\n",
      "Epoch 13, loss: 2.302511\n",
      "Epoch 14, loss: 2.302500\n",
      "Epoch 15, loss: 2.302488\n",
      "Epoch 16, loss: 2.302477\n",
      "Epoch 17, loss: 2.302466\n",
      "Epoch 18, loss: 2.302454\n",
      "Epoch 19, loss: 2.302443\n",
      "Epoch 20, loss: 2.302432\n",
      "Epoch 21, loss: 2.302420\n",
      "Epoch 22, loss: 2.302409\n",
      "Epoch 23, loss: 2.302398\n",
      "Epoch 24, loss: 2.302386\n",
      "Epoch 25, loss: 2.302375\n",
      "Epoch 26, loss: 2.302364\n",
      "Epoch 27, loss: 2.302352\n",
      "Epoch 28, loss: 2.302341\n",
      "Epoch 29, loss: 2.302330\n",
      "Epoch 30, loss: 2.302319\n",
      "Epoch 31, loss: 2.302308\n",
      "Epoch 32, loss: 2.302296\n",
      "Epoch 33, loss: 2.302285\n",
      "Epoch 34, loss: 2.302274\n",
      "Epoch 35, loss: 2.302263\n",
      "Epoch 36, loss: 2.302251\n",
      "Epoch 37, loss: 2.302240\n",
      "Epoch 38, loss: 2.302229\n",
      "Epoch 39, loss: 2.302218\n",
      "Epoch 40, loss: 2.302207\n",
      "Epoch 41, loss: 2.302196\n",
      "Epoch 42, loss: 2.302185\n",
      "Epoch 43, loss: 2.302174\n",
      "Epoch 44, loss: 2.302163\n",
      "Epoch 45, loss: 2.302151\n",
      "Epoch 46, loss: 2.302140\n",
      "Epoch 47, loss: 2.302129\n",
      "Epoch 48, loss: 2.302118\n",
      "Epoch 49, loss: 2.302107\n",
      "Epoch 50, loss: 2.302096\n",
      "Epoch 51, loss: 2.302085\n",
      "Epoch 52, loss: 2.302074\n",
      "Epoch 53, loss: 2.302063\n",
      "Epoch 54, loss: 2.302052\n",
      "Epoch 55, loss: 2.302041\n",
      "Epoch 56, loss: 2.302030\n",
      "Epoch 57, loss: 2.302019\n",
      "Epoch 58, loss: 2.302008\n",
      "Epoch 59, loss: 2.301997\n",
      "Epoch 60, loss: 2.301986\n",
      "Epoch 61, loss: 2.301975\n",
      "Epoch 62, loss: 2.301964\n",
      "Epoch 63, loss: 2.301953\n",
      "Epoch 64, loss: 2.301942\n",
      "Epoch 65, loss: 2.301931\n",
      "Epoch 66, loss: 2.301920\n",
      "Epoch 67, loss: 2.301910\n",
      "Epoch 68, loss: 2.301899\n",
      "Epoch 69, loss: 2.301888\n",
      "Epoch 70, loss: 2.301877\n",
      "Epoch 71, loss: 2.301866\n",
      "Epoch 72, loss: 2.301855\n",
      "Epoch 73, loss: 2.301844\n",
      "Epoch 74, loss: 2.301834\n",
      "Epoch 75, loss: 2.301823\n",
      "Epoch 76, loss: 2.301812\n",
      "Epoch 77, loss: 2.301801\n",
      "Epoch 78, loss: 2.301790\n",
      "Epoch 79, loss: 2.301780\n",
      "Epoch 80, loss: 2.301769\n",
      "Epoch 81, loss: 2.301758\n",
      "Epoch 82, loss: 2.301747\n",
      "Epoch 83, loss: 2.301736\n",
      "Epoch 84, loss: 2.301726\n",
      "Epoch 85, loss: 2.301715\n",
      "Epoch 86, loss: 2.301704\n",
      "Epoch 87, loss: 2.301693\n",
      "Epoch 88, loss: 2.301683\n",
      "Epoch 89, loss: 2.301672\n",
      "Epoch 90, loss: 2.301661\n",
      "Epoch 91, loss: 2.301650\n",
      "Epoch 92, loss: 2.301640\n",
      "Epoch 93, loss: 2.301629\n",
      "Epoch 94, loss: 2.301618\n",
      "Epoch 95, loss: 2.301607\n",
      "Epoch 96, loss: 2.301597\n",
      "Epoch 97, loss: 2.301586\n",
      "Epoch 98, loss: 2.301575\n",
      "Epoch 99, loss: 2.301565\n",
      "Epoch 100, loss: 2.301554\n",
      "Epoch 101, loss: 2.301543\n",
      "Epoch 102, loss: 2.301533\n",
      "Epoch 103, loss: 2.301522\n",
      "Epoch 104, loss: 2.301512\n",
      "Epoch 105, loss: 2.301501\n",
      "Epoch 106, loss: 2.301491\n",
      "Epoch 107, loss: 2.301480\n",
      "Epoch 108, loss: 2.301469\n",
      "Epoch 109, loss: 2.301459\n",
      "Epoch 110, loss: 2.301448\n",
      "Epoch 111, loss: 2.301438\n",
      "Epoch 112, loss: 2.301427\n",
      "Epoch 113, loss: 2.301416\n",
      "Epoch 114, loss: 2.301406\n",
      "Epoch 115, loss: 2.301395\n",
      "Epoch 116, loss: 2.301385\n",
      "Epoch 117, loss: 2.301374\n",
      "Epoch 118, loss: 2.301364\n",
      "Epoch 119, loss: 2.301353\n",
      "Epoch 120, loss: 2.301343\n",
      "Epoch 121, loss: 2.301332\n",
      "Epoch 122, loss: 2.301322\n",
      "Epoch 123, loss: 2.301311\n",
      "Epoch 124, loss: 2.301301\n",
      "Epoch 125, loss: 2.301290\n",
      "Epoch 126, loss: 2.301280\n",
      "Epoch 127, loss: 2.301269\n",
      "Epoch 128, loss: 2.301259\n",
      "Epoch 129, loss: 2.301248\n",
      "Epoch 130, loss: 2.301238\n",
      "Epoch 131, loss: 2.301228\n",
      "Epoch 132, loss: 2.301217\n",
      "Epoch 133, loss: 2.301207\n",
      "Epoch 134, loss: 2.301196\n",
      "Epoch 135, loss: 2.301186\n",
      "Epoch 136, loss: 2.301175\n",
      "Epoch 137, loss: 2.301165\n",
      "Epoch 138, loss: 2.301155\n",
      "Epoch 139, loss: 2.301144\n",
      "Epoch 140, loss: 2.301134\n",
      "Epoch 141, loss: 2.301124\n",
      "Epoch 142, loss: 2.301113\n",
      "Epoch 143, loss: 2.301103\n",
      "Epoch 144, loss: 2.301092\n",
      "Epoch 145, loss: 2.301082\n",
      "Epoch 146, loss: 2.301072\n",
      "Epoch 147, loss: 2.301061\n",
      "Epoch 148, loss: 2.301051\n",
      "Epoch 149, loss: 2.301041\n",
      "Epoch 150, loss: 2.301031\n",
      "Epoch 151, loss: 2.301020\n",
      "Epoch 152, loss: 2.301010\n",
      "Epoch 153, loss: 2.301000\n",
      "Epoch 154, loss: 2.300989\n",
      "Epoch 155, loss: 2.300979\n",
      "Epoch 156, loss: 2.300969\n",
      "Epoch 157, loss: 2.300959\n",
      "Epoch 158, loss: 2.300948\n",
      "Epoch 159, loss: 2.300938\n",
      "Epoch 160, loss: 2.300928\n",
      "Epoch 161, loss: 2.300917\n",
      "Epoch 162, loss: 2.300907\n",
      "Epoch 163, loss: 2.300897\n",
      "Epoch 164, loss: 2.300887\n",
      "Epoch 165, loss: 2.300877\n",
      "Epoch 166, loss: 2.300866\n",
      "Epoch 167, loss: 2.300856\n",
      "Epoch 168, loss: 2.300846\n",
      "Epoch 169, loss: 2.300836\n",
      "Epoch 170, loss: 2.300825\n",
      "Epoch 171, loss: 2.300815\n",
      "Epoch 172, loss: 2.300805\n",
      "Epoch 173, loss: 2.300795\n",
      "Epoch 174, loss: 2.300785\n",
      "Epoch 175, loss: 2.300774\n",
      "Epoch 176, loss: 2.300764\n",
      "Epoch 177, loss: 2.300754\n",
      "Epoch 178, loss: 2.300744\n",
      "Epoch 179, loss: 2.300734\n",
      "Epoch 180, loss: 2.300724\n",
      "Epoch 181, loss: 2.300714\n",
      "Epoch 182, loss: 2.300703\n",
      "Epoch 183, loss: 2.300693\n",
      "Epoch 184, loss: 2.300683\n",
      "Epoch 185, loss: 2.300673\n",
      "Epoch 186, loss: 2.300663\n",
      "Epoch 187, loss: 2.300653\n",
      "Epoch 188, loss: 2.300643\n",
      "Epoch 189, loss: 2.300633\n",
      "Epoch 190, loss: 2.300623\n",
      "Epoch 191, loss: 2.300613\n",
      "Epoch 192, loss: 2.300602\n",
      "Epoch 193, loss: 2.300592\n",
      "Epoch 194, loss: 2.300583\n",
      "Epoch 195, loss: 2.300572\n",
      "Epoch 196, loss: 2.300562\n",
      "Epoch 197, loss: 2.300552\n",
      "Epoch 198, loss: 2.300542\n",
      "Epoch 199, loss: 2.300532\n",
      "This accuracy = 0.122\n",
      "Start learning_rates = 1e-05, reg_strengths = 1e-06\n",
      "Epoch 0, loss: 2.302570\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302547\n",
      "Epoch 3, loss: 2.302537\n",
      "Epoch 4, loss: 2.302525\n",
      "Epoch 5, loss: 2.302514\n",
      "Epoch 6, loss: 2.302503\n",
      "Epoch 7, loss: 2.302492\n",
      "Epoch 8, loss: 2.302481\n",
      "Epoch 9, loss: 2.302470\n",
      "Epoch 10, loss: 2.302459\n",
      "Epoch 11, loss: 2.302448\n",
      "Epoch 12, loss: 2.302437\n",
      "Epoch 13, loss: 2.302426\n",
      "Epoch 14, loss: 2.302415\n",
      "Epoch 15, loss: 2.302404\n",
      "Epoch 16, loss: 2.302394\n",
      "Epoch 17, loss: 2.302382\n",
      "Epoch 18, loss: 2.302371\n",
      "Epoch 19, loss: 2.302360\n",
      "Epoch 20, loss: 2.302350\n",
      "Epoch 21, loss: 2.302339\n",
      "Epoch 22, loss: 2.302328\n",
      "Epoch 23, loss: 2.302317\n",
      "Epoch 24, loss: 2.302306\n",
      "Epoch 25, loss: 2.302295\n",
      "Epoch 26, loss: 2.302284\n",
      "Epoch 27, loss: 2.302273\n",
      "Epoch 28, loss: 2.302262\n",
      "Epoch 29, loss: 2.302252\n",
      "Epoch 30, loss: 2.302241\n",
      "Epoch 31, loss: 2.302230\n",
      "Epoch 32, loss: 2.302219\n",
      "Epoch 33, loss: 2.302209\n",
      "Epoch 34, loss: 2.302198\n",
      "Epoch 35, loss: 2.302187\n",
      "Epoch 36, loss: 2.302176\n",
      "Epoch 37, loss: 2.302165\n",
      "Epoch 38, loss: 2.302154\n",
      "Epoch 39, loss: 2.302144\n",
      "Epoch 40, loss: 2.302133\n",
      "Epoch 41, loss: 2.302122\n",
      "Epoch 42, loss: 2.302111\n",
      "Epoch 43, loss: 2.302101\n",
      "Epoch 44, loss: 2.302090\n",
      "Epoch 45, loss: 2.302079\n",
      "Epoch 46, loss: 2.302068\n",
      "Epoch 47, loss: 2.302058\n",
      "Epoch 48, loss: 2.302047\n",
      "Epoch 49, loss: 2.302036\n",
      "Epoch 50, loss: 2.302026\n",
      "Epoch 51, loss: 2.302015\n",
      "Epoch 52, loss: 2.302004\n",
      "Epoch 53, loss: 2.301994\n",
      "Epoch 54, loss: 2.301983\n",
      "Epoch 55, loss: 2.301973\n",
      "Epoch 56, loss: 2.301962\n",
      "Epoch 57, loss: 2.301951\n",
      "Epoch 58, loss: 2.301941\n",
      "Epoch 59, loss: 2.301930\n",
      "Epoch 60, loss: 2.301919\n",
      "Epoch 61, loss: 2.301909\n",
      "Epoch 62, loss: 2.301898\n",
      "Epoch 63, loss: 2.301888\n",
      "Epoch 64, loss: 2.301877\n",
      "Epoch 65, loss: 2.301866\n",
      "Epoch 66, loss: 2.301856\n",
      "Epoch 67, loss: 2.301845\n",
      "Epoch 68, loss: 2.301835\n",
      "Epoch 69, loss: 2.301824\n",
      "Epoch 70, loss: 2.301814\n",
      "Epoch 71, loss: 2.301803\n",
      "Epoch 72, loss: 2.301792\n",
      "Epoch 73, loss: 2.301782\n",
      "Epoch 74, loss: 2.301772\n",
      "Epoch 75, loss: 2.301761\n",
      "Epoch 76, loss: 2.301750\n",
      "Epoch 77, loss: 2.301740\n",
      "Epoch 78, loss: 2.301729\n",
      "Epoch 79, loss: 2.301719\n",
      "Epoch 80, loss: 2.301709\n",
      "Epoch 81, loss: 2.301698\n",
      "Epoch 82, loss: 2.301688\n",
      "Epoch 83, loss: 2.301677\n",
      "Epoch 84, loss: 2.301667\n",
      "Epoch 85, loss: 2.301656\n",
      "Epoch 86, loss: 2.301646\n",
      "Epoch 87, loss: 2.301635\n",
      "Epoch 88, loss: 2.301625\n",
      "Epoch 89, loss: 2.301615\n",
      "Epoch 90, loss: 2.301604\n",
      "Epoch 91, loss: 2.301594\n",
      "Epoch 92, loss: 2.301583\n",
      "Epoch 93, loss: 2.301573\n",
      "Epoch 94, loss: 2.301563\n",
      "Epoch 95, loss: 2.301552\n",
      "Epoch 96, loss: 2.301542\n",
      "Epoch 97, loss: 2.301532\n",
      "Epoch 98, loss: 2.301521\n",
      "Epoch 99, loss: 2.301511\n",
      "Epoch 100, loss: 2.301500\n",
      "Epoch 101, loss: 2.301490\n",
      "Epoch 102, loss: 2.301480\n",
      "Epoch 103, loss: 2.301469\n",
      "Epoch 104, loss: 2.301459\n",
      "Epoch 105, loss: 2.301449\n",
      "Epoch 106, loss: 2.301439\n",
      "Epoch 107, loss: 2.301428\n",
      "Epoch 108, loss: 2.301418\n",
      "Epoch 109, loss: 2.301408\n",
      "Epoch 110, loss: 2.301397\n",
      "Epoch 111, loss: 2.301387\n",
      "Epoch 112, loss: 2.301377\n",
      "Epoch 113, loss: 2.301366\n",
      "Epoch 114, loss: 2.301356\n",
      "Epoch 115, loss: 2.301346\n",
      "Epoch 116, loss: 2.301336\n",
      "Epoch 117, loss: 2.301325\n",
      "Epoch 118, loss: 2.301315\n",
      "Epoch 119, loss: 2.301305\n",
      "Epoch 120, loss: 2.301295\n",
      "Epoch 121, loss: 2.301285\n",
      "Epoch 122, loss: 2.301274\n",
      "Epoch 123, loss: 2.301264\n",
      "Epoch 124, loss: 2.301254\n",
      "Epoch 125, loss: 2.301244\n",
      "Epoch 126, loss: 2.301234\n",
      "Epoch 127, loss: 2.301223\n",
      "Epoch 128, loss: 2.301213\n",
      "Epoch 129, loss: 2.301203\n",
      "Epoch 130, loss: 2.301193\n",
      "Epoch 131, loss: 2.301183\n",
      "Epoch 132, loss: 2.301172\n",
      "Epoch 133, loss: 2.301162\n",
      "Epoch 134, loss: 2.301152\n",
      "Epoch 135, loss: 2.301142\n",
      "Epoch 136, loss: 2.301132\n",
      "Epoch 137, loss: 2.301122\n",
      "Epoch 138, loss: 2.301112\n",
      "Epoch 139, loss: 2.301101\n",
      "Epoch 140, loss: 2.301091\n",
      "Epoch 141, loss: 2.301081\n",
      "Epoch 142, loss: 2.301071\n",
      "Epoch 143, loss: 2.301061\n",
      "Epoch 144, loss: 2.301051\n",
      "Epoch 145, loss: 2.301041\n",
      "Epoch 146, loss: 2.301031\n",
      "Epoch 147, loss: 2.301021\n",
      "Epoch 148, loss: 2.301011\n",
      "Epoch 149, loss: 2.301000\n",
      "Epoch 150, loss: 2.300990\n",
      "Epoch 151, loss: 2.300980\n",
      "Epoch 152, loss: 2.300970\n",
      "Epoch 153, loss: 2.300960\n",
      "Epoch 154, loss: 2.300950\n",
      "Epoch 155, loss: 2.300940\n",
      "Epoch 156, loss: 2.300930\n",
      "Epoch 157, loss: 2.300920\n",
      "Epoch 158, loss: 2.300910\n",
      "Epoch 159, loss: 2.300900\n",
      "Epoch 160, loss: 2.300890\n",
      "Epoch 161, loss: 2.300880\n",
      "Epoch 162, loss: 2.300870\n",
      "Epoch 163, loss: 2.300860\n",
      "Epoch 164, loss: 2.300850\n",
      "Epoch 165, loss: 2.300840\n",
      "Epoch 166, loss: 2.300830\n",
      "Epoch 167, loss: 2.300820\n",
      "Epoch 168, loss: 2.300810\n",
      "Epoch 169, loss: 2.300800\n",
      "Epoch 170, loss: 2.300790\n",
      "Epoch 171, loss: 2.300780\n",
      "Epoch 172, loss: 2.300770\n",
      "Epoch 173, loss: 2.300760\n",
      "Epoch 174, loss: 2.300750\n",
      "Epoch 175, loss: 2.300741\n",
      "Epoch 176, loss: 2.300730\n",
      "Epoch 177, loss: 2.300720\n",
      "Epoch 178, loss: 2.300710\n",
      "Epoch 179, loss: 2.300701\n",
      "Epoch 180, loss: 2.300691\n",
      "Epoch 181, loss: 2.300681\n",
      "Epoch 182, loss: 2.300671\n",
      "Epoch 183, loss: 2.300661\n",
      "Epoch 184, loss: 2.300651\n",
      "Epoch 185, loss: 2.300641\n",
      "Epoch 186, loss: 2.300631\n",
      "Epoch 187, loss: 2.300621\n",
      "Epoch 188, loss: 2.300612\n",
      "Epoch 189, loss: 2.300602\n",
      "Epoch 190, loss: 2.300592\n",
      "Epoch 191, loss: 2.300582\n",
      "Epoch 192, loss: 2.300572\n",
      "Epoch 193, loss: 2.300562\n",
      "Epoch 194, loss: 2.300552\n",
      "Epoch 195, loss: 2.300543\n",
      "Epoch 196, loss: 2.300533\n",
      "Epoch 197, loss: 2.300523\n",
      "Epoch 198, loss: 2.300513\n",
      "Epoch 199, loss: 2.300503\n",
      "This accuracy = 0.101\n",
      "best validation accuracy achieved: 0.230000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        print(f'Start learning_rates = {lr}, reg_strengths = {rs}')\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        print(f'This accuracy = {accuracy}')\n",
    "        if (best_val_accuracy is None) or (accuracy > best_val_accuracy):\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.195000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
