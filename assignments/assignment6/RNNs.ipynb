{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMnl_P3PRYWm",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch\n",
        "!pip3 -qq install bokeh\n",
        "!pip3 -qq install gensim\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b22f0738-05c1-435a-bab5-ff3e60ea721b"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "69c06726-8ed6-4edc-b37b-db5edf086166"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "33adf57b-9aa0-48d1-ae38-b30b4d8f5b95"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f97cb1af-8637-477b-817e-0f9496c853b4"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'VERB', 'ADJ', 'X', 'NOUN', 'PRON', 'NUM', 'CONJ', '.', 'PRT', 'ADP', 'ADV', 'DET'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "f2a7e519-8ecc-49f5-9bda-0f3b0150c4a9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77d11230-d742-49fe-ebc4-3622a1752458"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45f4b14b-2e0d-4143-ad4f-f9b69061935b"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df16e00d-836a-425f-ee38-99df1636543b"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1be7530e-150d-4090-84f6-0031f62174e7"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count)\n",
        "        self.fc = nn.Linear(in_features=lstm_hidden_dim,\n",
        "                            out_features=tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.emb_layer(inputs)\n",
        "        output, _ = self.lstm(emb)\n",
        "\n",
        "        return self.fc(output)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f9f7167d-4588-4755-f1c5-4dc01672c0d6"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "pred = torch.argmax(logits, dim=-1)\n",
        "total_samples = float(torch.sum(y_batch != 0))\n",
        "correct_samples = float(torch.sum((pred == y_batch) * (y_batch != 0)))\n",
        "ave_acc = float(correct_samples / total_samples)\n",
        "\n",
        "print(f'correct_samples = {correct_samples}')\n",
        "print(f'total_samples = {total_samples}')\n",
        "print(f'Average accuracy = {ave_acc}')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct_samples = 7.0\n",
            "total_samples = 92.0\n",
            "Average accuracy = 0.07608695652173914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d033adc6-54b3-4d1f-8957-19b7ae0f9635"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "logits = model(X_batch)\n",
        "\n",
        "loss = 0\n",
        "for i, y_pred in enumerate(logits):\n",
        "    loss += criterion(y_pred, y_batch[i])\n",
        "\n",
        "print(f'loss = {loss}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = 81.7669677734375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "\n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "\n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "\n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = 0\n",
        "                for j, y_pred in enumerate(logits):\n",
        "                    loss += criterion(y_pred, y_batch[j])\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                pred = torch.argmax(logits, dim=-1)\n",
        "                cur_sum_count = float(torch.sum(y_batch != 0))\n",
        "                cur_correct_count = float(torch.sum((pred == y_batch) * (y_batch != 0)))\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af3f3225-c7d8-40f1-d846-5d67d13044da"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 49.87001, Accuracy = 73.75%: 100%|██████████| 572/572 [00:10<00:00, 54.25it/s]\n",
            "[1 / 50]   Val: Loss = 44.21222, Accuracy = 84.26%: 100%|██████████| 13/13 [00:00<00:00, 74.11it/s]\n",
            "[2 / 50] Train: Loss = 23.19467, Accuracy = 87.38%: 100%|██████████| 572/572 [00:10<00:00, 54.64it/s]\n",
            "[2 / 50]   Val: Loss = 29.99953, Accuracy = 89.47%: 100%|██████████| 13/13 [00:00<00:00, 65.10it/s]\n",
            "[3 / 50] Train: Loss = 15.98665, Accuracy = 91.34%: 100%|██████████| 572/572 [00:10<00:00, 56.98it/s]\n",
            "[3 / 50]   Val: Loss = 22.86872, Accuracy = 91.95%: 100%|██████████| 13/13 [00:00<00:00, 76.20it/s]\n",
            "[4 / 50] Train: Loss = 11.76021, Accuracy = 93.43%: 100%|██████████| 572/572 [00:10<00:00, 54.51it/s]\n",
            "[4 / 50]   Val: Loss = 18.98970, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 73.92it/s]\n",
            "[5 / 50] Train: Loss = 8.82670, Accuracy = 94.71%: 100%|██████████| 572/572 [00:10<00:00, 55.87it/s]\n",
            "[5 / 50]   Val: Loss = 17.16704, Accuracy = 93.80%: 100%|██████████| 13/13 [00:00<00:00, 69.51it/s]\n",
            "[6 / 50] Train: Loss = 6.82716, Accuracy = 95.59%: 100%|██████████| 572/572 [00:10<00:00, 56.34it/s]\n",
            "[6 / 50]   Val: Loss = 16.44938, Accuracy = 94.26%: 100%|██████████| 13/13 [00:00<00:00, 77.97it/s]\n",
            "[7 / 50] Train: Loss = 5.33017, Accuracy = 96.29%: 100%|██████████| 572/572 [00:10<00:00, 55.51it/s]\n",
            "[7 / 50]   Val: Loss = 14.81977, Accuracy = 94.67%: 100%|██████████| 13/13 [00:00<00:00, 71.73it/s]\n",
            "[8 / 50] Train: Loss = 4.27297, Accuracy = 96.84%: 100%|██████████| 572/572 [00:10<00:00, 56.78it/s]\n",
            "[8 / 50]   Val: Loss = 15.59200, Accuracy = 94.80%: 100%|██████████| 13/13 [00:00<00:00, 66.11it/s]\n",
            "[9 / 50] Train: Loss = 3.47076, Accuracy = 97.26%: 100%|██████████| 572/572 [00:10<00:00, 55.94it/s]\n",
            "[9 / 50]   Val: Loss = 15.29011, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 76.80it/s]\n",
            "[10 / 50] Train: Loss = 2.87375, Accuracy = 97.60%: 100%|██████████| 572/572 [00:10<00:00, 54.88it/s]\n",
            "[10 / 50]   Val: Loss = 14.94022, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 65.19it/s]\n",
            "[11 / 50] Train: Loss = 2.36300, Accuracy = 97.93%: 100%|██████████| 572/572 [00:10<00:00, 54.40it/s]\n",
            "[11 / 50]   Val: Loss = 15.81507, Accuracy = 95.09%: 100%|██████████| 13/13 [00:00<00:00, 57.63it/s]\n",
            "[12 / 50] Train: Loss = 1.98025, Accuracy = 98.19%: 100%|██████████| 572/572 [00:10<00:00, 56.06it/s]\n",
            "[12 / 50]   Val: Loss = 16.81486, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 74.23it/s]\n",
            "[13 / 50] Train: Loss = 1.63154, Accuracy = 98.45%: 100%|██████████| 572/572 [00:10<00:00, 55.38it/s]\n",
            "[13 / 50]   Val: Loss = 18.36283, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 74.48it/s]\n",
            "[14 / 50] Train: Loss = 1.32295, Accuracy = 98.69%: 100%|██████████| 572/572 [00:10<00:00, 55.66it/s]\n",
            "[14 / 50]   Val: Loss = 18.72566, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 74.36it/s]\n",
            "[15 / 50] Train: Loss = 1.10578, Accuracy = 98.88%: 100%|██████████| 572/572 [00:10<00:00, 56.90it/s]\n",
            "[15 / 50]   Val: Loss = 19.69785, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 74.96it/s]\n",
            "[16 / 50] Train: Loss = 0.88004, Accuracy = 99.07%: 100%|██████████| 572/572 [00:10<00:00, 55.67it/s]\n",
            "[16 / 50]   Val: Loss = 21.06320, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 70.80it/s]\n",
            "[17 / 50] Train: Loss = 0.70725, Accuracy = 99.23%: 100%|██████████| 572/572 [00:10<00:00, 54.70it/s]\n",
            "[17 / 50]   Val: Loss = 21.41114, Accuracy = 95.05%: 100%|██████████| 13/13 [00:00<00:00, 71.07it/s]\n",
            "[18 / 50] Train: Loss = 0.57743, Accuracy = 99.37%: 100%|██████████| 572/572 [00:10<00:00, 54.74it/s]\n",
            "[18 / 50]   Val: Loss = 21.01231, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 70.28it/s]\n",
            "[19 / 50] Train: Loss = 0.54528, Accuracy = 99.41%: 100%|██████████| 572/572 [00:10<00:00, 53.62it/s]\n",
            "[19 / 50]   Val: Loss = 22.64333, Accuracy = 94.84%: 100%|██████████| 13/13 [00:00<00:00, 68.21it/s]\n",
            "[20 / 50] Train: Loss = 0.52463, Accuracy = 99.44%: 100%|██████████| 572/572 [00:10<00:00, 54.90it/s]\n",
            "[20 / 50]   Val: Loss = 21.29223, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 71.58it/s]\n",
            "[21 / 50] Train: Loss = 0.33433, Accuracy = 99.62%: 100%|██████████| 572/572 [00:10<00:00, 53.12it/s]\n",
            "[21 / 50]   Val: Loss = 24.59247, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 70.30it/s]\n",
            "[22 / 50] Train: Loss = 0.24541, Accuracy = 99.72%: 100%|██████████| 572/572 [00:10<00:00, 55.54it/s]\n",
            "[22 / 50]   Val: Loss = 25.65466, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 70.88it/s]\n",
            "[23 / 50] Train: Loss = 0.19448, Accuracy = 99.76%: 100%|██████████| 572/572 [00:10<00:00, 55.18it/s]\n",
            "[23 / 50]   Val: Loss = 27.79172, Accuracy = 94.88%: 100%|██████████| 13/13 [00:00<00:00, 70.84it/s]\n",
            "[24 / 50] Train: Loss = 0.16326, Accuracy = 99.79%: 100%|██████████| 572/572 [00:10<00:00, 52.24it/s]\n",
            "[24 / 50]   Val: Loss = 26.22240, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 71.28it/s]\n",
            "[25 / 50] Train: Loss = 0.16148, Accuracy = 99.79%: 100%|██████████| 572/572 [00:10<00:00, 53.64it/s]\n",
            "[25 / 50]   Val: Loss = 27.43866, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 69.61it/s]\n",
            "[26 / 50] Train: Loss = 0.66139, Accuracy = 99.41%: 100%|██████████| 572/572 [00:10<00:00, 55.38it/s]\n",
            "[26 / 50]   Val: Loss = 28.54374, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 72.09it/s]\n",
            "[27 / 50] Train: Loss = 0.35086, Accuracy = 99.60%: 100%|██████████| 572/572 [00:10<00:00, 53.52it/s]\n",
            "[27 / 50]   Val: Loss = 28.94469, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 67.45it/s]\n",
            "[28 / 50] Train: Loss = 0.15153, Accuracy = 99.80%: 100%|██████████| 572/572 [00:10<00:00, 55.59it/s]\n",
            "[28 / 50]   Val: Loss = 28.36059, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 67.15it/s]\n",
            "[29 / 50] Train: Loss = 0.11252, Accuracy = 99.83%: 100%|██████████| 572/572 [00:10<00:00, 53.26it/s]\n",
            "[29 / 50]   Val: Loss = 27.71040, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 71.40it/s]\n",
            "[30 / 50] Train: Loss = 0.10269, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 54.25it/s]\n",
            "[30 / 50]   Val: Loss = 27.55769, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 71.32it/s]\n",
            "[31 / 50] Train: Loss = 0.09729, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 54.79it/s]\n",
            "[31 / 50]   Val: Loss = 29.95279, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 73.51it/s]\n",
            "[32 / 50] Train: Loss = 0.09595, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 53.25it/s]\n",
            "[32 / 50]   Val: Loss = 31.14930, Accuracy = 94.94%: 100%|██████████| 13/13 [00:00<00:00, 73.97it/s]\n",
            "[33 / 50] Train: Loss = 0.09611, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 54.32it/s]\n",
            "[33 / 50]   Val: Loss = 33.21612, Accuracy = 94.91%: 100%|██████████| 13/13 [00:00<00:00, 69.82it/s]\n",
            "[34 / 50] Train: Loss = 0.10066, Accuracy = 99.83%: 100%|██████████| 572/572 [00:10<00:00, 53.50it/s]\n",
            "[34 / 50]   Val: Loss = 32.02783, Accuracy = 94.93%: 100%|██████████| 13/13 [00:00<00:00, 72.36it/s]\n",
            "[35 / 50] Train: Loss = 0.65595, Accuracy = 99.49%: 100%|██████████| 572/572 [00:10<00:00, 53.64it/s]\n",
            "[35 / 50]   Val: Loss = 30.85561, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 70.37it/s]\n",
            "[36 / 50] Train: Loss = 0.67115, Accuracy = 99.40%: 100%|██████████| 572/572 [00:10<00:00, 54.67it/s]\n",
            "[36 / 50]   Val: Loss = 29.10182, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 77.12it/s]\n",
            "[37 / 50] Train: Loss = 0.17865, Accuracy = 99.77%: 100%|██████████| 572/572 [00:10<00:00, 56.30it/s]\n",
            "[37 / 50]   Val: Loss = 29.09150, Accuracy = 95.07%: 100%|██████████| 13/13 [00:00<00:00, 75.67it/s]\n",
            "[38 / 50] Train: Loss = 0.10728, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 55.39it/s]\n",
            "[38 / 50]   Val: Loss = 32.49047, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 74.60it/s]\n",
            "[39 / 50] Train: Loss = 0.09104, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 55.26it/s]\n",
            "[39 / 50]   Val: Loss = 32.89977, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 71.28it/s]\n",
            "[40 / 50] Train: Loss = 0.08633, Accuracy = 99.86%: 100%|██████████| 572/572 [00:10<00:00, 54.91it/s]\n",
            "[40 / 50]   Val: Loss = 31.04182, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 79.98it/s]\n",
            "[41 / 50] Train: Loss = 0.08338, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 56.08it/s]\n",
            "[41 / 50]   Val: Loss = 31.18631, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 79.48it/s]\n",
            "[42 / 50] Train: Loss = 0.08329, Accuracy = 99.85%: 100%|██████████| 572/572 [00:09<00:00, 57.81it/s]\n",
            "[42 / 50]   Val: Loss = 32.57521, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 71.36it/s]\n",
            "[43 / 50] Train: Loss = 0.08281, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 56.35it/s]\n",
            "[43 / 50]   Val: Loss = 33.88540, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 63.69it/s]\n",
            "[44 / 50] Train: Loss = 0.08506, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 55.11it/s]\n",
            "[44 / 50]   Val: Loss = 35.48025, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 73.64it/s]\n",
            "[45 / 50] Train: Loss = 0.14558, Accuracy = 99.80%: 100%|██████████| 572/572 [00:10<00:00, 54.02it/s]\n",
            "[45 / 50]   Val: Loss = 33.33659, Accuracy = 94.85%: 100%|██████████| 13/13 [00:00<00:00, 74.46it/s]\n",
            "[46 / 50] Train: Loss = 0.69140, Accuracy = 99.39%: 100%|██████████| 572/572 [00:10<00:00, 55.65it/s]\n",
            "[46 / 50]   Val: Loss = 31.10902, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 74.66it/s]\n",
            "[47 / 50] Train: Loss = 0.23306, Accuracy = 99.74%: 100%|██████████| 572/572 [00:10<00:00, 56.36it/s]\n",
            "[47 / 50]   Val: Loss = 34.64309, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 73.01it/s]\n",
            "[48 / 50] Train: Loss = 0.10575, Accuracy = 99.84%: 100%|██████████| 572/572 [00:10<00:00, 53.67it/s]\n",
            "[48 / 50]   Val: Loss = 28.00582, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 69.68it/s]\n",
            "[49 / 50] Train: Loss = 0.08396, Accuracy = 99.86%: 100%|██████████| 572/572 [00:10<00:00, 54.34it/s]\n",
            "[49 / 50]   Val: Loss = 34.49617, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 72.63it/s]\n",
            "[50 / 50] Train: Loss = 0.08008, Accuracy = 99.85%: 100%|██████████| 572/572 [00:10<00:00, 55.76it/s]\n",
            "[50 / 50]   Val: Loss = 33.61025, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 74.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a424d2d3-73fb-44a3-9fe8-2713b3c1686c"
      },
      "source": [
        "batches_count = math.ceil(len(X_test) / 512)\n",
        "epoch_loss = 0\n",
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), 512)):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    loss = 0\n",
        "    for j, y_pred in enumerate(logits):\n",
        "        loss += criterion(y_pred, y_batch[j])\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    pred = torch.argmax(logits, dim=-1)\n",
        "    cur_sum_count = float(torch.sum(y_batch != 0))\n",
        "    cur_correct_count = float(torch.sum((pred == y_batch) * (y_batch != 0)))\n",
        "\n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "print(f'Loss = {epoch_loss / batches_count:.5f}, accuracy = {correct_count / sum_count:.2f}')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 31.81033, accuracy = 0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az2SoZ9PRYYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(in_features=2*lstm_hidden_dim,\n",
        "                            out_features=tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.emb_layer(inputs)\n",
        "        output, _ = self.lstm(emb)\n",
        "\n",
        "        return self.fc(output)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx45QTL84r85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b316fe33-be16-475f-db1c-97ff4935b331"
      },
      "source": [
        "model_bi = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model_bi.parameters())\n",
        "\n",
        "fit(model_bi, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 41.89356, Accuracy = 77.97%: 100%|██████████| 572/572 [00:11<00:00, 47.68it/s]\n",
            "[1 / 50]   Val: Loss = 35.31317, Accuracy = 87.44%: 100%|██████████| 13/13 [00:00<00:00, 58.88it/s]\n",
            "[2 / 50] Train: Loss = 18.11076, Accuracy = 90.33%: 100%|██████████| 572/572 [00:11<00:00, 49.28it/s]\n",
            "[2 / 50]   Val: Loss = 25.47519, Accuracy = 91.62%: 100%|██████████| 13/13 [00:00<00:00, 59.52it/s]\n",
            "[3 / 50] Train: Loss = 11.48646, Accuracy = 93.63%: 100%|██████████| 572/572 [00:11<00:00, 48.65it/s]\n",
            "[3 / 50]   Val: Loss = 22.50198, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 61.66it/s]\n",
            "[4 / 50] Train: Loss = 7.63189, Accuracy = 95.49%: 100%|██████████| 572/572 [00:11<00:00, 49.09it/s]\n",
            "[4 / 50]   Val: Loss = 18.51950, Accuracy = 94.53%: 100%|██████████| 13/13 [00:00<00:00, 58.91it/s]\n",
            "[5 / 50] Train: Loss = 5.20474, Accuracy = 96.65%: 100%|██████████| 572/572 [00:11<00:00, 48.40it/s]\n",
            "[5 / 50]   Val: Loss = 15.43467, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 56.78it/s]\n",
            "[6 / 50] Train: Loss = 3.60792, Accuracy = 97.45%: 100%|██████████| 572/572 [00:11<00:00, 48.39it/s]\n",
            "[6 / 50]   Val: Loss = 13.27113, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 64.31it/s]\n",
            "[7 / 50] Train: Loss = 2.60006, Accuracy = 98.05%: 100%|██████████| 572/572 [00:11<00:00, 49.85it/s]\n",
            "[7 / 50]   Val: Loss = 14.15221, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 63.39it/s]\n",
            "[8 / 50] Train: Loss = 1.85330, Accuracy = 98.51%: 100%|██████████| 572/572 [00:11<00:00, 49.36it/s]\n",
            "[8 / 50]   Val: Loss = 15.37917, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 57.97it/s]\n",
            "[9 / 50] Train: Loss = 1.33462, Accuracy = 98.88%: 100%|██████████| 572/572 [00:11<00:00, 49.66it/s]\n",
            "[9 / 50]   Val: Loss = 16.59408, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 61.48it/s]\n",
            "[10 / 50] Train: Loss = 0.95947, Accuracy = 99.17%: 100%|██████████| 572/572 [00:11<00:00, 51.05it/s]\n",
            "[10 / 50]   Val: Loss = 18.84668, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 60.80it/s]\n",
            "[11 / 50] Train: Loss = 0.68068, Accuracy = 99.40%: 100%|██████████| 572/572 [00:12<00:00, 46.65it/s]\n",
            "[11 / 50]   Val: Loss = 17.31407, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 58.85it/s]\n",
            "[12 / 50] Train: Loss = 0.47123, Accuracy = 99.58%: 100%|██████████| 572/572 [00:11<00:00, 49.07it/s]\n",
            "[12 / 50]   Val: Loss = 17.33367, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 61.07it/s]\n",
            "[13 / 50] Train: Loss = 0.31349, Accuracy = 99.73%: 100%|██████████| 572/572 [00:12<00:00, 47.55it/s]\n",
            "[13 / 50]   Val: Loss = 17.89938, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 62.22it/s]\n",
            "[14 / 50] Train: Loss = 0.20748, Accuracy = 99.84%: 100%|██████████| 572/572 [00:11<00:00, 48.72it/s]\n",
            "[14 / 50]   Val: Loss = 20.24659, Accuracy = 95.96%: 100%|██████████| 13/13 [00:00<00:00, 59.18it/s]\n",
            "[15 / 50] Train: Loss = 0.13263, Accuracy = 99.91%: 100%|██████████| 572/572 [00:11<00:00, 49.70it/s]\n",
            "[15 / 50]   Val: Loss = 19.08303, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 61.54it/s]\n",
            "[16 / 50] Train: Loss = 0.07942, Accuracy = 99.95%: 100%|██████████| 572/572 [00:11<00:00, 48.74it/s]\n",
            "[16 / 50]   Val: Loss = 23.62690, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 60.92it/s]\n",
            "[17 / 50] Train: Loss = 0.04809, Accuracy = 99.98%: 100%|██████████| 572/572 [00:11<00:00, 48.89it/s]\n",
            "[17 / 50]   Val: Loss = 22.10877, Accuracy = 96.01%: 100%|██████████| 13/13 [00:00<00:00, 60.05it/s]\n",
            "[18 / 50] Train: Loss = 0.04784, Accuracy = 99.98%: 100%|██████████| 572/572 [00:11<00:00, 48.32it/s]\n",
            "[18 / 50]   Val: Loss = 24.01755, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 57.05it/s]\n",
            "[19 / 50] Train: Loss = 0.60775, Accuracy = 99.52%: 100%|██████████| 572/572 [00:11<00:00, 48.46it/s]\n",
            "[19 / 50]   Val: Loss = 27.11346, Accuracy = 95.60%: 100%|██████████| 13/13 [00:00<00:00, 63.99it/s]\n",
            "[20 / 50] Train: Loss = 0.24509, Accuracy = 99.80%: 100%|██████████| 572/572 [00:11<00:00, 48.45it/s]\n",
            "[20 / 50]   Val: Loss = 24.86472, Accuracy = 95.82%: 100%|██████████| 13/13 [00:00<00:00, 61.78it/s]\n",
            "[21 / 50] Train: Loss = 0.06170, Accuracy = 99.96%: 100%|██████████| 572/572 [00:12<00:00, 47.39it/s]\n",
            "[21 / 50]   Val: Loss = 24.00536, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 62.47it/s]\n",
            "[22 / 50] Train: Loss = 0.03033, Accuracy = 99.99%: 100%|██████████| 572/572 [00:11<00:00, 48.94it/s]\n",
            "[22 / 50]   Val: Loss = 22.64509, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 63.57it/s]\n",
            "[23 / 50] Train: Loss = 0.01693, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 49.87it/s]\n",
            "[23 / 50]   Val: Loss = 22.99409, Accuracy = 96.09%: 100%|██████████| 13/13 [00:00<00:00, 63.59it/s]\n",
            "[24 / 50] Train: Loss = 0.01208, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.01it/s]\n",
            "[24 / 50]   Val: Loss = 24.75284, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 55.16it/s]\n",
            "[25 / 50] Train: Loss = 0.00886, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 46.27it/s]\n",
            "[25 / 50]   Val: Loss = 29.12637, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 61.94it/s]\n",
            "[26 / 50] Train: Loss = 0.00681, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 48.27it/s]\n",
            "[26 / 50]   Val: Loss = 23.86003, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 62.43it/s]\n",
            "[27 / 50] Train: Loss = 0.00531, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 48.39it/s]\n",
            "[27 / 50]   Val: Loss = 24.46542, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 61.29it/s]\n",
            "[28 / 50] Train: Loss = 0.00408, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 47.69it/s]\n",
            "[28 / 50]   Val: Loss = 28.23969, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 62.79it/s]\n",
            "[29 / 50] Train: Loss = 0.00329, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.37it/s]\n",
            "[29 / 50]   Val: Loss = 27.00677, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 58.64it/s]\n",
            "[30 / 50] Train: Loss = 0.00288, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.31it/s]\n",
            "[30 / 50]   Val: Loss = 29.86039, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 61.19it/s]\n",
            "[31 / 50] Train: Loss = 0.47834, Accuracy = 99.67%: 100%|██████████| 572/572 [00:11<00:00, 48.34it/s]\n",
            "[31 / 50]   Val: Loss = 22.64331, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 60.69it/s]\n",
            "[32 / 50] Train: Loss = 0.28357, Accuracy = 99.78%: 100%|██████████| 572/572 [00:11<00:00, 49.02it/s]\n",
            "[32 / 50]   Val: Loss = 23.51150, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 62.67it/s]\n",
            "[33 / 50] Train: Loss = 0.05351, Accuracy = 99.96%: 100%|██████████| 572/572 [00:11<00:00, 49.02it/s]\n",
            "[33 / 50]   Val: Loss = 25.85319, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 58.75it/s]\n",
            "[34 / 50] Train: Loss = 0.01457, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 47.67it/s]\n",
            "[34 / 50]   Val: Loss = 23.96213, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 57.07it/s]\n",
            "[35 / 50] Train: Loss = 0.00842, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 48.12it/s]\n",
            "[35 / 50]   Val: Loss = 25.41981, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 60.80it/s]\n",
            "[36 / 50] Train: Loss = 0.00617, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 46.33it/s]\n",
            "[36 / 50]   Val: Loss = 23.47445, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 61.02it/s]\n",
            "[37 / 50] Train: Loss = 0.00480, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 46.50it/s]\n",
            "[37 / 50]   Val: Loss = 23.83806, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 61.09it/s]\n",
            "[38 / 50] Train: Loss = 0.00377, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 46.63it/s]\n",
            "[38 / 50]   Val: Loss = 24.78126, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 60.28it/s]\n",
            "[39 / 50] Train: Loss = 0.00296, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.11it/s]\n",
            "[39 / 50]   Val: Loss = 27.81981, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 58.75it/s]\n",
            "[40 / 50] Train: Loss = 0.00249, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.63it/s]\n",
            "[40 / 50]   Val: Loss = 22.80683, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 60.31it/s]\n",
            "[41 / 50] Train: Loss = 0.00208, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 48.26it/s]\n",
            "[41 / 50]   Val: Loss = 23.55267, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 60.09it/s]\n",
            "[42 / 50] Train: Loss = 0.00172, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 49.07it/s]\n",
            "[42 / 50]   Val: Loss = 30.65989, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 58.97it/s]\n",
            "[43 / 50] Train: Loss = 0.00179, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 47.96it/s]\n",
            "[43 / 50]   Val: Loss = 29.40069, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 58.31it/s]\n",
            "[44 / 50] Train: Loss = 0.04934, Accuracy = 99.96%: 100%|██████████| 572/572 [00:12<00:00, 47.64it/s]\n",
            "[44 / 50]   Val: Loss = 23.50423, Accuracy = 96.07%: 100%|██████████| 13/13 [00:00<00:00, 58.83it/s]\n",
            "[45 / 50] Train: Loss = 0.82381, Accuracy = 99.50%: 100%|██████████| 572/572 [00:11<00:00, 48.42it/s]\n",
            "[45 / 50]   Val: Loss = 26.84023, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 59.44it/s]\n",
            "[46 / 50] Train: Loss = 0.13758, Accuracy = 99.89%: 100%|██████████| 572/572 [00:11<00:00, 48.64it/s]\n",
            "[46 / 50]   Val: Loss = 23.13501, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 59.94it/s]\n",
            "[47 / 50] Train: Loss = 0.02892, Accuracy = 99.98%: 100%|██████████| 572/572 [00:12<00:00, 46.23it/s]\n",
            "[47 / 50]   Val: Loss = 18.67616, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 59.83it/s]\n",
            "[48 / 50] Train: Loss = 0.01303, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 47.03it/s]\n",
            "[48 / 50]   Val: Loss = 26.34811, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 58.61it/s]\n",
            "[49 / 50] Train: Loss = 0.00795, Accuracy = 100.00%: 100%|██████████| 572/572 [00:11<00:00, 47.85it/s]\n",
            "[49 / 50]   Val: Loss = 21.90918, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 61.93it/s]\n",
            "[50 / 50] Train: Loss = 0.00590, Accuracy = 100.00%: 100%|██████████| 572/572 [00:12<00:00, 46.65it/s]\n",
            "[50 / 50]   Val: Loss = 25.18917, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 54.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmaMpOYQ4_he",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d323df3-187c-4cdf-d92d-2087c7565500"
      },
      "source": [
        "batches_count = math.ceil(len(X_test) / 512)\n",
        "epoch_loss = 0\n",
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), 512)):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model_bi(X_batch)\n",
        "\n",
        "    loss = 0\n",
        "    for j, y_pred in enumerate(logits):\n",
        "        loss += criterion(y_pred, y_batch[j])\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    pred = torch.argmax(logits, dim=-1)\n",
        "    cur_sum_count = float(torch.sum(y_batch != 0))\n",
        "    cur_correct_count = float(torch.sum((pred == y_batch) * (y_batch != 0)))\n",
        "\n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "print(f'Loss = {epoch_loss / batches_count:.5f}, accuracy = {correct_count / sum_count:.2f}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 23.37945, accuracy = 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f7ee8164-6ff6-4c5f-ffd1-77bb49605b49"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.2% 124.5/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87f43370-dfd2-4df9-8f77-09dcc41d7e42"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.emb_layer = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(input_size=embeddings.shape[1],\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(in_features=2*lstm_hidden_dim,\n",
        "                            out_features=tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.emb_layer(inputs)\n",
        "        output, _ = self.lstm(emb)\n",
        "\n",
        "        return self.fc(output)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d11c5552-aa5a-403f-ef26-03c89f33197b"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 43.51732, Accuracy = 79.80%: 100%|██████████| 572/572 [00:11<00:00, 51.85it/s]\n",
            "[1 / 50]   Val: Loss = 33.00947, Accuracy = 89.11%: 100%|██████████| 13/13 [00:00<00:00, 66.86it/s]\n",
            "[2 / 50] Train: Loss = 16.82947, Accuracy = 91.77%: 100%|██████████| 572/572 [00:11<00:00, 49.89it/s]\n",
            "[2 / 50]   Val: Loss = 26.16677, Accuracy = 92.31%: 100%|██████████| 13/13 [00:00<00:00, 64.29it/s]\n",
            "[3 / 50] Train: Loss = 12.64051, Accuracy = 93.62%: 100%|██████████| 572/572 [00:10<00:00, 52.40it/s]\n",
            "[3 / 50]   Val: Loss = 20.69848, Accuracy = 93.67%: 100%|██████████| 13/13 [00:00<00:00, 65.32it/s]\n",
            "[4 / 50] Train: Loss = 10.30919, Accuracy = 94.67%: 100%|██████████| 572/572 [00:10<00:00, 53.75it/s]\n",
            "[4 / 50]   Val: Loss = 15.63526, Accuracy = 94.46%: 100%|██████████| 13/13 [00:00<00:00, 60.33it/s]\n",
            "[5 / 50] Train: Loss = 8.53167, Accuracy = 95.38%: 100%|██████████| 572/572 [00:10<00:00, 52.77it/s]\n",
            "[5 / 50]   Val: Loss = 14.60038, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 56.80it/s]\n",
            "[6 / 50] Train: Loss = 7.36959, Accuracy = 95.87%: 100%|██████████| 572/572 [00:10<00:00, 52.33it/s]\n",
            "[6 / 50]   Val: Loss = 14.04286, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 67.40it/s]\n",
            "[7 / 50] Train: Loss = 6.44484, Accuracy = 96.25%: 100%|██████████| 572/572 [00:11<00:00, 51.39it/s]\n",
            "[7 / 50]   Val: Loss = 12.62934, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 53.49it/s]\n",
            "[8 / 50] Train: Loss = 5.77356, Accuracy = 96.52%: 100%|██████████| 572/572 [00:10<00:00, 52.24it/s]\n",
            "[8 / 50]   Val: Loss = 11.80123, Accuracy = 95.86%: 100%|██████████| 13/13 [00:00<00:00, 56.02it/s]\n",
            "[9 / 50] Train: Loss = 5.15338, Accuracy = 96.75%: 100%|██████████| 572/572 [00:10<00:00, 52.65it/s]\n",
            "[9 / 50]   Val: Loss = 12.26410, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 63.57it/s]\n",
            "[10 / 50] Train: Loss = 4.77760, Accuracy = 96.94%: 100%|██████████| 572/572 [00:11<00:00, 49.97it/s]\n",
            "[10 / 50]   Val: Loss = 11.49313, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 64.34it/s]\n",
            "[11 / 50] Train: Loss = 4.30087, Accuracy = 97.12%: 100%|██████████| 572/572 [00:11<00:00, 51.93it/s]\n",
            "[11 / 50]   Val: Loss = 10.87623, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 63.44it/s]\n",
            "[12 / 50] Train: Loss = 4.03875, Accuracy = 97.25%: 100%|██████████| 572/572 [00:11<00:00, 51.79it/s]\n",
            "[12 / 50]   Val: Loss = 10.75379, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 63.22it/s]\n",
            "[13 / 50] Train: Loss = 3.71760, Accuracy = 97.37%: 100%|██████████| 572/572 [00:10<00:00, 52.60it/s]\n",
            "[13 / 50]   Val: Loss = 10.65327, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 60.40it/s]\n",
            "[14 / 50] Train: Loss = 3.45305, Accuracy = 97.50%: 100%|██████████| 572/572 [00:11<00:00, 51.59it/s]\n",
            "[14 / 50]   Val: Loss = 10.67784, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 65.25it/s]\n",
            "[15 / 50] Train: Loss = 3.25523, Accuracy = 97.57%: 100%|██████████| 572/572 [00:10<00:00, 53.14it/s]\n",
            "[15 / 50]   Val: Loss = 10.67243, Accuracy = 96.56%: 100%|██████████| 13/13 [00:00<00:00, 64.40it/s]\n",
            "[16 / 50] Train: Loss = 3.00329, Accuracy = 97.70%: 100%|██████████| 572/572 [00:11<00:00, 51.03it/s]\n",
            "[16 / 50]   Val: Loss = 10.00621, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 55.04it/s]\n",
            "[17 / 50] Train: Loss = 2.86033, Accuracy = 97.78%: 100%|██████████| 572/572 [00:10<00:00, 53.31it/s]\n",
            "[17 / 50]   Val: Loss = 10.76874, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 63.25it/s]\n",
            "[18 / 50] Train: Loss = 2.65658, Accuracy = 97.88%: 100%|██████████| 572/572 [00:10<00:00, 53.69it/s]\n",
            "[18 / 50]   Val: Loss = 9.04828, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 64.45it/s]\n",
            "[19 / 50] Train: Loss = 2.51946, Accuracy = 97.97%: 100%|██████████| 572/572 [00:11<00:00, 51.09it/s]\n",
            "[19 / 50]   Val: Loss = 9.73256, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 64.24it/s]\n",
            "[20 / 50] Train: Loss = 2.39588, Accuracy = 98.03%: 100%|██████████| 572/572 [00:11<00:00, 50.63it/s]\n",
            "[20 / 50]   Val: Loss = 10.12571, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 62.69it/s]\n",
            "[21 / 50] Train: Loss = 2.27054, Accuracy = 98.09%: 100%|██████████| 572/572 [00:11<00:00, 51.39it/s]\n",
            "[21 / 50]   Val: Loss = 9.80351, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 65.13it/s]\n",
            "[22 / 50] Train: Loss = 2.16567, Accuracy = 98.17%: 100%|██████████| 572/572 [00:11<00:00, 49.31it/s]\n",
            "[22 / 50]   Val: Loss = 8.46878, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 65.30it/s]\n",
            "[23 / 50] Train: Loss = 2.05793, Accuracy = 98.24%: 100%|██████████| 572/572 [00:11<00:00, 49.43it/s]\n",
            "[23 / 50]   Val: Loss = 9.90109, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 55.48it/s]\n",
            "[24 / 50] Train: Loss = 1.96351, Accuracy = 98.29%: 100%|██████████| 572/572 [00:11<00:00, 51.36it/s]\n",
            "[24 / 50]   Val: Loss = 10.78686, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 64.21it/s]\n",
            "[25 / 50] Train: Loss = 1.88734, Accuracy = 98.35%: 100%|██████████| 572/572 [00:11<00:00, 49.50it/s]\n",
            "[25 / 50]   Val: Loss = 9.30813, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 65.20it/s]\n",
            "[26 / 50] Train: Loss = 1.75907, Accuracy = 98.42%: 100%|██████████| 572/572 [00:10<00:00, 52.75it/s]\n",
            "[26 / 50]   Val: Loss = 9.34758, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 60.60it/s]\n",
            "[27 / 50] Train: Loss = 1.69344, Accuracy = 98.47%: 100%|██████████| 572/572 [00:11<00:00, 52.00it/s]\n",
            "[27 / 50]   Val: Loss = 10.91232, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 64.50it/s]\n",
            "[28 / 50] Train: Loss = 1.62385, Accuracy = 98.53%: 100%|██████████| 572/572 [00:11<00:00, 51.08it/s]\n",
            "[28 / 50]   Val: Loss = 10.30086, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 64.04it/s]\n",
            "[29 / 50] Train: Loss = 1.54895, Accuracy = 98.58%: 100%|██████████| 572/572 [00:10<00:00, 52.87it/s]\n",
            "[29 / 50]   Val: Loss = 10.08191, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 64.57it/s]\n",
            "[30 / 50] Train: Loss = 1.45247, Accuracy = 98.65%: 100%|██████████| 572/572 [00:11<00:00, 51.59it/s]\n",
            "[30 / 50]   Val: Loss = 9.83722, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 53.00it/s]\n",
            "[31 / 50] Train: Loss = 1.39757, Accuracy = 98.69%: 100%|██████████| 572/572 [00:11<00:00, 51.30it/s]\n",
            "[31 / 50]   Val: Loss = 10.30968, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 62.53it/s]\n",
            "[32 / 50] Train: Loss = 1.34427, Accuracy = 98.74%: 100%|██████████| 572/572 [00:11<00:00, 51.13it/s]\n",
            "[32 / 50]   Val: Loss = 10.62598, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 54.58it/s]\n",
            "[33 / 50] Train: Loss = 1.29037, Accuracy = 98.77%: 100%|██████████| 572/572 [00:11<00:00, 51.97it/s]\n",
            "[33 / 50]   Val: Loss = 10.63675, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 54.78it/s]\n",
            "[34 / 50] Train: Loss = 1.22132, Accuracy = 98.84%: 100%|██████████| 572/572 [00:11<00:00, 51.42it/s]\n",
            "[34 / 50]   Val: Loss = 10.40736, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 56.46it/s]\n",
            "[35 / 50] Train: Loss = 1.17637, Accuracy = 98.87%: 100%|██████████| 572/572 [00:10<00:00, 52.46it/s]\n",
            "[35 / 50]   Val: Loss = 9.97358, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 69.54it/s]\n",
            "[36 / 50] Train: Loss = 1.10629, Accuracy = 98.94%: 100%|██████████| 572/572 [00:11<00:00, 51.71it/s]\n",
            "[36 / 50]   Val: Loss = 10.53577, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 67.36it/s]\n",
            "[37 / 50] Train: Loss = 1.05105, Accuracy = 98.98%: 100%|██████████| 572/572 [00:11<00:00, 51.42it/s]\n",
            "[37 / 50]   Val: Loss = 11.69714, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 53.33it/s]\n",
            "[38 / 50] Train: Loss = 1.02971, Accuracy = 99.00%: 100%|██████████| 572/572 [00:11<00:00, 49.97it/s]\n",
            "[38 / 50]   Val: Loss = 10.59226, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 55.64it/s]\n",
            "[39 / 50] Train: Loss = 0.97666, Accuracy = 99.05%: 100%|██████████| 572/572 [00:10<00:00, 54.07it/s]\n",
            "[39 / 50]   Val: Loss = 12.16001, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 54.68it/s]\n",
            "[40 / 50] Train: Loss = 0.92889, Accuracy = 99.10%: 100%|██████████| 572/572 [00:11<00:00, 52.00it/s]\n",
            "[40 / 50]   Val: Loss = 11.07539, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 49.03it/s]\n",
            "[41 / 50] Train: Loss = 0.89229, Accuracy = 99.13%: 100%|██████████| 572/572 [00:10<00:00, 52.54it/s]\n",
            "[41 / 50]   Val: Loss = 11.52890, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 67.10it/s]\n",
            "[42 / 50] Train: Loss = 0.84367, Accuracy = 99.17%: 100%|██████████| 572/572 [00:11<00:00, 51.53it/s]\n",
            "[42 / 50]   Val: Loss = 13.04657, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 63.79it/s]\n",
            "[43 / 50] Train: Loss = 0.80784, Accuracy = 99.20%: 100%|██████████| 572/572 [00:10<00:00, 53.43it/s]\n",
            "[43 / 50]   Val: Loss = 12.66002, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 63.80it/s]\n",
            "[44 / 50] Train: Loss = 0.76901, Accuracy = 99.24%: 100%|██████████| 572/572 [00:10<00:00, 52.99it/s]\n",
            "[44 / 50]   Val: Loss = 12.10863, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 67.29it/s]\n",
            "[45 / 50] Train: Loss = 0.73121, Accuracy = 99.28%: 100%|██████████| 572/572 [00:10<00:00, 55.12it/s]\n",
            "[45 / 50]   Val: Loss = 13.36436, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 66.63it/s]\n",
            "[46 / 50] Train: Loss = 0.71426, Accuracy = 99.30%: 100%|██████████| 572/572 [00:10<00:00, 52.85it/s]\n",
            "[46 / 50]   Val: Loss = 10.90725, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 69.14it/s]\n",
            "[47 / 50] Train: Loss = 0.67729, Accuracy = 99.33%: 100%|██████████| 572/572 [00:10<00:00, 54.28it/s]\n",
            "[47 / 50]   Val: Loss = 12.99652, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 67.71it/s]\n",
            "[48 / 50] Train: Loss = 0.69329, Accuracy = 99.31%: 100%|██████████| 572/572 [00:11<00:00, 50.00it/s]\n",
            "[48 / 50]   Val: Loss = 12.11054, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 58.06it/s]\n",
            "[49 / 50] Train: Loss = 0.62098, Accuracy = 99.39%: 100%|██████████| 572/572 [00:10<00:00, 53.77it/s]\n",
            "[49 / 50]   Val: Loss = 14.71641, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 61.88it/s]\n",
            "[50 / 50] Train: Loss = 0.56577, Accuracy = 99.45%: 100%|██████████| 572/572 [00:11<00:00, 51.63it/s]\n",
            "[50 / 50]   Val: Loss = 13.69769, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 66.96it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e3cb400-6595-4d5e-9147-7777dbb42d0d"
      },
      "source": [
        "batches_count = math.ceil(len(X_test) / 512)\n",
        "epoch_loss = 0\n",
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), 512)):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    loss = 0\n",
        "    for j, y_pred in enumerate(logits):\n",
        "        loss += criterion(y_pred, y_batch[j])\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    pred = torch.argmax(logits, dim=-1)\n",
        "    cur_sum_count = float(torch.sum(y_batch != 0))\n",
        "    cur_correct_count = float(torch.sum((pred == y_batch) * (y_batch != 0)))\n",
        "\n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "print(f'Loss = {epoch_loss / batches_count:.5f}, accuracy = {correct_count / sum_count:.2f}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 15.06503, accuracy = 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFzcpWf1-wmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}